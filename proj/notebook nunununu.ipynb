{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conv1 = ConvolutionLayer(22, 22, (1,7), (1,3),).to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2 = ConvolutionLayer(22, 22, (3,3), (2,2)).to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chromas = torch.randn(32,22,12,431)\n",
    "mfccs = \n",
    "batch_size = 32\n",
    "\n",
    "chromas = chromas.reshape(batch_size, 1, 12, 431)\n",
    "chromas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv3 = ConvolutionLayer(1,8,kernel_size=(1,11),stride=(1,6)).to('cuda:0')\n",
    "conv4 = ConvolutionLayer(8,22,kernel_size=(1,3),stride=(1,2)).to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conv3(chromas.float()).shape\n",
    "h = conv3(chromas.float())\n",
    "j = conv4(h)\n",
    "j.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = conv1(c.float())\n",
    "print(d.shape)\n",
    "e = conv2(d)\n",
    "print(e.shape)\n",
    "f = conv2(e)\n",
    "f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_conv_dimensions(F,T,):\n",
    "    x = torch.randn(1,1,F,T).to('cuda:0').double()\n",
    "    return conv4(conv3(x)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv3 = ConvolutionLayer(1,8,kernel_size=(1,11),stride=(1,3)).to('cuda:0')\n",
    "conv3.double()\n",
    "conv4 = ConvolutionLayer(8,22,kernel_size=(1,3),stride=(1,2)).to('cuda:0')\n",
    "conv4.double()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_conv_dimensions(1025,431)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_memory():\n",
    "    t = torch.cuda.get_device_properties(0).total_memory\n",
    "    r = torch.cuda.memory_reserved(0)\n",
    "    a = torch.cuda.memory_allocated(0)\n",
    "    f = r-a  # free inside reserved\n",
    "    GB = 1024 ** 3\n",
    "    print(\"Total: \", t / GB)\n",
    "    print(\"Reserved \", r / GB)\n",
    "    print(\"Allocated: \", a / GB)\n",
    "    print(\"free: \", f / GB)\n",
    "    \n",
    "def free_memory(var):\n",
    "    del var\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLSTM Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ablstms = AlternatingBLSTMs(22, 96, 70).to('cuda:0').double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "z = torch.randn(32,22,96,70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "z = z.to('cuda:0').double()\n",
    "z = ablstms(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = ablstms(z[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bblstm(z).permute(0,2,3,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bblstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chromas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deconv = TransposeConvolutionLayer(22, 22, (2,2), (2,2)).to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zz = torch.randn(32,22,96,70).to('cuda:0')\n",
    "deconv(zz).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deconv.double()\n",
    "deconv(out).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(\n",
    "\tparam.numel() for param in conv4.parameters()\n",
    ")\n",
    "total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = model.bandsplit(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv3 = ConvolutionLayer(1,8,kernel_size=(1,11),stride=(1,3)).to(device).double()\n",
    "conv4 = ConvolutionLayer(8,22,kernel_size=(1,3),stride=(1,2)).to(device).double()\n",
    "conv5 = ConvolutionLayer(1,8,kernel_size=(1,11),stride=(1,3)).to(device).double()\n",
    "conv6 = ConvolutionLayer(8,22,kernel_size=(1,3),stride=(1,2)).to(device).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = ConvolutionLayer(22,22,kernel_size=(1,7),stride=(1,3)).to('cuda:0').double()\n",
    "conv2 = ConvolutionLayer(22,22,kernel_size=(2,3),stride=(2,2)).to('cuda:0').double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deconv1 = TransposeConvolutionLayer(22,22,kernel_size=(1,7),stride=(1,3)).to('cuda:0').double()\n",
    "deconv2 = TransposeConvolutionLayer(22,22,kernel_size=(2,3),stride=(2,2)).to('cuda:0').double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test = conv2(conv1(bands))\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = deconv2(test)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfccs = torch.randn(32,22,128,431).to(device).double()\n",
    "chromas = torch.randn(32,1,12,431).to(device).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv4(conv3(mfccs)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SSLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def test_module(module, input_size):\n",
    "    \"\"\"\n",
    "    Test an nn.Module subclass with random input data of the given size.\n",
    "    \n",
    "    :param module: The nn.Module subclass to test.\n",
    "    :param input_size: A tuple of integers specifying the size of the input data.\n",
    "    \"\"\"\n",
    "    # Create random input data with the specified size\n",
    "    input_data = torch.randn(input_size)\n",
    "    print(f'Input shape: {input_data.shape}')\n",
    "\n",
    "    # Run the input data through the module\n",
    "    output = module(input_data)\n",
    "    print(f'Output shape: {output.shape}')\n",
    "    \n",
    "    \n",
    "    \n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, bandwidths, N, time_steps = 431, freq_bands = 1025, kernel1 = (1,7), stride1=(1,3), kernel2 = (4,4), stride2 = (2,2), device = 'cuda:0', dtype = 'double', n_mels = 32):\n",
    "        super(Model, self).__init__()\n",
    "        self.device = device\n",
    "        self.N = N\n",
    "        self.n_mels = n_mels\n",
    "        self.K = len(bandwidths)\n",
    "        self.time_steps = time_steps\n",
    "        self.bandsplit = BandSplit(bandwidths, N).to(device)\n",
    "        self.bandsplit.double()\n",
    "        self.conv1 = ConvolutionLayer(self.K, self.K, kernel1, stride1).to(device)\n",
    "        self.conv1.double()\n",
    "        self.conv2 = ConvolutionLayer(self.K, self.K, kernel2, stride2).to(device)\n",
    "        self.conv2.double()\n",
    "        self.conv3 = ConvolutionLayer(1,8,kernel_size=(1,11),stride=(1,3)).to(device)\n",
    "        self.conv3.double()\n",
    "        self.conv4 = ConvolutionLayer(8,22,kernel_size=(1,3),stride=(1,2)).to(device)\n",
    "        self.conv4.double()\n",
    "        self.conv5 = ConvolutionLayer(1,8,kernel_size=(1,11),stride=(1,3)).to(device)\n",
    "        self.conv5.double()\n",
    "        self.conv6 = ConvolutionLayer(8,22,kernel_size=(1,3),stride=(1,2)).to(device)\n",
    "        self.conv6.double()\n",
    "        self.blstms1 = AlternatingBLSTMs(self.K, 70, 63, 64).to(device).double()\n",
    "        self.blstms2 = AlternatingBLSTMs(self.K, 70, 96, 64).to(device).double()\n",
    "        self.blstms3 = AlternatingBLSTMs(self.K, 70, 76, 64 ).to(device).double()\n",
    "        self.deconv1 = TransposeConvolutionLayer(self.K, self.K, kernel2, stride2).to(device).double()\n",
    "        self.deconv2 = TransposeConvolutionLayer(self.K, self.K, kernel1, stride1).to(device).double()\n",
    "        self.masks = MaskEstimation(bandwidths, 130, 32).to(device).double()\n",
    "    def forward(self, X, chromas, mfccs):\n",
    "        print(\"1 : \" ,X.shape)\n",
    "        X = self.bandsplit(X)\n",
    "        print(\"2 : \" ,X.shape)\n",
    "        #Shape: torch.Size([32, 22, 431, 128]) (batch_size, num_bands, time_steps, freq_N)\n",
    "        X = self.conv1(X)\n",
    "        X = self.conv2(X)\n",
    "        print(\"3 : \" ,X.shape)\n",
    "        batch_size = X.shape[0]\n",
    "        mfccs = mfccs.reshape(batch_size,1,self.n_mels,-1).to(self.device)\n",
    "        mfccs = self.conv3(mfccs)\n",
    "        mfccs = self.conv4(mfccs)\n",
    "        chromas = chromas.reshape(batch_size,1,12,-1)\n",
    "        chromas = chromas.to(self.device)\n",
    "        chromas = self.conv5(chromas)\n",
    "        chromas = self.conv6(chromas)\n",
    "        X, _ = self.blstms1(X)\n",
    "        print(\"4 : \" ,X.shape)\n",
    "        mfccs = mfccs.to(self.device)\n",
    "        xmfccs = torch.cat((mfccs,X), 2)\n",
    "        X, _ = self.blstms2(xmfccs)\n",
    "        print(\"5 : \" ,X.shape)        \n",
    "        mfccs = mfccs.to(self.device)\n",
    "        print(chromas.shape)\n",
    "        xchromas = torch.cat((chromas,X),2)\n",
    "        print(xchromas.shape)\n",
    "        X, _ = self.blstms3(xchromas)\n",
    "        print(\"6 : \" ,X.shape)        \n",
    "        mfccs = mfccs.to(self.device)\n",
    "        print(X.shape)\n",
    "        X = self.deconv1(X)\n",
    "        X = self.deconv2(X)\n",
    "        print(\"TEST: \", X.shape)\n",
    "        X = self.masks(X)\n",
    "        print(\"TEST RETURN : \", X.shape)\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def post_conv_dimensions(self,N,time_steps,in_channels):\n",
    "        x = torch.randn(1,in_channels,N,time_steps).to(self.device).double()\n",
    "        return self.conv2(self.conv1(x)).shape\n",
    "        \n",
    "class ConvolutionLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding=0, dtype='double'):\n",
    "        super(ConvolutionLayer, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding=padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "    \n",
    "class TransposeConvolutionLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, dtype='double'):\n",
    "        super(TransposeConvolutionLayer, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "# This class defines a module that runs t\n",
    "class AlternatingBLSTMs(nn.Module):\n",
    "    def __init__(self, num_bands, time_steps, N, out_size, axis=1):\n",
    "        super(AlternatingBLSTMs, self).__init__()\n",
    "        self.band_blstm = BandBiLSTM(num_bands, time_steps, N)\n",
    "        self.temporal_blstm = TemporalBiLSTM(num_bands, time_steps, N, out_size)\n",
    "        self.num_bands = num_bands\n",
    "        self.time_steps = time_steps\n",
    "        self.N = N\n",
    "        # hidden size = freq_steps_per_band * time_steps \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input shape: (batch_size, num_bands, N, time_steps)\n",
    "        # Prepare for Band BLSTM: shape = (batch_size, num_bands, N * time_steps)\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.reshape(batch_size, self.num_bands, -1)\n",
    "        x = self.band_blstm(x)\n",
    "        x = x.reshape(batch_size, self.time_steps, -1)\n",
    "        x = self.temporal_blstm(x)\n",
    "        #x += residual\n",
    "        # Return the output of the module\n",
    "        return x    \n",
    "    \n",
    "# This class defines a module that runs the input, with shape (num_bands, num_timesteps, N), through a normalization layer, then a temporal biLSTM, then a fully connected layer.\n",
    "# Then, the output of that layer is of the same shape as the input to the module, which will be fed into a similar structure, but this time with a band biLSTM, following the same normalization, biLSTM, FC structure.\n",
    "class BandBiLSTM(nn.Module):\n",
    "    def __init__(self, num_bands, time_steps, N, axis=1):\n",
    "        super(BandBiLSTM, self).__init__()\n",
    "        self.norm = nn.GroupNorm(num_bands, num_bands)\n",
    "        self.input_size = time_steps * N\n",
    "        self.hidden_size = self.input_size // 2\n",
    "        self.bilstm = nn.LSTM(self.input_size, self.hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(N, N)\n",
    "        self.axis = axis\n",
    "        self.N = N\n",
    "        self.num_bands = num_bands\n",
    "        self.time_steps = time_steps\n",
    "        # hidden size = freq_steps_per_band * time_steps \n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"BAND INPUT SHAPE: \", x.shape)\n",
    "        batch_size = x.shape[0]\n",
    "        # (batch_size,time_steps, num_bands, N)\n",
    "        x = self.norm(x)\n",
    "        residual = x.clone().detach()\n",
    "        x, lstm_vars = self.bilstm(x)\n",
    "        # (batch_size, num_bands, 2 * hidden_size)\n",
    "        x = x.reshape(batch_size, self.num_bands, self.time_steps, self.N)\n",
    "        # (batch_size, num_bands, time_steps, N)\n",
    "        x = self.fc(x)\n",
    "        # (batch_size, num_bands, time_steps, N)\n",
    "        print(x.shape)\n",
    "        #x += residual\n",
    "        # Return the output of the module\n",
    "        return x\n",
    "    \n",
    "class TemporalBiLSTM(nn.Module):\n",
    "    def __init__(self, num_bands, time_steps, N, out, axis=1):\n",
    "        super(TemporalBiLSTM, self).__init__()\n",
    "        self.norm = nn.GroupNorm(time_steps, time_steps)\n",
    "        self.input_size = num_bands * N\n",
    "        self.hidden_size = num_bands * out // 2\n",
    "        self.out = out\n",
    "        self.bilstm = nn.LSTM(self.input_size, self.hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(out, out)\n",
    "        self.axis = axis\n",
    "        self.N = N\n",
    "        self.time_steps = time_steps\n",
    "        self.num_bands = num_bands\n",
    "        # hidden size = freq_steps_per_band * time_steps \n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        print(\"TEMPORAL INPUT SHAPE: \", x.shape)\n",
    "        # (batch_size,time_steps, num_bands, N)\n",
    "        x = self.norm(x)\n",
    "        residual = x.clone().detach()\n",
    "        x, lstm_vars = self.bilstm(x)\n",
    "        # (batch_size, num_bands, 2 * hidden_size)\n",
    "        x = x.reshape(batch_size, self.num_bands, self.time_steps, self.out)\n",
    "        print(x.shape)\n",
    "        # (batch_size, num_bands, time_steps, N)\n",
    "        x = self.fc(x)\n",
    "        x = x.permute(0,1,3,2)\n",
    "        # (batch_size, num_bands, time_steps, N)\n",
    "        #x += residual\n",
    "        # Return the output of the module\n",
    "        return x, lstm_vars\n",
    "\n",
    "class BandSplit(torch.nn.Module):\n",
    "    # Input shape: torch.Size([16, 2, 1025, 431, 2])\n",
    "    def __init__(self, bandwidths, N):\n",
    "        # bandwidth\n",
    "        super(BandSplit, self).__init__()\n",
    "        self.bandwidths = bandwidths\n",
    "        self.norm_layers = torch.nn.ModuleList([torch.nn.LayerNorm(2 * bandwidth) for bandwidth in self.bandwidths])\n",
    "        self.fc_layers = torch.nn.ModuleList([torch.nn.Linear(2 * bandwidth, N) for bandwidth in self.bandwidths])\n",
    "\n",
    "    def forward(self, X):\n",
    "        subband_spectrograms = []\n",
    "        K = len(self.bandwidths)\n",
    "        for i in range(K):\n",
    "            start_index = sum(self.bandwidths[:i])\n",
    "            end_index = start_index + self.bandwidths[i]\n",
    "            subband_spectrogram = X[:, :,start_index:end_index, :]\n",
    "            subband_spectrogram = subband_spectrogram.permute(0,1,4,2,3)\n",
    "            subband_spectrogram = subband_spectrogram.reshape(2 * X.shape[0], X.shape[3], 2 * self.bandwidths[i])\n",
    "            subband_spectrograms.append(subband_spectrogram)\n",
    "\n",
    "        subband_features = []\n",
    "        for i in range(K):\n",
    "            norm_output = self.norm_layers[i](subband_spectrograms[i])\n",
    "            fc_output = self.fc_layers[i](norm_output)\n",
    "            subband_features.append(fc_output)\n",
    "\n",
    "        Z = torch.stack(subband_features, dim=1)\n",
    "        Z = Z.permute(0,1,3,2)\n",
    "        return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([20,20,20,30,30,30,30,30,30,30,30,30,30,50,50,50,50,70,70,100,100,125],128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "free_memory(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_in = torch.randn(16,2,1025,431,2).to('cuda:0').double()\n",
    "ex_chromas = torch.randn(32,12,431).to('cuda:0').double()\n",
    "ex_mfccs = torch.randn(32,32,431).to('cuda:0').double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = torch.randn(32,22,1025,431).to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_out = model(ex_in, ex_chromas, ex_mfccs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandwidths = [20,20,20,30,30,30,30,30,30,30,30,30,30,50,50,50,50,70,70,100,100,125]\n",
    "N = 130"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_out = torch.randn(32,22,130,430)\n",
    "masks = MaskEstimation(bandwidths, N, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = masks(ex_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, hidden_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.MLP = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, out_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.MLP(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class MaskEstimation(nn.Module):\n",
    "    def __init__(self, bandwidths, N, batch_size):\n",
    "        super(MaskEstimation, self).__init__()\n",
    "        self.num_bands = len(bandwidths)\n",
    "        self.bandwidths = bandwidths\n",
    "        self.batch_size = batch_size\n",
    "        self.norm_layers = torch.nn.ModuleList([torch.nn.LayerNorm(N) for bandwidth in self.bandwidths])\n",
    "        self.MLP_layers = torch.nn.ModuleList([MLP(N, bandwidth * 2, N * 2) for bandwidth in self.bandwidths])\n",
    "    def forward(self, x):\n",
    "        # Input shape: (batch_size, num_bands, N, T)\n",
    "        time_steps = x.shape[3] + 1\n",
    "        x = torch.cat((x,x[:,:,:,-1].unsqueeze(3)), 3)\n",
    "        x = x.permute(1, 0 , 3, 2)\n",
    "        out = []\n",
    "        print(\"TESTING\")\n",
    "        print(self.num_bands)\n",
    "        # shape: (num_bands, batch_size, T, N)\n",
    "        for i in range(self.num_bands):\n",
    "            y = self.norm_layers[i](x[i])\n",
    "            print(y.shape)\n",
    "            y = self.MLP_layers[i](y)\n",
    "            print(y.shape)\n",
    "            out.append(y)\n",
    "        out = torch.cat(out, 2)\n",
    "        print(\"OUT SHAPE: \", out.shape)\n",
    "        out = out.reshape(self.batch_size // 2, 2, sum(self.bandwidths), time_steps, 2)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BandSplit(torch.nn.Module):\n",
    "    # Input shape: torch.Size([batch_size, num_channels, freq_bins, time_steps, 2])\n",
    "    def __init__(self, bandwidths, N):\n",
    "        # bandwidths: list of integers representing the number of frequency bins for each subband\n",
    "        # N: the number of neurons in the fully connected layers\n",
    "        super(BandSplit, self).__init__()\n",
    "        self.bandwidths = bandwidths\n",
    "        # Initialize a LayerNorm module for each subband\n",
    "        self.norm_layers = torch.nn.ModuleList([torch.nn.LayerNorm(2 * bandwidth) for bandwidth in self.bandwidths])\n",
    "        # Initialize a fully connected module for each subband\n",
    "        self.fc_layers = torch.nn.ModuleList([torch.nn.Linear(2 * bandwidth, N) for bandwidth in self.bandwidths])\n",
    "\n",
    "    def forward(self, X):\n",
    "        subband_spectrograms = []\n",
    "        K = len(self.bandwidths)\n",
    "        # Iterate through each subband\n",
    "        for i in range(K):\n",
    "            # Find the start and end indices of the frequency bins for the current subband\n",
    "            start_index = sum(self.bandwidths[:i])\n",
    "            end_index = start_index + self.bandwidths[i]\n",
    "            # Extract the subband spectrogram from the input tensor\n",
    "            subband_spectrogram = X[:, :,start_index:end_index, :]\n",
    "            # Permute and reshape the subband spectrogram to prepare it for the LayerNorm and fully connected layers\n",
    "            subband_spectrogram = subband_spectrogram.permute(0,1,4,2,3)\n",
    "            subband_spectrogram = subband_spectrogram.reshape(2 * X.shape[0], X.shape[3], 2 * self.bandwidths[i])\n",
    "            subband_spectrograms.append(subband_spectrogram)\n",
    "\n",
    "        subband_features = []\n",
    "        # Iterate through\n",
    "        subband_features = []\n",
    "        for i in range(K):\n",
    "            # Apply normalization to subband spectrograms\n",
    "            norm_output = self.norm_layers[i](subband_spectrograms[i])\n",
    "            # Apply linear layer to the normalized subband spectrograms\n",
    "            fc_output = self.fc_layers[i](norm_output)\n",
    "            # Append the output of the linear layer as a feature for this subband\n",
    "            subband_features.append(fc_output)\n",
    "        # Stack the subband features together along a new dimension\n",
    "        Z = torch.stack(subband_features, dim=1)\n",
    "        # Permute the dimensions to match the desired output shape\n",
    "        Z = Z.permute(0,1,3,2)\n",
    "        return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del outex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "u0odoBGcxvyv"
   },
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    # we define a custom function that overrides the PyTorch built in collate in order to make sure\n",
    "    # our input to the model matches the dimensionality we want.\n",
    "    # our input to the function is a list of (input, label) tuples. The list will have size = batch_size as defined in the DataLoader\n",
    "    # We will use a batch_size of 1 to receive a list of tuples (in this case a list with 1 tuple), each tuple has two elements:\n",
    "    # First, the input, a tensor with shape (batch_size, stft_dim_F, stft_dim_T, 2)\n",
    "    # Second, the label, a tensor with shape (num_stems - 1, batch_size, stft_dim_F, stft_dim_T, 2)\n",
    "    input, chromas, mfccs, labels = batch[0]\n",
    "    return input, chromas, mfccs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = Transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import musdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "SHORTESTDUR:  7204864\n",
      "NUMSEGMENTS:  33\n",
      "NUMSEGMENTS:  tensor(26, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "musValidation = MusdbDataset(T, \"musdb18/\", \"valid\", \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(musTraining, batch_size=1, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min([int(track.duration * 44100) for track in mus.tracks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "musTraining.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "SHORTESTDUR:  5019648\n",
      "NUMSEGMENTS:  23\n",
      "NUMSEGMENTS:  tensor(18, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "musTraining = MusdbDataset(T, \"musdb18/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install musdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validationloader = torch.utils.data.DataLoader(musValidation, batch_size=1, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(list((musTraining.durations.values()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(list((musValidation.durations.values()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "musTraining.find_batch_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 919
    },
    "id": "F0WM94k-FTuk",
    "outputId": "d8f3a363-b549-4009-c6f3-ac1df4ddcffb"
   },
   "outputs": [],
   "source": [
    "\n",
    "y = 0\n",
    "for x in trainloader:\n",
    "    y = x\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, train_dataset, validation_dataset, summary_writer,\n",
    "             optimizer, model, learning_rate, loss, epochs = 1000, report_loss_frequency=15):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING EPOCH 1\n",
      "STEMS SHAPE : torch.Size([5, 7552000, 2])\n",
      "5019648\n",
      "<class 'torch.Tensor'>\n",
      "SEGMENT SHAPE:  torch.Size([22, 5, 441000, 2])\n",
      "STFTs SHAPE:  torch.Size([16, 5, 2, 1025, 431, 2])\n",
      "123    :  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7720/4252175868.py:53: FutureWarning: Pass y=[[-3.57055664e-03 -9.73510742e-03 -3.96728516e-03 ... -2.18963623e-01\n",
      "  -2.19665527e-01 -2.24182129e-01]\n",
      " [-2.10296631e-01 -2.27508545e-01 -2.01507568e-01 ...  1.07635498e-01\n",
      "   5.70678711e-02  1.15875244e-01]\n",
      " [-1.22070312e-04 -1.83105469e-04 -1.52587891e-04 ... -9.46044922e-04\n",
      "  -1.31225586e-03 -1.09863281e-03]\n",
      " ...\n",
      " [ 2.74658203e-03 -6.71386719e-04  2.68554688e-03 ...  5.79833984e-04\n",
      "   3.05175781e-04  7.32421875e-04]\n",
      " [-2.75695801e-01 -2.77801514e-01 -2.75756836e-01 ...  7.55920410e-02\n",
      "   7.55310059e-02  7.54699707e-02]\n",
      " [ 7.50732422e-02  7.51647949e-02  7.46154785e-02 ...  2.30285645e-01\n",
      "   2.30102539e-01  2.30010986e-01]] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  return torch.as_tensor(librosa.feature.chroma_stft(x.detach().cpu().numpy(), sr=sr, n_fft = n_fft, hop_length = hop_length, win_length = win_length, window = window), device = device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1234   : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7720/4252175868.py:50: FutureWarning: Pass y=[[-3.57055664e-03 -9.73510742e-03 -3.96728516e-03 ... -2.18963623e-01\n",
      "  -2.19665527e-01 -2.24182129e-01]\n",
      " [-2.10296631e-01 -2.27508545e-01 -2.01507568e-01 ...  1.07635498e-01\n",
      "   5.70678711e-02  1.15875244e-01]\n",
      " [-1.22070312e-04 -1.83105469e-04 -1.52587891e-04 ... -9.46044922e-04\n",
      "  -1.31225586e-03 -1.09863281e-03]\n",
      " ...\n",
      " [ 2.74658203e-03 -6.71386719e-04  2.68554688e-03 ...  5.79833984e-04\n",
      "   3.05175781e-04  7.32421875e-04]\n",
      " [-2.75695801e-01 -2.77801514e-01 -2.75756836e-01 ...  7.55920410e-02\n",
      "   7.55310059e-02  7.54699707e-02]\n",
      " [ 7.50732422e-02  7.51647949e-02  7.46154785e-02 ...  2.30285645e-01\n",
      "   2.30102539e-01  2.30010986e-01]] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  return torch.as_tensor(librosa.feature.melspectrogram(x.detach().cpu().numpy(), sr = 44100, n_fft = n_fft, hop_length = hop_length, window = window, n_mels = n_mels, win_length = win_length))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12345  : 0\n",
      "123    :  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7720/4252175868.py:53: FutureWarning: Pass y=[[ 6.56738281e-02  5.06591797e-02  7.41882324e-02 ...  2.91442871e-02\n",
      "  -1.56250000e-02  3.45153809e-02]\n",
      " [-2.35290527e-02  3.35083008e-02 -2.89916992e-02 ...  6.47888184e-02\n",
      "   8.33129883e-02  6.20117188e-02]\n",
      " [ 9.15527344e-05  6.40869141e-04  3.05175781e-04 ...  2.01416016e-03\n",
      "   9.76562500e-04  1.83105469e-03]\n",
      " ...\n",
      " [ 5.45959473e-02  1.59912109e-02  5.89294434e-02 ...  2.88696289e-02\n",
      "   1.73034668e-02  2.65502930e-02]\n",
      " [ 3.72894287e-01  2.94708252e-01  3.75885010e-01 ...  4.29382324e-02\n",
      "  -2.44140625e-03  4.08630371e-02]\n",
      " [-4.45556641e-03  3.84216309e-02 -5.40161133e-03 ...  4.76379395e-02\n",
      "   7.63854980e-02 -5.76782227e-03]] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  return torch.as_tensor(librosa.feature.chroma_stft(x.detach().cpu().numpy(), sr=sr, n_fft = n_fft, hop_length = hop_length, win_length = win_length, window = window), device = device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1234   : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7720/4252175868.py:50: FutureWarning: Pass y=[[ 6.56738281e-02  5.06591797e-02  7.41882324e-02 ...  2.91442871e-02\n",
      "  -1.56250000e-02  3.45153809e-02]\n",
      " [-2.35290527e-02  3.35083008e-02 -2.89916992e-02 ...  6.47888184e-02\n",
      "   8.33129883e-02  6.20117188e-02]\n",
      " [ 9.15527344e-05  6.40869141e-04  3.05175781e-04 ...  2.01416016e-03\n",
      "   9.76562500e-04  1.83105469e-03]\n",
      " ...\n",
      " [ 5.45959473e-02  1.59912109e-02  5.89294434e-02 ...  2.88696289e-02\n",
      "   1.73034668e-02  2.65502930e-02]\n",
      " [ 3.72894287e-01  2.94708252e-01  3.75885010e-01 ...  4.29382324e-02\n",
      "  -2.44140625e-03  4.08630371e-02]\n",
      " [-4.45556641e-03  3.84216309e-02 -5.40161133e-03 ...  4.76379395e-02\n",
      "   7.63854980e-02 -5.76782227e-03]] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  return torch.as_tensor(librosa.feature.melspectrogram(x.detach().cpu().numpy(), sr = 44100, n_fft = n_fft, hop_length = hop_length, window = window, n_mels = n_mels, win_length = win_length))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12345  : 1\n",
      "Chromas SHAPE:  torch.Size([32, 12, 431])\n",
      "DATA : torch.Size([16, 2, 1025, 431, 2])  LABELS:  torch.Size([16, 4, 2, 1025, 431, 2])\n",
      "torch.Size([16, 2, 1025, 431, 2]) torch.Size([32, 12, 431]) torch.Size([32, 32, 431]) torch.Size([16, 4, 2, 1025, 431, 2])\n",
      "1 :  torch.Size([16, 2, 1025, 431, 2])\n",
      "2 :  torch.Size([32, 22, 128, 431])\n",
      "3 :  torch.Size([32, 22, 63, 70])\n",
      "BAND INPUT SHAPE:  torch.Size([32, 22, 4410])\n",
      "torch.Size([32, 22, 70, 63])\n",
      "TEMPORAL INPUT SHAPE:  torch.Size([32, 70, 1386])\n",
      "torch.Size([32, 22, 70, 64])\n",
      "4 :  torch.Size([32, 22, 64, 70])\n",
      "BAND INPUT SHAPE:  torch.Size([32, 22, 6720])\n",
      "torch.Size([32, 22, 70, 96])\n",
      "TEMPORAL INPUT SHAPE:  torch.Size([32, 70, 2112])\n",
      "torch.Size([32, 22, 70, 64])\n",
      "5 :  torch.Size([32, 22, 64, 70])\n",
      "torch.Size([32, 22, 12, 70])\n",
      "torch.Size([32, 22, 76, 70])\n",
      "BAND INPUT SHAPE:  torch.Size([32, 22, 5320])\n",
      "torch.Size([32, 22, 70, 76])\n",
      "TEMPORAL INPUT SHAPE:  torch.Size([32, 70, 1672])\n",
      "torch.Size([32, 22, 70, 64])\n",
      "6 :  torch.Size([32, 22, 64, 70])\n",
      "torch.Size([32, 22, 64, 70])\n",
      "TEST:  torch.Size([32, 22, 130, 430])\n",
      "TESTING\n",
      "22\n",
      "torch.Size([32, 431, 130])\n",
      "torch.Size([32, 431, 40])\n",
      "torch.Size([32, 431, 130])\n",
      "torch.Size([32, 431, 40])\n",
      "torch.Size([32, 431, 130])\n",
      "torch.Size([32, 431, 40])\n",
      "torch.Size([32, 431, 130])\n",
      "torch.Size([32, 431, 60])\n",
      "torch.Size([32, 431, 130])\n",
      "torch.Size([32, 431, 60])\n",
      "torch.Size([32, 431, 130])\n",
      "torch.Size([32, 431, 60])\n",
      "torch.Size([32, 431, 130])\n",
      "torch.Size([32, 431, 60])\n",
      "torch.Size([32, 431, 130])\n",
      "torch.Size([32, 431, 60])\n",
      "torch.Size([32, 431, 130])\n",
      "torch.Size([32, 431, 60])\n",
      "torch.Size([32, 431, 130])\n",
      "torch.Size([32, 431, 60])\n",
      "torch.Size([32, 431, 130])\n",
      "torch.Size([32, 431, 60])\n",
      "torch.Size([32, 431, 130])\n",
      "torch.Size([32, 431, 60])\n",
      "torch.Size([32, 431, 130])\n",
      "torch.Size([32, 431, 60])\n",
      "torch.Size([32, 431, 130])\n",
      "torch.Size([32, 431, 100])\n",
      "torch.Size([32, 431, 130])\n",
      "torch.Size([32, 431, 100])\n",
      "torch.Size([32, 431, 130])\n",
      "torch.Size([32, 431, 100])\n",
      "torch.Size([32, 431, 130])\n",
      "torch.Size([32, 431, 100])\n",
      "torch.Size([32, 431, 130])\n",
      "torch.Size([32, 431, 140])\n",
      "torch.Size([32, 431, 130])\n",
      "torch.Size([32, 431, 140])\n",
      "torch.Size([32, 431, 130])\n",
      "torch.Size([32, 431, 200])\n",
      "torch.Size([32, 431, 130])\n",
      "torch.Size([32, 431, 200])\n",
      "torch.Size([32, 431, 130])\n",
      "torch.Size([32, 431, 250])\n",
      "OUT SHAPE:  torch.Size([32, 431, 2050])\n",
      "TEST RETURN :  torch.Size([16, 2, 1025, 431, 2])\n",
      "PREDICTION SHAPE:  torch.Size([16, 2, 1025, 431, 2])\n",
      "PREDICTION SHAPE:  torch.Size([16, 4, 2, 1025, 431, 2])\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.46 GiB (GPU 0; 23.64 GiB total capacity; 16.08 GiB already allocated; 468.25 MiB free; 21.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m timestamp \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m writer \u001b[38;5;241m=\u001b[39m SummaryWriter(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mruns/train_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(timestamp))\n\u001b[0;32m----> 4\u001b[0m train \u001b[38;5;241m=\u001b[39m \u001b[43mTrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmusTraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmusValidation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSSloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreport_loss_frequency\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 19\u001b[0m, in \u001b[0;36mTrain.__init__\u001b[0;34m(self, train_dataset, validation_dataset, collate, summary_writer, optimizer, model, learning_rate, loss, epochs, report_loss_frequency)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreport_loss_frequency \u001b[38;5;241m=\u001b[39m report_loss_frequency\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 65\u001b[0m, in \u001b[0;36mTrain.run_model\u001b[0;34m(self, epochs)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# We want to learn in our learning loops, so we make sure our gradients are updating during each epoch.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 65\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_number\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# We don't need or want the gradient opertaion on any of the \u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[17], line 41\u001b[0m, in \u001b[0;36mTrain.train_one_epoch\u001b[0;34m(self, epoch_number)\u001b[0m\n\u001b[1;32m     39\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(predicted_stems_stft, real_stems_stft[:,\u001b[38;5;241m3\u001b[39m])\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# We calculate the gradient\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Then update the weights based on the gradient\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.46 GiB (GPU 0; 23.64 GiB total capacity; 16.08 GiB already allocated; 468.25 MiB free; 21.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.001)\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/train_{}'.format(timestamp))\n",
    "train = Train(musTraining, musValidation, collate, writer, optimizer, model, 0.001, SSloss, epochs = 1, report_loss_frequency = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "94EMbmuf8TsS"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is the file to retreive the data from MusDB, it allows us to choose between the HQ dataset with .wav files\n",
    "or the compressed dataset with .stem.mp4 files. It also allows us to choose between the train and test subsets of the dataset.\n",
    "The class MusdbDataset is a subclass of torch.utils.data.Dataset, which allows us to use the PyTorch DataLoader class\n",
    "to load the data in batches. The __init__ method initializes the dataset, and the __getitem__ method returns the data\n",
    "for a given index. The __len__ method returns the length of the dataset.\n",
    "\n",
    "NOTES TO SELF: Fix threshold to set only off mix source\n",
    "                OTHER functions than RMS\n",
    "                Make self.batch_size\n",
    "\"\"\"\n",
    "\n",
    "import musdb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# Padding, constant batch size\n",
    "\n",
    "\n",
    "class MusdbDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, transforms, musdb_root, split='train', subset='train', is_wav=False, sample_rate=44100, segment_length = 10, segment_chunks = 10, discard_low_energy = True, segment_overlap = 0.5, drop_percentile =  0.1, chunks_below_percentile = 0.5):\n",
    "        # Check if a GPU is available\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.device(\"cuda\")\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "        assert(subset == 'train' or subset == 'test')\n",
    "        self.mode = subset\n",
    "        self.mus = musdb.DB(musdb_root, subsets=subset, split=split, is_wav=is_wav)\n",
    "        self.transforms = transforms\n",
    "        self.sample_rate = sample_rate\n",
    "        self.split = split\n",
    "        self.discard_low_energy = discard_low_energy\n",
    "        self.segment_length = segment_length\n",
    "        self.segment_chunks = segment_chunks\n",
    "        self.chunks_below_percentile = chunks_below_percentile\n",
    "        self.segment_overlap = segment_overlap\n",
    "        self.drop_percentile = drop_percentile\n",
    "        self.segment_samples = int(self.segment_length * self.sample_rate)\n",
    "        self.stft_size = self.STFT_dimensions(self.segment_samples)\n",
    "        self.chunk_samples = int(self.segment_samples / self.segment_chunks)\n",
    "        self.num_stems = self.mus.tracks[0].stems.shape[0]\n",
    "        self.num_channels = self.mus.tracks[0].stems.shape[2]\n",
    "        self.durations = dict()\n",
    "        self.filtered_indices = dict()\n",
    "        self.len = self.init_durations()\n",
    "        self.shortest_duration = self.shortest_duration_in_samples(self.mus)\n",
    "        self.batch_size = self.find_batch_size()\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # this function should return a batch of segment STFTs from the song as well as their stem STFTs.\n",
    "        track = self.mus.tracks[self.filtered_indices[idx]]\n",
    "        # stems is a list of the stems of the track, in the order of the stems in the track\n",
    "        stems = torch.as_tensor(track.stems, device = self.device)\n",
    "        print(\"STEMS SHAPE :\", stems.shape)\n",
    "        if self.mode == 'train':\n",
    "            segments = self._batchize_training_item(stems)\n",
    "        else:\n",
    "            segments = self._batchize_testing_item(stems)\n",
    "        print(\"SEGMENT SHAPE: \", segments.shape)\n",
    "        # we apply a STFT to each of those segments, and that is how we achieve our constant batch and input sizes.\n",
    "        stfts = self.stft_segments(segments)\n",
    "        print(\"STFTs SHAPE: \", stfts.shape)\n",
    "        chromas, mfccs = self.chroma_and_mfcc_segments(segments)\n",
    "        print(\"Chromas SHAPE: \", chromas.shape)\n",
    "        # we then return the segments as a torch tensor\n",
    "        data = stfts[:,0]\n",
    "        labels = stfts[:,1:]\n",
    "        mfccs\n",
    "        print(\"DATA :\", data.shape, \" LABELS: \", labels.shape)\n",
    "        return data, chromas, mfccs, labels\n",
    "\n",
    "    def _batchize_training_item(self,stems):\n",
    "        # we need to trim the stems to the shortest duration track, starting from a random location\n",
    "        stems = self.trim_stems(stems, self.random_start(stems.shape[1]))\n",
    "        # we split the stems from shape (num_stems, num_samples, num_channels) into a tensor with shape (num_stems, num_segments, num_samples_per_segment, num_channels)\n",
    "        segments = self.split_track(stems)\n",
    "        # now we need to drop out the low energy segments\n",
    "        print(type(segments))\n",
    "        segments = self.high_energy_segments(segments)\n",
    "        # then we choose a random and continuous yet constant number of segments from the track minus the dropped segments\n",
    "        return segments\n",
    "\n",
    "    def _batchize_testing_item(self,stems):\n",
    "        # first we make sure the length of the song will produce a whole number of segments by padding with zeros to the end of the next segment.\n",
    "        stems = self.add_zero_padding(stems)\n",
    "        # now we extend the song with zeros to make sure we have an equal batch size for every output.\n",
    "        to_pad = self.longest_duration_in_samples() - stems.shape[1]\n",
    "        stems = self.add_N_zeros(stems, to_pad)\n",
    "        # now we split the stems into equal size segments\n",
    "        segments = self.split_track(stems)\n",
    "        # we are now ready to operate on the song since we don't want to drop or modify our data as we conserve it to reconstruct our signal.\n",
    "        return segments\n",
    "\n",
    "    def collate(batch):\n",
    "        # we define a custom function that overrides the PyTorch built in collate in order to make sure\n",
    "        # our input to the model matches the dimensionality we want.\n",
    "        # our input to the function is a list of (input, label) tuples. The list will have size = batch_size as defined in the DataLoader\n",
    "        # We will use a batch_size of 1 to receive a list of tuples (in this case a list with 1 tuple), each tuple has two elements:\n",
    "        # First, the input, a tensor with shape (batch_size, stft_dim_F, stft_dim_T, 2)\n",
    "        # Second, the label, a tensor with shape (num_stems - 1, batch_size, stft_dim_F, stft_dim_T, 2)\n",
    "        input, chromas, mfccs, labels = batch[0]\n",
    "        return input, chromas, mfccs, labels\n",
    "\n",
    "    def init_durations(self):\n",
    "        pos = 0\n",
    "        for idx, track in enumerate(self.mus.tracks):\n",
    "            print(idx)\n",
    "            self.durations[idx] = track.stems.shape[1]\n",
    "            if self.durations[idx] >= self.segment_samples * 9:\n",
    "                self.filtered_indices[pos] = idx\n",
    "                pos += 1\n",
    "        return pos + 1\n",
    "\n",
    "    def stft_segments(self, segments):\n",
    "        # this function should take in a list of segments and apply a STFT to each of them\n",
    "        # it should return a tensor of STFTs, of shape (num_stems, batch_size, STFT_F, STFT_T, num_channels)\n",
    "        # where batch_size is the number of STFTs we can fit into a batch\n",
    "        # STFT_F is the number of frequency bins in the STFT\n",
    "        # STFT_T is the number of time bins in the STFT\n",
    "        # num_stems is the number of stems in the track\n",
    "        # num_channels is the number of channels in the track\n",
    "        # the STFTs should be applied to each segment, and then the segments should be concatenated along the batch_size axis\n",
    "        stem_stfts = []\n",
    "        batch_size = self.batch_size\n",
    "        # we then keep only the first batch_size segments, and apply a STFT to each of them. \n",
    "        all_segments = segments[:batch_size, :, :, :].view(self.num_stems, self.num_channels, batch_size, self.segment_samples)\n",
    "        for i in range(self.num_stems):\n",
    "            channel_stfts = []\n",
    "            # we take the first batch of stfts, we reshape it to allow for stft operation using torch.stft and taking advantage of 2D batched STFTs\n",
    "            for j in range(self.num_channels): \n",
    "                # we apply a STFT to each segment\n",
    "                stem_batch_channel_segments = all_segments[i,j,:,:]\n",
    "                batch_stfts = self.transforms.stft(stem_batch_channel_segments)\n",
    "                channel_stfts.append(batch_stfts)\n",
    "                # we then add the STFT to the list of STFTs\n",
    "            stem_stfts.append(torch.stack(channel_stfts))\n",
    "        stem_stfts = torch.stack(stem_stfts)\n",
    "        return stem_stfts.view(batch_size, self.num_stems, self.num_channels, self.stft_size[0], self.stft_size[1], 2)\n",
    "        \n",
    "    def chroma_and_mfcc_segments(self, segments):\n",
    "        # this function should take in a list of segments and apply a STFT to each of them\n",
    "        # it should return a tensor of STFTs, of shape (num_stems, batch_size, STFT_F, STFT_T, num_channels)\n",
    "        # where batch_size is the number of STFTs we can fit into a batch\n",
    "        # STFT_F is the number of frequency bins in the STFT\n",
    "        # STFT_T is the number of time bins in the STFT\n",
    "        # num_stems is the number of stems in the track\n",
    "        # num_channels is the number of channels in the track\n",
    "        # the STFTs should be applied to each segment, and then the segments should be concatenated along the batch_size axis\n",
    "        batch_size = self.batch_size\n",
    "        # we then keep only the first batch_size segments, and apply a STFT to each of them. \n",
    "        all_segments = segments[:batch_size, :, :, :].view(self.num_stems, self.num_channels, batch_size, self.segment_samples)\n",
    "        channel_chromas = []\n",
    "        channel_mfccs = []\n",
    "        # we take the first batch of stfts, we reshape it to allow for stft operation using torch.stft and taking advantage of 2D batched STFTs\n",
    "        for j in range(self.num_channels): \n",
    "            print(\"123    : \", j)\n",
    "            # we apply a STFT to each segment\n",
    "            stem_batch_channel_segments = all_segments[0,j,:,:]\n",
    "            batch_chromas = self.transforms.chromagram(stem_batch_channel_segments)\n",
    "            print(\"1234   :\", j)\n",
    "            batch_mfccs = self.transforms.mel_spectrogram(stem_batch_channel_segments)\n",
    "            print(\"12345  :\", j)\n",
    "            channel_chromas.append(batch_chromas)\n",
    "            channel_mfccs.append(batch_mfccs)\n",
    "            # we then add the STFT to the list of STFTs\n",
    "        channel_mfccs = torch.stack(channel_mfccs)\n",
    "        channel_chromas = torch.stack(channel_chromas)\n",
    "        return channel_chromas.view(batch_size * self.num_channels, 12, self.stft_size[1]),  channel_mfccs.view(batch_size * self.num_channels, self.transforms.n_mels , self.stft_size[1])\n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def trim_stems(self, stems, start):\n",
    "        print(self.shortest_duration)\n",
    "        # this function should trim the track to the shortest duration of the track from the start index, it allows looping back across the song.\n",
    "        if start + self.shortest_duration > stems.shape[1]:\n",
    "            first_half = stems[:, start:, :]\n",
    "            remaining = self.shortest_duration - (stems.shape[1] - start)\n",
    "            second_half = stems[:, :remaining, :]\n",
    "            return torch.cat((first_half, second_half), axis=1)\n",
    "        else:\n",
    "            return stems[:, start:start+self.shortest_duration, :]\n",
    "\n",
    "    def pad_stems(self, stems):\n",
    "        length_in_samples = stems.shape[1]\n",
    "        to_pad = self.longest_duration_in_samples() - length_in_samples\n",
    "\n",
    "    def find_batch_size(self):\n",
    "        # this function should tell us how many STFTs we can fit into a batch based on finding the floor power of 2 of the number of STFTs we fit over the duration of the song\n",
    "        # each STFT will represent a STFT over a fixed segment length.\n",
    "        num_segments = self.num_segments_in_track(self.shortest_duration)\n",
    "        print(\"SHORTESTDUR: \", self.shortest_duration)\n",
    "        print(\"NUMSEGMENTS: \", num_segments)\n",
    "        # we anticipate a drop of up to twice the drop percentile (impossible, just to be safe) of the segments.\n",
    "        num_segments = torch.as_tensor(int(num_segments * (1 - 2 * self.drop_percentile)), device = self.device)\n",
    "        print(\"NUMSEGMENTS: \", num_segments)\n",
    "        # We return the closest power of two to that anticipated number of segments. Of course we use a floor because we want to fill every batch.\n",
    "        return 2 ** int(torch.floor(torch.log2(num_segments)))\n",
    "\n",
    "    def num_segments_in_track(self, duration_in_samples):\n",
    "        # this function should return the number of segments in the track and consider the overlap factor, self.segment_overlap\n",
    "        return int(torch.ceil(torch.as_tensor(duration_in_samples / (self.segment_samples * (1 - self.segment_overlap)), device=self.device)))\n",
    "\n",
    "    def random_start(self, duration_in_samples):\n",
    "        # this function should return a random start index for the track\n",
    "        return torch.randint(0, duration_in_samples, (1,))\n",
    "\n",
    "\n",
    "    def STFT_dimensions(self, duration_in_samples):\n",
    "        # this function should return the dimensions of the STFT of the shortest track in the dataset\n",
    "        F = int(self.transforms.n_fft / 2 + 1)\n",
    "        T = int(torch.ceil(torch.as_tensor(duration_in_samples / self.transforms.hop_length, device = self.device)))\n",
    "        return (F,T)\n",
    "        \n",
    "\n",
    "    def shortest_duration_in_samples(self, mus):\n",
    "        # this function should return the shortest duration of the stems in the track\n",
    "        min = 100000000\n",
    "        for i, dur in self.durations.items():\n",
    "            if i in self.filtered_indices.values():\n",
    "                if dur < min:\n",
    "                    min = dur\n",
    "        return min\n",
    "\n",
    "    def longest_duration_in_samples(self, mus):\n",
    "        # this function should return the longest duration of the stems in the track\n",
    "        return max(min([track.stems.shape[1] for track in mus.tracks]))\n",
    "\n",
    "    def is_high_energy_segment(self, segment, threshold):\n",
    "        # this function decides based on the provided threshold whether a sufficient number of chunks in the segment have an energy above the threshold\n",
    "        mix_chunk_energies = self.segment_chunk_energies(segment)[:]\n",
    "        return len(torch.argwhere(mix_chunk_energies > threshold)) > (self.chunks_below_percentile * self.segment_chunks)\n",
    "\n",
    "    def high_energy_segments(self, segments):\n",
    "        # this function should take in a full track's stems, it will then split the track into segments. \n",
    "        # The segments should have an overlap factor of self.segment_overlap.\n",
    "        # Then, it will split each segment into chunks, and it will calculate the energy of each segment, and store the energy of each segment in a list.\n",
    "        # With this list, it will calculate the percentile of the energy of the chunks, and it will discard the segments where 25% of the chunks have an energy below the percentile.\n",
    "        # It will then return the list of segments that have a high enough energy.\n",
    "        high_energy_indices = []\n",
    "        threshold = self.segment_energy_threshold(segments)\n",
    "        for idx, segment in enumerate(segments):\n",
    "            if self.is_high_energy_segment(segment, threshold):\n",
    "                high_energy_indices.append(idx)\n",
    "        high_energy_indices = torch.as_tensor(high_energy_indices, device = self.device)\n",
    "        return torch.index_select(segments, 0, high_energy_indices)\n",
    "\n",
    "    def segment_energy_threshold(self, segments):\n",
    "        # this function should split every segment into self.segment_chunks chunks, and it will calculate the energy of each chunk using the RMS energy function.\n",
    "        # It will save the energy of each chunk in a list, and it will return the value self.percentile_dropped percentile of the list.\n",
    "        chunk_energies = []\n",
    "        for segment in segments:\n",
    "            chunk_energies.extend(self.segment_chunk_energies(segment))\n",
    "        chunk_energies = torch.stack(chunk_energies)\n",
    "        percentile = torch.quantile(chunk_energies, self.drop_percentile, interpolation='midpoint')\n",
    "        return percentile\n",
    "            \n",
    "    def segment_chunk_energies(self, segment):\n",
    "        # this function should split the segment into self.segment_chunks chunks, and it will calculate the energy of each chunk using the RMS energy function.\n",
    "        # It will save the energy of each chunk in a list, and it will return the list.\n",
    "        chunk_energies = []\n",
    "        segment_samples = segment.shape[1]\n",
    "        chunk_samples = int(segment_samples / self.segment_chunks)\n",
    "        for i in range(0, segment_samples, chunk_samples):\n",
    "            chunk = segment[:, i:i+chunk_samples]\n",
    "            mix_track = chunk[0,:,:]\n",
    "            rms = self.transforms.RMS(mix_track)\n",
    "            chunk_energies.append(rms)\n",
    "        return torch.stack(chunk_energies).view(self.segment_chunks)\n",
    "\n",
    "    def split_track(self, stems):\n",
    "        # this function should take in a full track, and it will split the track into segments. \n",
    "        # The segments should have an overlap factor of self.segment_overlap.\n",
    "        # We add zero padding to the track to make sure that the track is divisible by the segment length.\n",
    "        # Then, it will split each segment into chunks, and it will return the list of chunks.\n",
    "        # The input is a tensor with shape (num_stems, num_samples, num_channels)\n",
    "        # The output is a tensor array with shape (num_stems, num_segments, num_samples_per_segment, num_channels)\n",
    "        stems = self.add_zero_padding(stems)\n",
    "        segments = []\n",
    "        num_samples = stems.shape[1]\n",
    "        step_in_samples = int(self.segment_samples * (1 - self.segment_overlap))\n",
    "        for i in range(0, num_samples - step_in_samples, step_in_samples):\n",
    "            segment = stems[:, i:i+self.segment_samples]\n",
    "            segments.append(segment)\n",
    "        segments = torch.stack(segments)\n",
    "        return segments\n",
    "\n",
    "    def add_zero_padding(self, stems):\n",
    "        # this function should add zero padding to the track to make sure that the track is divisible by the segment length and the residue from the overlap.\n",
    "        # the length of the array has to be segment_length + k * samples_in_steps for some nonnegative integer k.\n",
    "        num_samples = stems.shape[1]\n",
    "        step_in_samples = int(self.segment_samples * (1 - self.segment_overlap))\n",
    "        samples_in_last_segment = num_samples % step_in_samples\n",
    "        if samples_in_last_segment != 0:\n",
    "            padding = torch.zeros((stems.shape[0], step_in_samples - samples_in_last_segment, stems.shape[2]), device = self.device)\n",
    "            return torch.cat((stems, padding), axis=1)\n",
    "        else:\n",
    "            return stems\n",
    "\n",
    "    def add_N_zeros(self, stems, N):\n",
    "        zeros = torch.zeros((stems.shape[0], N, stems.shape[2]))\n",
    "        return torch.cat((stems, zeros), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ZDph6Wtw8kL0"
   },
   "outputs": [],
   "source": [
    "#This file will hold the loss functions for each loss I will test, including a composite loss function\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def SSloss(predicted_stfts, real_stfts):\n",
    "    # This function will calculate the loss for a given batch of predicted and real stems\n",
    "    # The loss will be the sum of the losses for each stem\n",
    "    # the dimensions of the predicted stems and real stems will be (batch_size, num_stems, stft_dim_F, stft_dim_T,num_channels)\n",
    "    # The loss will be a scalar\n",
    "    stft_loss = stft_mean_absolute_error(predicted_stfts, real_stfts)\n",
    "    # We want to find the signal representation from the STFT we computed. We can do this by taking the inverse STFT\n",
    "    # of each segment (input in the batch) separately and compare them. We don't need perfect reconstruction for our loss function,\n",
    "    # but we will need it to evaluate the model.\n",
    "    predicted_time_signals = batch_time_signals(predicted_stfts)\n",
    "    real_time_signals = batch_time_signals(real_stfts)\n",
    "    signal_loss = signal_mean_absolute_error(predicted_time_signals, real_time_signals)\n",
    "    return signal_loss + stft_loss\n",
    "\n",
    "def batch_time_signals(stfts):\n",
    "    # This function should compute the time signals for each segment in the batch\n",
    "    # input shape (batch_size, num_stems, stft_dim_F, stft_dim_T,num_channels)\n",
    "    # output shape (batch_size, num_stems, segment_samples, num_channels)\n",
    "    istft_list = []\n",
    "    for i in range(stfts.shape[0]):\n",
    "        for j in range(stfts.shape[1]):\n",
    "            istft = torch.istft(stfts[i, j], n_fft=2048, hop_length=1024, win_length=2048, window=torch.hann_window(2048, device = torch.device(\"cuda\")))\n",
    "            istft_list.append(istft)\n",
    "    istfts = torch.stack(istft_list, dim=0)\n",
    "    return istfts\n",
    "\n",
    "    \n",
    "    \n",
    "def signal_mean_absolute_error(predicted_time_signal, real_time_signal):\n",
    "    # This function will calculate the mean absolute error between the predicted and real time signals\n",
    "    # The dimensions of the predicted and real time signals will be (batch_size, num_stems, stft_dim_F, stft_dim_T,num_channels)\n",
    "    return torch.mean(torch.abs(predicted_time_signal - real_time_signal))\n",
    "\n",
    "def stft_mean_absolute_error(predicted_stfts, real_stfts):\n",
    "    # This function will calculate the mean absolute error between the predicted and real STFTs\n",
    "    # The dimensions of the predicted and real STFTs will be (batch_size, num_stems, stft_dim_F, stft_dim_T,num_channels)\n",
    "    return torch.mean(torch.abs(predicted_stfts - real_stfts))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "gLvGC9MG88Mu"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import torch\n",
    "import librosa\n",
    "# import torchaudio\n",
    "\n",
    "class Transforms():\n",
    "    #def __init__(self, sample_rate=44100, n_fft=2048, hop_length=1024, win_length=2048, window='hann', center=True, pad_mode='reflect', power=2.0, n_mels=128, fmin=0.0, fmax=None, htk=False, norm=1, top_db=80.0, ref=1.0, amin=1e-10):\n",
    "    def __init__(self, sample_rate=44100, n_fft=2048, hop_length=1024, win_length=2048, window='hann', center=True, pad_mode='reflect', power=2.0, n_mels=32, fmin=0.0, fmax=None, htk=False, norm=1, top_db=80.0, ref=1.0, amin=1e-10):\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.device(\"cuda\")\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "        self.sample_rate = sample_rate\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.win_length = win_length\n",
    "        self.window = window\n",
    "        self.center = center\n",
    "        self.pad_mode = pad_mode\n",
    "        self.power = power\n",
    "        self.n_mels = n_mels\n",
    "        self.fmin = fmin\n",
    "        self.fmax = fmax\n",
    "        self.htk = htk\n",
    "        self.norm = norm\n",
    "        self.top_db = top_db\n",
    "        self.ref = ref\n",
    "        self.amin = amin\n",
    "        #self.mfcc_transform = torchaudio.transforms.MelSpectrogram(sample_rate = 44100, n_fft = 2048, win_length= 2048, hop_length=1024, n_mels=64)\n",
    "\n",
    "    def __call__(self, x): \n",
    "        #x = torch.from_numpy(x)\n",
    "        #x = torch.Tensor(x)\n",
    "        x = torch.tensor(x)\n",
    "        x = x.permute(1, 0)\n",
    "        x = self.stft(x)\n",
    "        x = self.mel_spectrogram(x)\n",
    "        x = self.amplitude_to_db(x)\n",
    "        return x\n",
    "\n",
    "    def stft(self, x, n_fft=2048, hop_length=1024, win_length=2048, window=torch.hann_window(2048, device = torch.device('cuda')), center=True, pad_mode='reflect'):\n",
    "        return torch.stft(x, n_fft=n_fft, hop_length=hop_length, win_length=win_length, window=window, center=center, pad_mode=pad_mode, onesided=True, return_complex=False)\n",
    "\n",
    "    def istft(self, x, n_fft=2048, hop_length=1024, win_length=2048, window=torch.hann_window(2048,device = torch.device('cuda')), center=True):\n",
    "        return torch.istft(x, n_fft=n_fft, hop_length=hop_length, win_length=win_length, window=window, center=center, onesided=True)\n",
    "\n",
    "    def mel_spectrogram(self, x, sr=44100, n_fft=2048, hop_length = 1024, win_length = 2048, window = 'hann', n_mels=32):\n",
    "       # return self.mfcc_transform(x)\n",
    "        return torch.as_tensor(librosa.feature.melspectrogram(x.detach().cpu().numpy(), sr = 44100, n_fft = n_fft, hop_length = hop_length, window = window, n_mels = n_mels, win_length = win_length))\n",
    "    def chromagram(self, x, sr = 44100, n_fft = 2048, hop_length = 1024, win_length = 2048, window = 'hann'):\n",
    "        device = x.get_device()\n",
    "        return torch.as_tensor(librosa.feature.chroma_stft(x.detach().cpu().numpy(), sr=sr, n_fft = n_fft, hop_length = hop_length, win_length = win_length, window = window), device = device)\n",
    "    def amplitude_to_db(self, x, top_db=80.0, ref=1.0, amin=1e-10):\n",
    "        return librosa.amplitude_to_db(x, top_db=top_db, ref=ref, amin=amin)\n",
    "\n",
    "    def RMS(self, audio_tensor):\n",
    "        # this function is used to calculate the RMS of the audio signal\n",
    "        # the input will be an audio tensor of shape (num_samples)\n",
    "        # Note that this function is intended to use on smaller audio signals, and that\n",
    "        # typically, longer signals get windows similar to the STFT in order to calculate more localized\n",
    "        # RMS. The purpose of using this was for silent signal detection.\n",
    "        squared_tensor = torch.pow(audio_tensor, 2)\n",
    "        mean_power = torch.mean(squared_tensor)\n",
    "        rms = torch.sqrt(mean_power)\n",
    "        return rms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "6xiLn4FA9EJF"
   },
   "outputs": [],
   "source": [
    "# This class should define the training loop for the model. We will define a training loop function\n",
    "# as well as a train_one_epoch function. This is based on the tutorial on the official PyTorch website: \n",
    "# https://pytorch.org/tutorials/beginner/introyt/trainingyt.html#the-training-loop\n",
    "# we will also implement our training via train/validate/test splits, and save our best models parameters.\n",
    "# I will use SummaryWriter, a useful class for reporting our data from PyTorch.\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "class Train():\n",
    "    def __init__(self, train_dataset, validation_dataset, collate, summary_writer, optimizer, model, learning_rate, loss, epochs = 1000, report_loss_frequency=15):\n",
    "        self.train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=1, collate_fn=collate)\n",
    "        self.validation_dataloader = torch.utils.data.DataLoader(validation_dataset, batch_size=1, collate_fn=collate)\n",
    "        self.writer = summary_writer\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss = loss\n",
    "        self.report_loss_frequency = report_loss_frequency\n",
    "        self.run_model(epochs=epochs)\n",
    "\n",
    "    def train_one_epoch(self, epoch_number):\n",
    "        accumulating_loss = 0.0\n",
    "        last_loss = 0.0\n",
    "        for i, data in enumerate(self.train_dataloader):\n",
    "            # We will use the PyTorch Data Loader class to easily iterate through the data and collect our batches.\n",
    "            # See musdb.py for a deeper explanation of how we collect batches. Simply speaking, we take a an arbitrary\n",
    "            # fixed-length part of the song and we split it into fixed length segments, taking the STFT of each. \n",
    "            mixture_stft, chromas, mfccs, real_stems_stft = data\n",
    "            print(mixture_stft.shape, chromas.shape, mfccs.shape, real_stems_stft.shape)\n",
    "            # PyTorch accumulates gradients by default, so we zero out the gradients before each batch update in order to \n",
    "            # ensure that the optimizer only uses the gradients from the current batch during the update step.\n",
    "            # This also should help with performance and memory costs.\n",
    "            self.optimizer.zero_grad()\n",
    "            # We call our model to make a prediction\n",
    "            predicted_stems_stft = self.model(mixture_stft, chromas, mfccs)\n",
    "            # We compute the loss from our loss function defined in loss.py\n",
    "            print(\"PREDICTION SHAPE: \", predicted_stems_stft.shape)\n",
    "            print(\"PREDICTION SHAPE: \", real_stems_stft.shape)\n",
    "            loss = self.loss(predicted_stems_stft, real_stems_stft[:,3])\n",
    "            # We calculate the gradient\n",
    "            loss.backward()\n",
    "            # Then update the weights based on the gradient\n",
    "            self.optimizer.step()\n",
    "            # In order to report our loss, we print the loss every 15 batches\n",
    "            accumulating_loss += loss.item()\n",
    "            if i % self.report_loss_frequency == self.report_loss_frequency - 1:\n",
    "                last_loss = accumulating_loss / self.report_loss_frequency\n",
    "                print(\"Batch {}. Average loss over last {} batches is: {}\".format(i+1, self.report_loss_frequency, last_loss))\n",
    "                writer_index = epoch_number * len(self.train_dataloader) + i + 1\n",
    "                self.writer.add_scalar('Loss/train', last_loss, writer_index)\n",
    "                accumulating_loss = 0\n",
    "            \n",
    "            return last_loss\n",
    "\n",
    "    def run_model(self, epochs):\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        epoch_number = 0\n",
    "        #set arbitrarily large initial validation loss\n",
    "        best_vloss = 2.0 ** 30\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            print(\"STARTING EPOCH {}\".format(epoch_number + 1))\n",
    "            # We want to learn in our learning loops, so we make sure our gradients are updating during each epoch.\n",
    "            self.model.train(True)\n",
    "            avg_loss = self.train_one_epoch(epoch_number)\n",
    "            # We don't need or want the gradient opertaion on any of the \n",
    "            self.model.train(False)\n",
    "\n",
    "            accumulating_vloss = 0.0\n",
    "\n",
    "            for i, vdata in enumerate(self.validation_dataloader):\n",
    "                vmixture_stft, vchromas, vmfccs, vreal_stems_stft = vdata\n",
    "                vpredicted_stems_stft = self.model(vmixture_stft, vchromas, vmfccs)\n",
    "                vloss = self.loss(vpredicted_stems_stft, vreal_stems_stft)\n",
    "                accumulating_vloss += vloss\n",
    "            \n",
    "            avg_vloss = accumulating_vloss / (i + 1)\n",
    "            print('LOSS training set: {} validation set: {}'.format(avg_loss, avg_vloss))\n",
    "            \n",
    "            # log all of this \n",
    "            self.writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "            self.writer.flush()\n",
    "\n",
    "            if avg_vloss < best_vloss:\n",
    "                best_vloss = avg_vloss\n",
    "                model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "                torch.save(self.model.state_dict(), model_path)\n",
    "            \n",
    "            epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "V-b-ssPg9IUR"
   },
   "outputs": [],
   "source": [
    "# I will use the museval package to calculate the SDR, SIR, and SAR scores as well as\n",
    "# the time invariant SDR, SIR SAR variants.\n",
    "# Importantly, this class will allow us to test the model on full songs. For training, we make some sacrifices to opitimize \n",
    "# computations and introduce randomization and we end up processing the song in parts over training. This is also because our reconstruction\n",
    "# task is to recreate the segments of the song that were tested and to compare only with those features.\n",
    "import torch\n",
    "import museval \n",
    "\n",
    "def load_model(model_path):\n",
    "    model = my_model()\n",
    "    model.load_state_dict(model_path)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "class Evaluate():\n",
    "    def __init__(self, test_dataset, model_path):\n",
    "        self.test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size = 1, collate_fn = collate)\n",
    "        self.model = load_model(model_path)\n",
    "        self.mus = test_dataset.mus\n",
    "\n",
    "    def reconstruct_all_and_evaluate(self):\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(self.test_dataloader):\n",
    "                mixture_stft, _ = data\n",
    "                predicted_stems_stft = self.model(mixture_stft)\n",
    "                self.reconstruct_one_and_evaluate(predicted_stems_stft, i)\n",
    "\n",
    "    def reconstruct_one_and_evaluate(self, predicted_stems_stfts, i): \n",
    "        stem_estimates_time_signals = self.reconstruct_one_song(predicted_stems_stfts)\n",
    "        self.evaluate_one_song(stem_estimates_time_signals, i)\n",
    "\n",
    "    def evaluate_one_song(self, stems_estimates_time_signals, i):\n",
    "        # this function will evaluate the song using the museval package.\n",
    "        # it will receive the time signals of the stems and the song number in the dataset.\n",
    "        # it will both save the results in the results and print the scores after evaluating each song.\n",
    "        scores = museval.eval_mus_track(self.mus[i], stems_estimates_time_signals, output_dir='results')   \n",
    "        print(scores)\n",
    "\n",
    "    def reconstruct_one_song(self, predicted_stems_stft):\n",
    "        # this function will reconstruct the song from the predicted stems stft and return the time signals of the stems.\n",
    "        # We will use the overlap and add method to reconstruct the song after we recover the time signal for each segment for each stem.\n",
    "        time_signals = batch_time_signals(predicted_stems_stft)\n",
    "        stem_estimates_time_signals = self.overlap_and_add(time_signals)\n",
    "        return stem_estimates_time_signals\n",
    "\n",
    "    def overlap_and_add(self, time_signals):\n",
    "        # receives all the reconstructed time signals and overlaps them into one signal to reconstruct the original song.\n",
    "        # this will happen based on the parameters in the dataset for segment length and overlap.\n",
    "        step_in_samples = time_signals.shape[2] * (1 - self.test_dataset.overlap)\n",
    "        segment_length = time_signals.shape[2]\n",
    "        num_segments = time_signals.shape[1]\n",
    "        full_signal_length = segment_length + step_in_samples * (num_segments - 1)\n",
    "        new_signal = torch.zeros((time_signals.shape[0], full_signal_length))\n",
    "        for i in range(num_segments):\n",
    "            new_signal[:, i * step_in_samples : i * step_in_samples + segment_length] += time_signals[:, i, :]\n",
    "        return new_signal\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5YGv-8DC9QJa"
   },
   "outputs": [],
   "source": [
    "zz = torch.Tensor(10043)\n",
    "zz.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hq21ry7MA51w",
    "outputId": "8589fb76-ade5-4d47-8ea3-e4df2fb011b8"
   },
   "outputs": [],
   "source": [
    "int(torch.as_tensor(0.43))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aUU8IL4xGxqe"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
