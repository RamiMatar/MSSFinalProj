{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f53e5147",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchaudio\n",
    "from torchaudio.transforms import *\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import musdb\n",
    "import museval \n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0be7414",
   "metadata": {},
   "outputs": [],
   "source": [
    "Resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed569c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 7299072, 2])\n",
      "torch.Size([1, 5, 7204864, 2])\n",
      "torch.Size([1, 5, 18461696, 2])\n",
      "torch.Size([1, 5, 7728128, 2])\n",
      "torch.Size([1, 5, 8741888, 2])\n",
      "torch.Size([1, 5, 11293696, 2])\n",
      "torch.Size([1, 5, 12604416, 2])\n",
      "torch.Size([1, 5, 10222592, 2])\n",
      "torch.Size([1, 5, 12459008, 2])\n",
      "torch.Size([1, 5, 11446272, 2])\n",
      "torch.Size([1, 5, 14310400, 2])\n",
      "torch.Size([1, 5, 18578432, 2])\n",
      "torch.Size([1, 5, 10438656, 2])\n",
      "torch.Size([1, 5, 12251136, 2])\n",
      "duration with n_workers = 1  RUNTIME:  40.0316321849823\n",
      "torch.Size([1, 5, 7299072, 2])\n",
      "torch.Size([1, 5, 7204864, 2])\n",
      "torch.Size([1, 5, 18461696, 2])\n",
      "torch.Size([1, 5, 7728128, 2])\n",
      "torch.Size([1, 5, 8741888, 2])\n",
      "torch.Size([1, 5, 11293696, 2])\n",
      "torch.Size([1, 5, 12604416, 2])\n",
      "torch.Size([1, 5, 10222592, 2])\n",
      "torch.Size([1, 5, 12459008, 2])\n",
      "torch.Size([1, 5, 11446272, 2])\n",
      "torch.Size([1, 5, 14310400, 2])\n",
      "torch.Size([1, 5, 18578432, 2])\n",
      "torch.Size([1, 5, 10438656, 2])\n",
      "torch.Size([1, 5, 12251136, 2])\n",
      "duration with n_workers = 8  RUNTIME:  23.636282920837402\n",
      "torch.Size([1, 5, 7299072, 2])\n",
      "torch.Size([1, 5, 7204864, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).\n",
      "\u0000ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).\n",
      "\u0000"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 7713) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:1163\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1163\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/queues.py:107\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    106\u001b[0m timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Empty\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/connection.py:424\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 424\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n",
      "File \u001b[0;32m/usr/lib/python3.8/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/signal_handling.py:66\u001b[0m, in \u001b[0;36m_set_SIGCHLD_handler.<locals>.handler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhandler\u001b[39m(signum, frame):\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# Python can still get and update the process status successfully.\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     \u001b[43m_error_if_any_worker_fails\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m previous_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid 7713) is killed by signal: Bus error. It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(mus, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m) \n\u001b[1;32m     12\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(loader):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(data\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     15\u001b[0m time_it(start, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mduration with n_workers = 32\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:1359\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1356\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1359\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1361\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1362\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:1325\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1322\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1323\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1324\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1325\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1326\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1327\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:1176\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1175\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[0;32m-> 1176\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(pids_str)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[1;32m   1178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 7713) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "loader = torch.utils.data.DataLoader(musValidation, batch_size=1, num_workers=0)\n",
    "start = time.time()\n",
    "for i, data in enumerate(loader):\n",
    "    print(data.shape)\n",
    "time_it(start, \"duration with n_workers = 1\")   \n",
    "loader = torch.utils.data.DataLoader(mus, batch_size=1, num_workers=8) \n",
    "start = time.time()\n",
    "for i, data in enumerate(loader):\n",
    "    print(data.shape)\n",
    "time_it(start, \"duration with n_workers = 8\")\n",
    "loader = torch.utils.data.DataLoader(mus, batch_size=1, num_workers=32) \n",
    "start = time.time()\n",
    "for i, data in enumerate(loader):\n",
    "    print(data.shape)\n",
    "time_it(start, \"duration with n_workers = 32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1185d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f954fbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.multiprocessing.set_start_method('spawn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebce5ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230e15ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1e68d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2682fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7695167064666748\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "data = mus.mus[3].stems\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8ce9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "data = musValidation[35]\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "stems = musValidation.mus[musValidation.filtered_indices[1]].stems[0,1:10,:]\n",
    "\n",
    "end_end_time = time.time()\n",
    "\n",
    "x,c,m,y = data\n",
    "x = x.to('cuda:0')\n",
    "c = c.to('cuda:0')\n",
    "m = m.to('cuda:0')\n",
    "y = y.to('cuda:0')\n",
    "lightning = lightning.to('cuda:0')\n",
    "\n",
    "\n",
    "end_end_time_2 = time.time()\n",
    "\n",
    "\n",
    "y_hat = lightning(x,c,m)\n",
    "\n",
    "end_end_time_3 = time.time()\n",
    "\n",
    "loss = SSloss(y_hat, y[:,3])\n",
    "\n",
    "end_end_time_4 = time.time()\n",
    "\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "elapsed_time_2 = end_end_time - end_time\n",
    "elapsed_time_3 = end_end_time_2 - end_end_time\n",
    "elapsed_time_4 = end_end_time_3 - end_end_time_2\n",
    "elapsed_time_5 = end_end_time_4 - end_end_time_3\n",
    "print(\"1 ----- Elapsed time: {:.2f} seconds\".format(elapsed_time))\n",
    "print(\"2 ----- Elapsed time: {:.2f} seconds\".format(elapsed_time_2))\n",
    "print(\"3 ----- Elapsed time: {:.2f} seconds\".format(elapsed_time_3))\n",
    "print(\"4 ----- Elapsed time: {:.2f} seconds\".format(elapsed_time_4))\n",
    "print(\"5 ----- Elapsed time: {:.2f} seconds\".format(elapsed_time_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031d0c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testdataset(nn.util.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "829c9d77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANiMAL - Rockshow\n",
      "Actions - One Minute Smile\n",
      "Alexander Ross - Goodbye Bolero\n",
      "Clara Berry And Wooldog - Waltz For My Victims\n",
      "Fergessen - Nos Palpitants\n",
      "James May - On The Line\n",
      "Johnny Lokke - Promises & Lies\n",
      "Leaf - Summerghost\n",
      "Meaxic - Take A Step\n",
      "Patrick Talbot - A Reason To Leave\n",
      "Skelpolu - Human Mistakes\n",
      "Traffic Experiment - Sirens\n",
      "Triviul - Angelsaint\n",
      "Young Griffo - Pennies\n"
     ]
    }
   ],
   "source": [
    "\n",
    "musValidation = newMus(\"musdb/\", \"valid\", \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fefeb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define class to test the soeed of data loader\n",
    "\n",
    "# move all to lightning module\n",
    "\n",
    "# implement torchaudio preprocessing and reconstruction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "687e8f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class testDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.mus = musdb.DB(path, subsets = \"train\", split = \"valid\")\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.as_tensor(self.mus[idx].stems)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.mus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbdde0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mus = testDataset(\"musdb/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ca9d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "musTesting = prepare_testing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573a8c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "musValidation, musTraining = prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5c729c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d994f79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426a8384",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(max_epochs = 100, accelerator = \"gpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6adb53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([20,20,20,30,30,30,30,30,30,30,30,30,30,50,50,50,50,70,70,100,100,125],128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e80f458",
   "metadata": {},
   "outputs": [],
   "source": [
    "lightningtest = trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76a4ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lightning = LightningModel(model, musTraining, musValidation, collate, SSloss, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c99eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = torch.randn(16, 5, 2, 8400000)\n",
    "preprocess = preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2846b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = preprocess(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79729110",
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c00490",
   "metadata": {},
   "outputs": [],
   "source": [
    "y[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47483f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "newMus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d743f1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class DataHandler(nn.Module):\n",
    "    def __init(self, )\n",
    "    def batchize_training_item(self,stems):\n",
    "        # we need to trim the stems to the shortest duration track, starting from a random location\n",
    "        stems = self.trim_stems(stems, self.random_start(stems.shape[1]))\n",
    "        # we split the stems from shape (num_stems, num_samples, num_channels) into a tensor with shape (num_stems, num_segments, num_samples_per_segment, num_channels)\n",
    "        segments = self.split_track(stems)\n",
    "        start = time.time()\n",
    "        # now we need to drop out the low energy segments\n",
    "        segments = self.high_energy_segments(segments)\n",
    "        time_it(start, \"HIGH ENERGY SEGMENTS CHECK\")\n",
    "        # then we choose a random and continuous yet constant number of segments from the track minus the dropped segments\n",
    "        return segments\n",
    "\n",
    "    def batchize_testing_item(self,stems):\n",
    "        # first we make sure the length of the song will produce a whole number of segments by padding with zeros to the end of the next segment.\n",
    "        stems = self.add_zero_padding(stems)\n",
    "        # now we extend the song with zeros to make sure we have an equal batch size for every output.\n",
    "        to_pad = self.longest_duration_in_samples() - stems.shape[1]\n",
    "        stems = self.add_N_zeros(stems, to_pad)\n",
    "        # now we split the stems into equal size segments\n",
    "        segments = self.split_track(stems)\n",
    "        # we are now ready to operate on the song since we don't want to drop or modify our data as we conserve it to reconstruct our signal.\n",
    "        return segments\n",
    "        \n",
    "        \n",
    "    def trim_stems(self, stems, start):\n",
    "        # this function should trim the track to the shortest duration of the track from the start index, it allows looping back across the song.\n",
    "        if start + self.shortest_duration > stems.shape[1]:\n",
    "            first_half = stems[:, start:, :]\n",
    "            remaining = self.shortest_duration - (stems.shape[1] - start)\n",
    "            second_half = stems[:, :remaining, :]\n",
    "            return torch.cat((first_half, second_half), axis=1)\n",
    "        else:\n",
    "            return stems[:, start:start+self.shortest_duration, :]\n",
    "\n",
    "    def pad_stems(self, stems):\n",
    "        length_in_samples = stems.shape[1]\n",
    "        to_pad = self.longest_duration_in_samples() - length_in_samples\n",
    "\n",
    "    def num_segments_in_track(self, duration_in_samples):\n",
    "        # this function should return the number of segments in the track and consider the overlap factor, self.segment_overlap\n",
    "        return int(torch.ceil(torch.as_tensor(duration_in_samples / (self.segment_samples * (1 - self.segment_overlap)))))\n",
    "\n",
    "    def random_start(self, duration_in_samples):\n",
    "        # this function should return a random start index for the track\n",
    "        return torch.randint(0, duration_in_samples, (1,))\n",
    "\n",
    "        \n",
    "\n",
    "    def shortest_duration_in_samples(self, mus):\n",
    "        # this function should return the shortest duration of the stems in the track\n",
    "        min = 100000000\n",
    "        for i, dur in self.durations.items():\n",
    "            if i in self.filtered_indices.values():\n",
    "                if dur < min:\n",
    "                    min = dur\n",
    "        return min\n",
    "\n",
    "    def longest_duration_in_samples(self, mus):\n",
    "        # this function should return the longest duration of the stems in the track\n",
    "        return max(min([track.stems.shape[1] for track in mus.tracks]))\n",
    "\n",
    "    def is_high_energy_segment(self, segment, threshold):\n",
    "        # this function decides based on the provided threshold whether a sufficient number of chunks in the segment have an energy above the threshold\n",
    "        mix_chunk_energies = self.segment_chunk_energies(segment)[:]\n",
    "        return len(torch.argwhere(mix_chunk_energies > threshold)) > (self.chunks_below_percentile * self.segment_chunks)\n",
    "\n",
    "    def high_energy_segments(self, segments):\n",
    "        # this function should take in a full track's stems, it will then split the track into segments. \n",
    "        # The segments should have an overlap factor of self.segment_overlap.\n",
    "        # Then, it will split each segment into chunks, and it will calculate the energy of each segment, and store the energy of each segment in a list.\n",
    "        # With this list, it will calculate the percentile of the energy of the chunks, and it will discard the segments where 25% of the chunks have an energy below the percentile.\n",
    "        # It will then return the list of segments that have a high enough energy.\n",
    "        high_energy_indices = []\n",
    "        threshold = self.segment_energy_threshold(segments)\n",
    "        for idx, segment in enumerate(segments):\n",
    "            if self.is_high_energy_segment(segment, threshold):\n",
    "                high_energy_indices.append(idx)\n",
    "        high_energy_indices = torch.as_tensor(high_energy_indices)\n",
    "        return torch.index_select(segments, 0, high_energy_indices)\n",
    "\n",
    "    def segment_energy_threshold(self, segments):\n",
    "        # this function should split every segment into self.segment_chunks chunks, and it will calculate the energy of each chunk using the RMS energy function.\n",
    "        # It will save the energy of each chunk in a list, and it will return the value self.percentile_dropped percentile of the list.\n",
    "        chunk_energies = []\n",
    "        for segment in segments:\n",
    "            chunk_energies.extend(self.segment_chunk_energies(segment))\n",
    "        chunk_energies = torch.stack(chunk_energies)\n",
    "        percentile = torch.quantile(chunk_energies, self.drop_percentile, interpolation='midpoint')\n",
    "        return percentile\n",
    "            \n",
    "    def segment_chunk_energies(self, segment):\n",
    "        # this function should split the segment into self.segment_chunks chunks, and it will calculate the energy of each chunk using the RMS energy function.\n",
    "        # It will save the energy of each chunk in a list, and it will return the list.\n",
    "        chunk_energies = []\n",
    "        segment_samples = segment.shape[1]\n",
    "        chunk_samples = int(segment_samples / self.segment_chunks)\n",
    "        for i in range(0, segment_samples, chunk_samples):\n",
    "            chunk = segment[:, i:i+chunk_samples]\n",
    "            mix_track = chunk[0,:,:]\n",
    "            squared_tensor = torch.pow(chunk, 2)\n",
    "            mean_power = torch.mean(squared_tensor)\n",
    "            rms = torch.sqrt(mean_power)\n",
    "            chunk_energies.append(rms)\n",
    "        return torch.stack(chunk_energies).view(self.segment_chunks)\n",
    "\n",
    "    def split_track(self, stems):\n",
    "        # this function should take in a full track, and it will split the track into segments. \n",
    "        # The segments should have an overlap factor of self.segment_overlap.\n",
    "        # We add zero padding to the track to make sure that the track is divisible by the segment length.\n",
    "        # Then, it will split each segment into chunks, and it will return the list of chunks.\n",
    "        # The input is a tensor with shape (num_stems, num_samples, num_channels)\n",
    "        # The output is a tensor array with shape (num_stems, num_segments, num_samples_per_segment, num_channels)\n",
    "        stems = self.add_zero_padding(stems)\n",
    "        segments = []\n",
    "        num_samples = stems.shape[1]\n",
    "        step_in_samples = int(self.segment_samples * (1 - self.segment_overlap))\n",
    "        for i in range(0, num_samples - step_in_samples, step_in_samples):\n",
    "            segment = stems[:, i:i+self.segment_samples]\n",
    "            segments.append(segment)\n",
    "        segments = torch.stack(segments)\n",
    "        return segments\n",
    "\n",
    "    def add_zero_padding(self, stems):\n",
    "        # this function should add zero padding to the track to make sure that the track is divisible by the segment length and the residue from the overlap.\n",
    "        # the length of the array has to be segment_length + k * samples_in_steps for some nonnegative integer k.\n",
    "        num_samples = stems.shape[1]\n",
    "        step_in_samples = int(self.segment_samples * (1 - self.segment_overlap))\n",
    "        samples_in_last_segment = num_samples % step_in_samples\n",
    "        if samples_in_last_segment != 0:\n",
    "            padding = torch.zeros((stems.shape[0], step_in_samples - samples_in_last_segment, stems.shape[2]))\n",
    "            return torch.cat((stems, padding), axis=1)\n",
    "        else:\n",
    "            return stems\n",
    "\n",
    "    def add_N_zeros(self, stems, N):\n",
    "        zeros = torch.zeros((stems.shape[0], N, stems.shape[2]))\n",
    "        return torch.cat((stems, zeros), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8280b0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANiMAL - Rockshow\n",
      "Actions - One Minute Smile\n",
      "Alexander Ross - Goodbye Bolero\n",
      "Clara Berry And Wooldog - Waltz For My Victims\n",
      "Fergessen - Nos Palpitants\n",
      "James May - On The Line\n",
      "Johnny Lokke - Promises & Lies\n",
      "Leaf - Summerghost\n",
      "Meaxic - Take A Step\n",
      "Patrick Talbot - A Reason To Leave\n",
      "Skelpolu - Human Mistakes\n",
      "Traffic Experiment - Sirens\n",
      "Triviul - Angelsaint\n",
      "Young Griffo - Pennies\n"
     ]
    }
   ],
   "source": [
    "musValidation = newMus('musdb/', 'valid', 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fcdc28cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Classic Education - NightOwl\n",
      "ANiMAL - Clinic A\n",
      "ANiMAL - Easy Tiger\n",
      "Actions - Devil's Words\n",
      "Actions - South Of The Water\n",
      "Aimee Norwich - Child\n",
      "Alexander Ross - Velvet Curtain\n",
      "Angela Thomas Wade - Milk Cow Blues\n",
      "Atlantis Bound - It Was My Fault For Waiting\n",
      "Auctioneer - Our Future Faces\n",
      "AvaLuna - Waterduct\n",
      "BigTroubles - Phantom\n",
      "Bill Chudziak - Children Of No-one\n",
      "Black Bloc - If You Want Success\n",
      "Celestial Shore - Die For Us\n",
      "Chris Durban - Celebrate\n",
      "Clara Berry And Wooldog - Air Traffic\n",
      "Clara Berry And Wooldog - Stella\n",
      "Cnoc An Tursa - Bannockburn\n",
      "Creepoid - OldTree\n",
      "Dark Ride - Burning Bridges\n",
      "Dreamers Of The Ghetto - Heavy Love\n",
      "Drumtracks - Ghost Bitch\n",
      "Faces On Film - Waiting For Ga\n",
      "Fergessen - Back From The Start\n",
      "Fergessen - The Wind\n",
      "Flags - 54\n",
      "Giselle - Moss\n",
      "Grants - PunchDrunk\n",
      "Helado Negro - Mitad Del Mundo\n",
      "Hezekiah Jones - Borrowed Heart\n",
      "Hollow Ground - Left Blind\n",
      "Hop Along - Sister Cities\n",
      "Invisible Familiars - Disturbing Wildlife\n",
      "James May - All Souls Moon\n",
      "James May - Dont Let Go\n",
      "James May - If You Say\n",
      "Jay Menon - Through My Eyes\n",
      "Johnny Lokke - Whisper To A Scream\n",
      "Jokers, Jacks & Kings - Sea Of Leaves\n",
      "Leaf - Come Around\n",
      "Leaf - Wicked\n",
      "Lushlife - Toynbee Suite\n",
      "Matthew Entwistle - Dont You Ever\n",
      "Meaxic - You Listen\n",
      "Music Delta - 80s Rock\n",
      "Music Delta - Beatles\n",
      "Music Delta - Britpop\n",
      "Music Delta - Country1\n",
      "Music Delta - Country2\n",
      "Music Delta - Disco\n",
      "Music Delta - Gospel\n",
      "Music Delta - Grunge\n",
      "Music Delta - Hendrix\n",
      "Music Delta - Punk\n",
      "Music Delta - Reggae\n",
      "Music Delta - Rock\n",
      "Music Delta - Rockabilly\n",
      "Night Panther - Fire\n",
      "North To Alaska - All The Same\n",
      "Patrick Talbot - Set Me Free\n",
      "Phre The Eon - Everybody's Falling Apart\n",
      "Port St Willow - Stay Even\n",
      "Remember December - C U Next Time\n",
      "Secret Mountains - High Horse\n",
      "Skelpolu - Together Alone\n",
      "Snowmine - Curfews\n",
      "Spike Mullings - Mike's Sulking\n",
      "St Vitus - Word Gets Around\n",
      "Steven Clark - Bounty\n",
      "Strand Of Oaks - Spacestation\n",
      "Sweet Lights - You Let Me Down\n",
      "Swinging Steaks - Lost My Way\n",
      "The Districts - Vermont\n",
      "The Long Wait - Back Home To Blue\n",
      "The Scarlet Brand - Les Fleurs Du Mal\n",
      "The So So Glos - Emergency\n",
      "The Wrong'Uns - Rothko\n",
      "Tim Taler - Stalker\n",
      "Titanium - Haunted Age\n",
      "Traffic Experiment - Once More (With Feeling)\n",
      "Triviul - Dorothy\n",
      "Voelund - Comfort Lives In Belief\n",
      "Wall Of Death - Femme\n",
      "Young Griffo - Blood To Bone\n",
      "Young Griffo - Facade\n"
     ]
    }
   ],
   "source": [
    "musTraining = newMus('musdb/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47eb6109",
   "metadata": {},
   "outputs": [],
   "source": [
    "class newMus(torch.utils.data.Dataset):\n",
    "    def __init__(self, musdb_root, split='train', subset='train', is_wav=False, sample_rate=44100, segment_length = 10, segment_chunks = 10, discard_low_energy = True, segment_overlap = 0.5, drop_percentile =  0.1, chunks_below_percentile = 0.5):\n",
    "        assert(subset == 'train' or subset == 'test')\n",
    "        self.mode = subset\n",
    "        self.split = split \n",
    "        self.mus = musdb.DB(musdb_root, subsets=subset, split=split, is_wav=is_wav)\n",
    "        self.sample_rate = sample_rate\n",
    "        self.discard_low_energy = discard_low_energy\n",
    "        self.segment_length = segment_length\n",
    "        self.segment_chunks = segment_chunks\n",
    "        self.chunks_below_percentile = chunks_below_percentile\n",
    "        self.segment_overlap = segment_overlap\n",
    "        self.drop_percentile = drop_percentile\n",
    "        self.segment_samples = int(self.segment_length * self.sample_rate)\n",
    "        self.chunk_samples = int(self.segment_samples / self.segment_chunks)\n",
    "        self.num_stems = self.mus.tracks[0].stems.shape[0]\n",
    "        self.num_channels = self.mus.tracks[0].stems.shape[2]\n",
    "        self.durations = dict()\n",
    "        self.filtered_indices = dict()\n",
    "        self.len = self.init_durations()\n",
    "        self.shortest_duration = self.shortest_duration_in_samples(self.mus)\n",
    "        self.batch_size = self.find_batch_size()\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # this function should return a batch of segment STFTs from the song as well as their stem STFTs.\n",
    "        start = time.time()\n",
    "        track = self.mus.tracks[self.filtered_indices[idx]]\n",
    "        # stems is a list of the stems of the track, in the order of the stems in the track\n",
    "        stems = torch.as_tensor(track.stems)\n",
    "        return stems\n",
    "\n",
    "    def batchize_training_item(self,stems):\n",
    "        # we need to trim the stems to the shortest duration track, starting from a random location\n",
    "        stems = self.trim_stems(stems, self.random_start(stems.shape[1]))\n",
    "        # we split the stems from shape (num_stems, num_samples, num_channels) into a tensor with shape (num_stems, num_segments, num_samples_per_segment, num_channels)\n",
    "        segments = self.split_track(stems)\n",
    "        start = time.time()\n",
    "        # now we need to drop out the low energy segments\n",
    "        segments = self.high_energy_segments(segments)\n",
    "        time_it(start, \"HIGH ENERGY SEGMENTS CHECK\")\n",
    "        # then we choose a random and continuous yet constant number of segments from the track minus the dropped segments\n",
    "        return segments\n",
    "\n",
    "    def batchize_testing_item(self,stems):\n",
    "        # first we make sure the length of the song will produce a whole number of segments by padding with zeros to the end of the next segment.\n",
    "        stems = self.add_zero_padding(stems)\n",
    "        # now we extend the song with zeros to make sure we have an equal batch size for every output.\n",
    "        to_pad = self.longest_duration_in_samples() - stems.shape[1]\n",
    "        stems = self.add_N_zeros(stems, to_pad)\n",
    "        # now we split the stems into equal size segments\n",
    "        segments = self.split_track(stems)\n",
    "        # we are now ready to operate on the song since we don't want to drop or modify our data as we conserve it to reconstruct our signal.\n",
    "        return segments\n",
    "\n",
    "    def collate(batch):\n",
    "        # we define a custom function that overrides the PyTorch built in collate in order to make sure\n",
    "        # our input to the model matches the dimensionality we want.\n",
    "        # our input to the function is a list of (input, label) tuples. The list will have size = batch_size as defined in the DataLoader\n",
    "        # We will use a batch_size of 1 to receive a list of tuples (in this case a list with 1 tuple), each tuple has two elements:\n",
    "        # First, the input, a tensor with shape (batch_size, stft_dim_F, stft_dim_T, 2)\n",
    "        # Second, the label, a tensor with shape (num_stems - 1, batch_size, stft_dim_F, stft_dim_T, 2)\n",
    "        input, chromas, mfccs, labels = batch[0]\n",
    "        return input, chromas, mfccs, labels\n",
    "\n",
    "    def init_durations(self):\n",
    "        pos = 0\n",
    "        for idx, track in enumerate(self.mus.tracks):\n",
    "            print(track.name)\n",
    "            self.durations[idx] = track.stems.shape[1]\n",
    "            if self.durations[idx] >= self.segment_samples * 9:\n",
    "                self.filtered_indices[pos] = idx\n",
    "                pos += 1\n",
    "        return pos\n",
    "        \n",
    "        \n",
    "    def trim_stems(self, stems, start):\n",
    "        # this function should trim the track to the shortest duration of the track from the start index, it allows looping back across the song.\n",
    "        if start + self.shortest_duration > stems.shape[1]:\n",
    "            first_half = stems[:, start:, :]\n",
    "            remaining = self.shortest_duration - (stems.shape[1] - start)\n",
    "            second_half = stems[:, :remaining, :]\n",
    "            return torch.cat((first_half, second_half), axis=1)\n",
    "        else:\n",
    "            return stems[:, start:start+self.shortest_duration, :]\n",
    "\n",
    "    def pad_stems(self, stems):\n",
    "        length_in_samples = stems.shape[1]\n",
    "        to_pad = self.longest_duration_in_samples() - length_in_samples\n",
    "\n",
    "    def find_batch_size(self):\n",
    "        # this function should tell us how many STFTs we can fit into a batch based on finding the floor power of 2 of the number of STFTs we fit over the duration of the song\n",
    "        # each STFT will represent a STFT over a fixed segment length.\n",
    "        num_segments = self.num_segments_in_track(self.shortest_duration)\n",
    "        # we anticipate a drop of up to twice the drop percentile (impossible, just to be safe) of the segments.\n",
    "        num_segments = torch.as_tensor(int(num_segments * (1 - 2 * self.drop_percentile)))\n",
    "        # We return the closest power of two to that anticipated number of segments. Of course we use a floor because we want to fill every batch.\n",
    "        return 2 ** int(torch.floor(torch.log2(num_segments)))\n",
    "\n",
    "    def num_segments_in_track(self, duration_in_samples):\n",
    "        # this function should return the number of segments in the track and consider the overlap factor, self.segment_overlap\n",
    "        return int(torch.ceil(torch.as_tensor(duration_in_samples / (self.segment_samples * (1 - self.segment_overlap)))))\n",
    "\n",
    "    def random_start(self, duration_in_samples):\n",
    "        # this function should return a random start index for the track\n",
    "        return torch.randint(0, duration_in_samples, (1,))\n",
    "\n",
    "        \n",
    "\n",
    "    def shortest_duration_in_samples(self, mus):\n",
    "        # this function should return the shortest duration of the stems in the track\n",
    "        min = 100000000\n",
    "        for i, dur in self.durations.items():\n",
    "            if i in self.filtered_indices.values():\n",
    "                if dur < min:\n",
    "                    min = dur\n",
    "        return min\n",
    "\n",
    "    def longest_duration_in_samples(self, mus):\n",
    "        # this function should return the longest duration of the stems in the track\n",
    "        return max(min([track.stems.shape[1] for track in mus.tracks]))\n",
    "\n",
    "    def is_high_energy_segment(self, segment, threshold):\n",
    "        # this function decides based on the provided threshold whether a sufficient number of chunks in the segment have an energy above the threshold\n",
    "        mix_chunk_energies = self.segment_chunk_energies(segment)[:]\n",
    "        return len(torch.argwhere(mix_chunk_energies > threshold)) > (self.chunks_below_percentile * self.segment_chunks)\n",
    "\n",
    "    def high_energy_segments(self, segments):\n",
    "        # this function should take in a full track's stems, it will then split the track into segments. \n",
    "        # The segments should have an overlap factor of self.segment_overlap.\n",
    "        # Then, it will split each segment into chunks, and it will calculate the energy of each segment, and store the energy of each segment in a list.\n",
    "        # With this list, it will calculate the percentile of the energy of the chunks, and it will discard the segments where 25% of the chunks have an energy below the percentile.\n",
    "        # It will then return the list of segments that have a high enough energy.\n",
    "        high_energy_indices = []\n",
    "        threshold = self.segment_energy_threshold(segments)\n",
    "        for idx, segment in enumerate(segments):\n",
    "            if self.is_high_energy_segment(segment, threshold):\n",
    "                high_energy_indices.append(idx)\n",
    "        high_energy_indices = torch.as_tensor(high_energy_indices)\n",
    "        return torch.index_select(segments, 0, high_energy_indices)\n",
    "\n",
    "    def segment_energy_threshold(self, segments):\n",
    "        # this function should split every segment into self.segment_chunks chunks, and it will calculate the energy of each chunk using the RMS energy function.\n",
    "        # It will save the energy of each chunk in a list, and it will return the value self.percentile_dropped percentile of the list.\n",
    "        chunk_energies = []\n",
    "        for segment in segments:\n",
    "            chunk_energies.extend(self.segment_chunk_energies(segment))\n",
    "        chunk_energies = torch.stack(chunk_energies)\n",
    "        percentile = torch.quantile(chunk_energies, self.drop_percentile, interpolation='midpoint')\n",
    "        return percentile\n",
    "            \n",
    "    def segment_chunk_energies(self, segment):\n",
    "        # this function should split the segment into self.segment_chunks chunks, and it will calculate the energy of each chunk using the RMS energy function.\n",
    "        # It will save the energy of each chunk in a list, and it will return the list.\n",
    "        chunk_energies = []\n",
    "        segment_samples = segment.shape[1]\n",
    "        chunk_samples = int(segment_samples / self.segment_chunks)\n",
    "        for i in range(0, segment_samples, chunk_samples):\n",
    "            chunk = segment[:, i:i+chunk_samples]\n",
    "            mix_track = chunk[0,:,:]\n",
    "            squared_tensor = torch.pow(chunk, 2)\n",
    "            mean_power = torch.mean(squared_tensor)\n",
    "            rms = torch.sqrt(mean_power)\n",
    "            chunk_energies.append(rms)\n",
    "        return torch.stack(chunk_energies).view(self.segment_chunks)\n",
    "\n",
    "    def split_track(self, stems):\n",
    "        # this function should take in a full track, and it will split the track into segments. \n",
    "        # The segments should have an overlap factor of self.segment_overlap.\n",
    "        # We add zero padding to the track to make sure that the track is divisible by the segment length.\n",
    "        # Then, it will split each segment into chunks, and it will return the list of chunks.\n",
    "        # The input is a tensor with shape (num_stems, num_samples, num_channels)\n",
    "        # The output is a tensor array with shape (num_stems, num_segments, num_samples_per_segment, num_channels)\n",
    "        stems = self.add_zero_padding(stems)\n",
    "        segments = []\n",
    "        num_samples = stems.shape[1]\n",
    "        step_in_samples = int(self.segment_samples * (1 - self.segment_overlap))\n",
    "        for i in range(0, num_samples - step_in_samples, step_in_samples):\n",
    "            segment = stems[:, i:i+self.segment_samples]\n",
    "            segments.append(segment)\n",
    "        segments = torch.stack(segments)\n",
    "        return segments\n",
    "\n",
    "    def add_zero_padding(self, stems):\n",
    "        # this function should add zero padding to the track to make sure that the track is divisible by the segment length and the residue from the overlap.\n",
    "        # the length of the array has to be segment_length + k * samples_in_steps for some nonnegative integer k.\n",
    "        num_samples = stems.shape[1]\n",
    "        step_in_samples = int(self.segment_samples * (1 - self.segment_overlap))\n",
    "        samples_in_last_segment = num_samples % step_in_samples\n",
    "        if samples_in_last_segment != 0:\n",
    "            padding = torch.zeros((stems.shape[0], step_in_samples - samples_in_last_segment, stems.shape[2]))\n",
    "            return torch.cat((stems, padding), axis=1)\n",
    "        else:\n",
    "            return stems\n",
    "\n",
    "    def add_N_zeros(self, stems, N):\n",
    "        zeros = torch.zeros((stems.shape[0], N, stems.shape[2]))\n",
    "        return torch.cat((stems, zeros), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6b1aae4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "        \"mus_path\": \"musdb/\",\n",
    "        \"num_bandwidths\": 23,\n",
    "        \"bandwidths\": \"20,20,20,30,30,30,30,30,30,30,30,30,30,50,50,50,50,70,70,100,100,125\",\n",
    "        \"bandwidth_freq_out_size\": 128,\n",
    "        \"n_fft\": 2048,\n",
    "        \"hop_length\": 1024,\n",
    "        \"win_length\": 2048,\n",
    "        \"conv_1_kernel_size\":(1,7),\n",
    "        \"conv_1_stride\":(1,3),\n",
    "        \"conv_2_kernel_size\":(4,4),\n",
    "        \"conv_2_stride\":(2,2),\n",
    "        \"conv_3_kernel_size\":(1,7),\n",
    "        \"conv_3_stride\":(1,3),\n",
    "        \"conv_3_ch_out_1\":8,\n",
    "        \"time_steps\": 431,\n",
    "        \"freq_bands\": 1025,\n",
    "        \"n_mels\": 32,\n",
    "        \"input_sampling_rate\": 44100,\n",
    "        \"resampling_rate\": 16000\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "01e962e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_waveform(waveform, sr, title=\"Waveform\"):\n",
    "    waveform = waveform.numpy()\n",
    "\n",
    "    num_channels, num_frames = waveform.shape\n",
    "    time_axis = torch.arange(0, num_frames) / sr\n",
    "\n",
    "    figure, axes = plt.subplots(num_channels, 1)\n",
    "    axes.plot(time_axis, waveform[0], linewidth=1)\n",
    "    axes.grid(True)\n",
    "    figure.suptitle(title)\n",
    "    plt.show(block=False)\n",
    "\n",
    "\n",
    "def plot_spectrogram(specgram, title=None, ylabel=\"freq_bin\"):\n",
    "    fig, axs = plt.subplots(1, 1)\n",
    "    axs.set_title(title or \"Spectrogram (db)\")\n",
    "    axs.set_ylabel(ylabel)\n",
    "    axs.set_xlabel(\"frame\")\n",
    "    im = axs.imshow(librosa.power_to_db(specgram), origin=\"lower\", aspect=\"auto\")\n",
    "    fig.colorbar(im, ax=axs)\n",
    "    plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f828c52e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 20, 20, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 50, 50, 50, 50, 70, 70, 100, 100, 125]\n"
     ]
    }
   ],
   "source": [
    "lightning = LightningModel(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ec95f736",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[91], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m lightning \u001b[38;5;241m=\u001b[39m \u001b[43mlightning\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda:0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/lightning_fabric/utilities/device_dtype_mixin.py:54\u001b[0m, in \u001b[0;36m_DeviceDtypeModuleMixin.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m device, dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39m_parse_to(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__update_properties(device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:927\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    923\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    924\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m    925\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 927\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 579\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    583\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    584\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    590\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 579\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    583\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    584\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    590\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 579\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    583\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    584\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    590\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/rnn.py:189\u001b[0m, in \u001b[0;36mRNNBase._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;28;01mlambda\u001b[39;00m wn: \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, wn) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, wn) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)(wn) \u001b[38;5;28;01mfor\u001b[39;00m wn \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights_names]\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# Flattens params (on CUDA)\u001b[39;00m\n\u001b[0;32m--> 189\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/rnn.py:175\u001b[0m, in \u001b[0;36mRNNBase.flatten_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    174\u001b[0m     num_weights \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 175\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cudnn_rnn_flatten_weight\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_cudnn_mode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproj_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "lightning = lightning.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d62e96d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "aeb16393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10.2'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.version.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2ff4021e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: /root/MSSFinalProj/proj/lightning_logs\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[94], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(limit_train_batches\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m'\u001b[39m, devices \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlightning\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmusTraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmusValidation\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py:608\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    606\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`Trainer.fit()` requires a `LightningModule`, got: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m_lightning_module \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m--> 608\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/call.py:38\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     41\u001b[0m     trainer\u001b[38;5;241m.\u001b[39m_call_teardown_hook()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py:650\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    643\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m ckpt_path \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresume_from_checkpoint\n\u001b[1;32m    644\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_set_ckpt_path(\n\u001b[1;32m    645\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    646\u001b[0m     ckpt_path,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    647\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    648\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    649\u001b[0m )\n\u001b[0;32m--> 650\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py:1084\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger_connector\u001b[38;5;241m.\u001b[39mreset_metrics()\n\u001b[1;32m   1083\u001b[0m \u001b[38;5;66;03m# strategy will configure model and move it to the device\u001b[39;00m\n\u001b[0;32m-> 1084\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[38;5;66;03m# hook\u001b[39;00m\n\u001b[1;32m   1087\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;241m==\u001b[39m TrainerFn\u001b[38;5;241m.\u001b[39mFITTING:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/strategies/single_device.py:73\u001b[0m, in \u001b[0;36mSingleDeviceStrategy.setup\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msetup\u001b[39m(\u001b[38;5;28mself\u001b[39m, trainer: pl\u001b[38;5;241m.\u001b[39mTrainer) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39msetup(trainer)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/strategies/single_device.py:70\u001b[0m, in \u001b[0;36mSingleDeviceStrategy.model_to_device\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmodel_to_device\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself.model must be set before self.model.to()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot_device\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/lightning_fabric/utilities/device_dtype_mixin.py:54\u001b[0m, in \u001b[0;36m_DeviceDtypeModuleMixin.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m device, dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39m_parse_to(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__update_properties(device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:927\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    923\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    924\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m    925\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 927\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 579\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    583\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    584\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    590\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 579\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    583\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    584\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    590\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 579\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    583\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    584\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    590\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/rnn.py:189\u001b[0m, in \u001b[0;36mRNNBase._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;28;01mlambda\u001b[39;00m wn: \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, wn) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, wn) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)(wn) \u001b[38;5;28;01mfor\u001b[39;00m wn \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights_names]\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# Flattens params (on CUDA)\u001b[39;00m\n\u001b[0;32m--> 189\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/rnn.py:175\u001b[0m, in \u001b[0;36mRNNBase.flatten_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    174\u001b[0m     num_weights \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 175\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cudnn_rnn_flatten_weight\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_cudnn_mode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproj_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(limit_train_batches=100, max_epochs=1, accelerator='gpu', devices = 1)\n",
    "trainer.fit(lightning, musTraining, musValidation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a0b67b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(next(lightning.parameters()).is_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "513f436e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "dc063ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningModel(pl.LightningModule): \n",
    "    \n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.mus_path = hparams['mus_path']\n",
    "        self.bandwidths = [int(bandwidth) for bandwidth in hparams['bandwidths'].split(',')]\n",
    "        print(self.bandwidths)\n",
    "        self.n_mels = hparams['n_mels']\n",
    "        self.N = hparams['bandwidth_freq_out_size']\n",
    "        self.K = len(self.bandwidths)\n",
    "        self.time_steps = hparams['time_steps']\n",
    "        self.transforms = Transforms().double()\n",
    "        self.kernel1 = hparams['conv_1_kernel_size']\n",
    "        self.stride1 = hparams['conv_1_stride']\n",
    "        self.kernel2 = hparams['conv_2_kernel_size']\n",
    "        self.stride2 = hparams['conv_2_stride']\n",
    "        self.training_dataloader = None\n",
    "        self.testing_dataloader = None\n",
    "        self.validation_dataloader = None\n",
    "        self.bandsplit = BandSplit(self.bandwidths, self.N).double()\n",
    "        self.conv1 = ConvolutionLayer(self.K, self.K, self.kernel1, self.stride1).double()\n",
    "        self.conv2 = ConvolutionLayer(self.K, self.K, self.kernel2, self.stride2).double()\n",
    "        self.conv3 = ConvolutionLayer(1,8,kernel_size=(1,11),stride=(1,3)).double()\n",
    "        self.conv4 = ConvolutionLayer(8,22,kernel_size=(1,3),stride=(1,2)).double()\n",
    "        self.conv5 = ConvolutionLayer(1,8,kernel_size=(1,11),stride=(1,3)).double()\n",
    "        self.conv6 = ConvolutionLayer(8,22,kernel_size=(1,3),stride=(1,2)).double()\n",
    "        self.blstms1 = AlternatingBLSTMs(self.K, 70, 63, 64).double()\n",
    "        self.blstms2 = AlternatingBLSTMs(self.K, 70, 96, 64).double()\n",
    "        self.blstms3 = AlternatingBLSTMs(self.K, 70, 76, 63 ).double()\n",
    "        self.deconv1 = TransposeConvolutionLayer(self.K, self.K, self.kernel2, self.stride2).double()\n",
    "        self.deconv2 = TransposeConvolutionLayer(self.K, self.K, self.kernel1, self.stride1).double()\n",
    "        self.masks = MaskEstimation(self.bandwidths, 128,32).double()\n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        if self.step % 5 == 0:a\n",
    "            torch.cuda.empty_cache()\n",
    "        self.step += 1\n",
    "        \n",
    "        data = self.training_dataloader.dataset.batchize_training_item(batch)\n",
    "        stfts, chromas, mfccs = self.transforms(data)\n",
    "        predicted_sources = self.forward_pass(stfts, chromas, mfccs)\n",
    "        loss = self.loss(predicted_sources, real_sources[:,3])\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        if self.step % 5 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "        self.step += 1\n",
    "        \n",
    "        data = self.validation_dataloader.dataset.batchize_training_item(batch)\n",
    "        stfts, chromas, mfccs = self.transforms(data)\n",
    "        predicted_sources = self.forward_pass(stfts, chromas, mfccs)\n",
    "        loss = self.loss(predicted_sources, real_sources[:,3])\n",
    "        self.log(\"valid_loss\", loss)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        pass\n",
    "    \n",
    "    def forward_pass(self, X, chromas, mfccs):\n",
    "        # takes in STFTs, chromas, mfccs\n",
    "        X1 = self.bandsplit(X)\n",
    "        batch_size = X1.shape[0]\n",
    "        #Shape: torch.Size([32, 22, 431, 128]) (batch_size, num_bands, time_steps, freq_N)\n",
    "        X2 = self.conv1(X1)\n",
    "        X3 = self.conv2(X2)\n",
    "        mfccs = mfccs.reshape(batch_size,1,self.n_mels,-1)\n",
    "        mfccs = self.conv3(mfccs)\n",
    "        mfccs = self.conv4(mfccs)\n",
    "        chromas = chromas.reshape(batch_size,1,12,-1)\n",
    "        chromas = self.conv5(chromas)\n",
    "        chromas = self.conv6(chromas)\n",
    "        X, _ = self.blstms1(X3)\n",
    "        xmfccs = torch.cat((mfccs,X), 2)\n",
    "        X, _ = self.blstms2(xmfccs)\n",
    "        xchromas = torch.cat((chromas,X),2)\n",
    "        X, _ = self.blstms3(xchromas)      \n",
    "        X = self.deconv1(X + X3)\n",
    "        X = self.deconv2(X + X2)\n",
    "        X = self.masks(torch.cat((X, X[:,:,:,-1].unsqueeze(3)), 3))\n",
    "        return X\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr = 0.001)\n",
    "        return optimizer\n",
    "    \n",
    "\n",
    "        \n",
    "class Chroma(nn.Module):\n",
    "    def __init__(self, n_fft, sampling_rate):\n",
    "        self.n_fft = n_fft\n",
    "        self.sampling_rate = sampling_rate\n",
    "        \n",
    "    def forward(self, x):\n",
    "        pass\n",
    "    \n",
    "        # x is a spectrogram with shape(... , T, F)\n",
    "class Transforms(nn.Module):\n",
    "    def __init__(self, input_freq = 44100, resample_freq = 16000, n_fft = 2048, hop_length = 1024, win_length=2048, n_mels = 32):\n",
    "        super().__init__()\n",
    "        self.resample = Resample(input_freq, resample_freq)\n",
    "        self.stft = Spectrogram(n_fft = n_fft, hop_length = hop_length, win_length = win_length)\n",
    "        self.mel = MelScale(sample_rate = resample_freq, n_mels = n_mels, n_stft = n_fft // 2 + 1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.resample(X)\n",
    "        \n",
    "        stft = self.stft(X)\n",
    "    \n",
    "        mfccs= self.mel(stft)\n",
    "        \n",
    "        chromas = self.chromas()\n",
    "        return stft, mfccs\n",
    "    \n",
    "def post_conv_dimensions(self,N,time_steps,in_channels):\n",
    "        x = torch.randn(1,in_channels,N,time_steps).to(self.device).double()\n",
    "        return self.conv2(self.conv1(x)).shape\n",
    "        \n",
    "class ConvolutionLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding=0, dtype='double'):\n",
    "        super(ConvolutionLayer, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding=padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "    \n",
    "class TransposeConvolutionLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, dtype='double'):\n",
    "        super(TransposeConvolutionLayer, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "# This class defines a module that runs t\n",
    "class AlternatingBLSTMs(nn.Module):\n",
    "    def __init__(self, num_bands, time_steps, N, out_size, axis=1):\n",
    "        super(AlternatingBLSTMs, self).__init__()\n",
    "        self.band_blstm = BandBiLSTM(num_bands, time_steps, N)\n",
    "        self.temporal_blstm = TemporalBiLSTM(num_bands, time_steps, N, out_size)\n",
    "        self.num_bands = num_bands\n",
    "        self.time_steps = time_steps\n",
    "        self.N = N\n",
    "        # hidden size = freq_steps_per_band * time_steps \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input shape: (batch_size, num_bands, N, time_steps)\n",
    "        # Prepare for Band BLSTM: shape = (batch_size, num_bands, N * time_steps)\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.reshape(batch_size, self.num_bands, -1)\n",
    "        x = self.band_blstm(x)\n",
    "        x = x.reshape(batch_size, self.time_steps, -1)\n",
    "        x = self.temporal_blstm(x)\n",
    "        #x += residual\n",
    "        # Return the output of the module\n",
    "        return x    \n",
    "    \n",
    "# This class defines a module that runs the input, with shape (num_bands, num_timesteps, N), through a normalization layer, then a temporal biLSTM, then a fully connected layer.\n",
    "# Then, the output of that layer is of the same shape as the input to the module, which will be fed into a similar structure, but this time with a band biLSTM, following the same normalization, biLSTM, FC structure.\n",
    "class BandBiLSTM(nn.Module):\n",
    "    def __init__(self, num_bands, time_steps, N, axis=1):\n",
    "        super(BandBiLSTM, self).__init__()\n",
    "        self.norm = nn.GroupNorm(num_bands, num_bands)\n",
    "        self.input_size = time_steps * N\n",
    "        self.hidden_size = self.input_size // 2\n",
    "        self.bilstm = nn.LSTM(self.input_size, self.hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(N, N)\n",
    "        self.axis = axis\n",
    "        self.N = N\n",
    "        self.num_bands = num_bands\n",
    "        self.time_steps = time_steps\n",
    "        # hidden size = freq_steps_per_band * time_steps \n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        # (batch_size,time_steps, num_bands, N)\n",
    "        x = self.norm(x)\n",
    "        residual = x.clone().detach()\n",
    "        x, lstm_vars = self.bilstm(x)\n",
    "        # (batch_size, num_bands, 2 * hidden_size)\n",
    "        x = x.reshape(batch_size, self.num_bands, self.time_steps, self.N)\n",
    "        # (batch_size, num_bands, time_steps, N)\n",
    "        x = self.fc(x)\n",
    "        # (batch_size, num_bands, time_steps, N)\n",
    "        #x += residual\n",
    "        # Return the output of the module\n",
    "        return x\n",
    "    \n",
    "class TemporalBiLSTM(nn.Module):\n",
    "    def __init__(self, num_bands, time_steps, N, out, axis=1):\n",
    "        super(TemporalBiLSTM, self).__init__()\n",
    "        self.norm = nn.GroupNorm(time_steps, time_steps)\n",
    "        self.input_size = num_bands * N\n",
    "        self.hidden_size = num_bands * out // 2\n",
    "        self.out = out\n",
    "        self.bilstm = nn.LSTM(self.input_size, self.hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(out, out)\n",
    "        self.axis = axis\n",
    "        self.N = N\n",
    "        self.time_steps = time_steps\n",
    "        self.num_bands = num_bands\n",
    "        # hidden size = freq_steps_per_band * time_steps \n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        # (batch_size,time_steps, num_bands, N)\n",
    "        x = self.norm(x)\n",
    "        residual = x.clone().detach()\n",
    "        x, lstm_vars = self.bilstm(x)\n",
    "        # (batch_size, num_bands, 2 * hidden_size)\n",
    "        x = x.reshape(batch_size, self.num_bands, self.time_steps, self.out)\n",
    "        # (batch_size, num_bands, time_steps, N)\n",
    "        x = self.fc(x)\n",
    "        x = x.permute(0,1,3,2)\n",
    "        # (batch_size, num_bands, time_steps, N)\n",
    "        #x += residual\n",
    "        # Return the output of the module\n",
    "        return x, lstm_vars\n",
    "\n",
    "class BandSplit(torch.nn.Module):\n",
    "    # Input shape: torch.Size([16, 2, 1025, 431, 2])\n",
    "    def __init__(self, bandwidths, N):\n",
    "        # bandwidth\n",
    "        super(BandSplit, self).__init__()\n",
    "        self.bandwidths = bandwidths\n",
    "        self.norm_layers = torch.nn.ModuleList([torch.nn.LayerNorm(2 * bandwidth) for bandwidth in self.bandwidths])\n",
    "        self.fc_layers = torch.nn.ModuleList([torch.nn.Linear(2 * bandwidth, N) for bandwidth in self.bandwidths])\n",
    "\n",
    "    def forward(self, X):\n",
    "        subband_spectrograms = []\n",
    "        K = len(self.bandwidths)\n",
    "        for i in range(K):\n",
    "            start_index = sum(self.bandwidths[:i])\n",
    "            end_index = start_index + self.bandwidths[i]\n",
    "            subband_spectrogram = X[:, :,start_index:end_index, :]\n",
    "            subband_spectrogram = subband_spectrogram.permute(0,1,4,2,3)\n",
    "            subband_spectrogram = subband_spectrogram.reshape(2 * X.shape[0], X.shape[3], 2 * self.bandwidths[i])\n",
    "            subband_spectrograms.append(subband_spectrogram)\n",
    "\n",
    "        subband_features = []\n",
    "        for i in range(K):\n",
    "            norm_output = self.norm_layers[i](subband_spectrograms[i])\n",
    "            fc_output = self.fc_layers[i](norm_output)\n",
    "            subband_features.append(fc_output)\n",
    "\n",
    "        Z = torch.stack(subband_features, dim=1)\n",
    "        Z = Z.permute(0,1,3,2)\n",
    "        return Z\n",
    "    \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, hidden_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.MLP = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, out_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.MLP(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class MaskEstimation(nn.Module):\n",
    "    def __init__(self, bandwidths, N, batch_size):\n",
    "        super(MaskEstimation, self).__init__()\n",
    "        self.num_bands = len(bandwidths)\n",
    "        self.bandwidths = bandwidths\n",
    "        self.batch_size = batch_size\n",
    "        self.norm_layers = torch.nn.ModuleList([torch.nn.LayerNorm(N) for bandwidth in self.bandwidths])\n",
    "        self.MLP_layers = torch.nn.ModuleList([MLP(N, bandwidth * 2, N * 2) for bandwidth in self.bandwidths])\n",
    "    def forward(self, x):\n",
    "        # Input shape: (batch_size, num_bands, N, T)\n",
    "        time_steps = x.shape[3]\n",
    "        x = x.permute(1, 0 , 3, 2)\n",
    "        out = []\n",
    "        # shape: (num_bands, batch_size, T, N)\n",
    "        for i in range(self.num_bands):\n",
    "            y = self.norm_layers[i](x[i])\n",
    "            y = self.MLP_layers[i](y)\n",
    "            out.append(y)\n",
    "        out = torch.cat(out, 2)\n",
    "        out = out.reshape(self.batch_size // 2, 2, sum(self.bandwidths), time_steps, 2)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a931b035",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc40aa7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581e76c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.get_num_threads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dd73cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.fit(lightning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a5b83d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab240f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "run(musValidation, musTraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a91dd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_testing():\n",
    "    T = Transforms()\n",
    "    musTesting = MusdbDataset(T, \"musdb18/\", \"test\")\n",
    "    return musTesting\n",
    "def prepare():\n",
    "    T = Transforms()\n",
    "    musValidation = MusdbDataset(T, \"musdb18/\", \"valid\", \"train\")\n",
    "    musTraining = MusdbDataset(T, \"musdb18/\")\n",
    "    return musValidation, musTraining\n",
    "\n",
    "def run(musValidation, musTraining):\n",
    "    model = Model([20,20,20,30,30,30,30,30,30,30,30,30,30,50,50,50,50,70,70,100,100,125],128)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr = 0.001)\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    writer = SummaryWriter('runs/train_{}'.format(timestamp))\n",
    "    check_memory()\n",
    "    train = Train(musTraining, musValidation, collate, writer, optimizer, model, 0.0015, SSloss, epochs = 1, report_loss_frequency = 10)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c78feb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabee023",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697d9a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformsModule(nn.Module):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84ed42d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1271bda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    # we define a custom function that overrides the PyTorch built in collate in order to make sure\n",
    "    # our input to the model matches the dimensionality we want.\n",
    "    # our input to the function is a list of (input, label) tuples. The list will have size = batch_size as defined in the DataLoader\n",
    "    # We will use a batch_size of 1 to receive a list of tuples (in this case a list with 1 tuple), each tuple has two elements:\n",
    "    # First, the input, a tensor with shape (batch_size, stft_dim_F, stft_dim_T, 2)\n",
    "    # Second, the label, a tensor with shape (num_stems - 1, batch_size, stft_dim_F, stft_dim_T, 2)\n",
    "    input, chromas, mfccs, labels = batch[0]\n",
    "    return input, chromas, mfccs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e94fada1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_it(start, to_print=\"TEST\"):\n",
    "    end = time.time()\n",
    "    print(to_print, \" RUNTIME: \", end - start)\n",
    "    return end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20f2ad2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e6c9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusdbDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, transforms, musdb_root, split='train', subset='train', is_wav=False, sample_rate=44100, segment_length = 10, segment_chunks = 10, discard_low_energy = True, segment_overlap = 0.5, drop_percentile =  0.1, chunks_below_percentile = 0.5):\n",
    "        # Check if a GPU is available\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.device(\"cuda\")\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "        assert(subset == 'train' or subset == 'test')\n",
    "        self.mode = subset\n",
    "        self.mus = musdb.DB(musdb_root, subsets=subset, split=split, is_wav=is_wav)\n",
    "        self.transforms = transforms\n",
    "        self.sample_rate = sample_rate\n",
    "        self.split = split\n",
    "        self.discard_low_energy = discard_low_energy\n",
    "        self.segment_length = segment_length\n",
    "        self.segment_chunks = segment_chunks\n",
    "        self.chunks_below_percentile = chunks_below_percentile\n",
    "        self.segment_overlap = segment_overlap\n",
    "        self.drop_percentile = drop_percentile\n",
    "        self.segment_samples = int(self.segment_length * self.sample_rate)\n",
    "        self.stft_size = self.STFT_dimensions(self.segment_samples)\n",
    "        self.chunk_samples = int(self.segment_samples / self.segment_chunks)\n",
    "        self.num_stems = self.mus.tracks[0].stems.shape[0]\n",
    "        self.num_channels = self.mus.tracks[0].stems.shape[2]\n",
    "        self.durations = dict()\n",
    "        self.filtered_indices = dict()\n",
    "        self.len = self.init_durations()\n",
    "        self.shortest_duration = self.shortest_duration_in_samples(self.mus)\n",
    "        self.batch_size = self.find_batch_size()\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # this function should return a batch of segment STFTs from the song as well as their stem STFTs.\n",
    "        start = time.time()\n",
    "        track = self.mus.tracks[self.filtered_indices[idx]]\n",
    "        # stems is a list of the stems of the track, in the order of the stems in the track\n",
    "        stems = torch.as_tensor(track.stems, device = self.device)\n",
    "        check = time_it(start, \"retreive tracks as tensor\")\n",
    "        if self.mode == 'train':\n",
    "            segments = self._batchize_training_item(stems)\n",
    "        else:\n",
    "            segments = self._batchize_testing_item(stems)\n",
    "        check = time_it(check, \"get segments as batch tensor\")\n",
    "        # we apply a STFT to each of those segments, and that is how we achieve our constant batch and input sizes.\n",
    "        stfts = self.stft_segments(segments)\n",
    "        check = time_it(check, \"STFTs\")\n",
    "        chromas, mfccs = self.chroma_and_mfcc_segments(segments)\n",
    "        check = time_it(check, \"MFCC CHROMA\")\n",
    "        # we then return the segments as a torch tensor\n",
    "        data = stfts[:,0]\n",
    "        labels = stfts[:,1:]\n",
    "        print(\"processing \", track.name)\n",
    "        return data, chromas, mfccs, labels\n",
    "\n",
    "    def _batchize_training_item(self,stems):\n",
    "        # we need to trim the stems to the shortest duration track, starting from a random location\n",
    "        stems = self.trim_stems(stems, self.random_start(stems.shape[1]))\n",
    "        # we split the stems from shape (num_stems, num_samples, num_channels) into a tensor with shape (num_stems, num_segments, num_samples_per_segment, num_channels)\n",
    "        segments = self.split_track(stems)\n",
    "        start = time.time()\n",
    "        # now we need to drop out the low energy segments\n",
    "        segments = self.high_energy_segments(segments)\n",
    "        time_it(start, \"HIGH ENERGY SEGMENTS CHECK\")\n",
    "        # then we choose a random and continuous yet constant number of segments from the track minus the dropped segments\n",
    "        return segments\n",
    "\n",
    "    def _batchize_testing_item(self,stems):\n",
    "        # first we make sure the length of the song will produce a whole number of segments by padding with zeros to the end of the next segment.\n",
    "        stems = self.add_zero_padding(stems)\n",
    "        # now we extend the song with zeros to make sure we have an equal batch size for every output.\n",
    "        to_pad = self.longest_duration_in_samples() - stems.shape[1]\n",
    "        stems = self.add_N_zeros(stems, to_pad)\n",
    "        # now we split the stems into equal size segments\n",
    "        segments = self.split_track(stems)\n",
    "        # we are now ready to operate on the song since we don't want to drop or modify our data as we conserve it to reconstruct our signal.\n",
    "        return segments\n",
    "\n",
    "    def collate(batch):\n",
    "        # we define a custom function that overrides the PyTorch built in collate in order to make sure\n",
    "        # our input to the model matches the dimensionality we want.\n",
    "        # our input to the function is a list of (input, label) tuples. The list will have size = batch_size as defined in the DataLoader\n",
    "        # We will use a batch_size of 1 to receive a list of tuples (in this case a list with 1 tuple), each tuple has two elements:\n",
    "        # First, the input, a tensor with shape (batch_size, stft_dim_F, stft_dim_T, 2)\n",
    "        # Second, the label, a tensor with shape (num_stems - 1, batch_size, stft_dim_F, stft_dim_T, 2)\n",
    "        input, chromas, mfccs, labels = batch[0]\n",
    "        return input, chromas, mfccs, labels\n",
    "\n",
    "    def init_durations(self):\n",
    "        pos = 0\n",
    "        for idx, track in enumerate(self.mus.tracks):\n",
    "            print(track.name)\n",
    "            self.durations[idx] = track.stems.shape[1]\n",
    "            if self.durations[idx] >= self.segment_samples * 9:\n",
    "                self.filtered_indices[pos] = idx\n",
    "                pos += 1\n",
    "        return pos\n",
    "\n",
    "    def stft_segments(self, segments):\n",
    "        # this function should take in a list of segments and apply a STFT to each of them\n",
    "        # it should return a tensor of STFTs, of shape (num_stems, batch_size, STFT_F, STFT_T, num_channels)\n",
    "        # where batch_size is the number of STFTs we can fit into a batch\n",
    "        # STFT_F is the number of frequency bins in the STFT\n",
    "        # STFT_T is the number of time bins in the STFT\n",
    "        # num_stems is the number of stems in the track\n",
    "        # num_channels is the number of channels in the track\n",
    "        # the STFTs should be applied to each segment, and then the segments should be concatenated along the batch_size axis\n",
    "        stem_stfts = []\n",
    "        batch_size = self.batch_size\n",
    "        # we then keep only the first batch_size segments, and apply a STFT to each of them. \n",
    "        all_segments = segments[:batch_size, :, :, :].view(self.num_stems, self.num_channels, batch_size, self.segment_samples)\n",
    "        for i in range(self.num_stems):\n",
    "            channel_stfts = []\n",
    "            # we take the first batch of stfts, we reshape it to allow for stft operation using torch.stft and taking advantage of 2D batched STFTs\n",
    "            for j in range(self.num_channels): \n",
    "                # we apply a STFT to each segment\n",
    "                stem_batch_channel_segments = all_segments[i,j,:,:]\n",
    "                batch_stfts = self.transforms.stft(stem_batch_channel_segments)\n",
    "                channel_stfts.append(batch_stfts)\n",
    "                # we then add the STFT to the list of STFTs\n",
    "            stem_stfts.append(torch.stack(channel_stfts))\n",
    "        stem_stfts = torch.stack(stem_stfts)\n",
    "        return stem_stfts.view(batch_size, self.num_stems, self.num_channels, self.stft_size[0], self.stft_size[1], 2)\n",
    "        \n",
    "    def chroma_and_mfcc_segments(self, segments):\n",
    "        # this function should take in a list of segments and apply a STFT to each of them\n",
    "        # it should return a tensor of STFTs, of shape (num_stems, batch_size, STFT_F, STFT_T, num_channels)\n",
    "        # where batch_size is the number of STFTs we can fit into a batch\n",
    "        # STFT_F is the number of frequency bins in the STFT\n",
    "        # STFT_T is the number of time bins in the STFT\n",
    "        # num_stems is the number of stems in the track\n",
    "        # num_channels is the number of channels in the track\n",
    "        # the STFTs should be applied to each segment, and then the segments should be concatenated along the batch_size axis\n",
    "        batch_size = self.batch_size\n",
    "        # we then keep only the first batch_size segments, and apply a STFT to each of them. \n",
    "        all_segments = segments[:batch_size, :, :, :].view(self.num_stems, self.num_channels, batch_size, self.segment_samples)\n",
    "        channel_chromas = []\n",
    "        channel_mfccs = []\n",
    "        # we take the first batch of stfts, we reshape it to allow for stft operation using torch.stft and taking advantage of 2D batched STFTs\n",
    "        for j in range(self.num_channels): \n",
    "            # we apply a STFT to each segment\n",
    "            stem_batch_channel_segments = all_segments[0,j,:,:]\n",
    "            batch_chromas = self.transforms.chromagram(stem_batch_channel_segments)\n",
    "            batch_mfccs = self.transforms.mel_spectrogram(stem_batch_channel_segments)\n",
    "            channel_chromas.append(batch_chromas)\n",
    "            channel_mfccs.append(batch_mfccs)\n",
    "            # we then add the STFT to the list of STFTs\n",
    "        channel_mfccs = torch.stack(channel_mfccs)\n",
    "        channel_chromas = torch.stack(channel_chromas)\n",
    "        return channel_chromas.view(batch_size * self.num_channels, 12, self.stft_size[1]),  channel_mfccs.view(batch_size * self.num_channels, self.transforms.n_mels , self.stft_size[1])\n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def trim_stems(self, stems, start):\n",
    "        # this function should trim the track to the shortest duration of the track from the start index, it allows looping back across the song.\n",
    "        if start + self.shortest_duration > stems.shape[1]:\n",
    "            first_half = stems[:, start:, :]\n",
    "            remaining = self.shortest_duration - (stems.shape[1] - start)\n",
    "            second_half = stems[:, :remaining, :]\n",
    "            return torch.cat((first_half, second_half), axis=1)\n",
    "        else:\n",
    "            return stems[:, start:start+self.shortest_duration, :]\n",
    "\n",
    "    def pad_stems(self, stems):\n",
    "        length_in_samples = stems.shape[1]\n",
    "        to_pad = self.longest_duration_in_samples() - length_in_samples\n",
    "\n",
    "    def find_batch_size(self):\n",
    "        # this function should tell us how many STFTs we can fit into a batch based on finding the floor power of 2 of the number of STFTs we fit over the duration of the song\n",
    "        # each STFT will represent a STFT over a fixed segment length.\n",
    "        num_segments = self.num_segments_in_track(self.shortest_duration)\n",
    "        # we anticipate a drop of up to twice the drop percentile (impossible, just to be safe) of the segments.\n",
    "        num_segments = torch.as_tensor(int(num_segments * (1 - 2 * self.drop_percentile)), device = self.device)\n",
    "        # We return the closest power of two to that anticipated number of segments. Of course we use a floor because we want to fill every batch.\n",
    "        return 2 ** int(torch.floor(torch.log2(num_segments)))\n",
    "\n",
    "    def num_segments_in_track(self, duration_in_samples):\n",
    "        # this function should return the number of segments in the track and consider the overlap factor, self.segment_overlap\n",
    "        return int(torch.ceil(torch.as_tensor(duration_in_samples / (self.segment_samples * (1 - self.segment_overlap)), device=self.device)))\n",
    "\n",
    "    def random_start(self, duration_in_samples):\n",
    "        # this function should return a random start index for the track\n",
    "        return torch.randint(0, duration_in_samples, (1,))\n",
    "\n",
    "\n",
    "    def STFT_dimensions(self, duration_in_samples):\n",
    "        # this function should return the dimensions of the STFT of the shortest track in the dataset\n",
    "        F = int(self.transforms.n_fft / 2 + 1)\n",
    "        T = int(torch.ceil(torch.as_tensor(duration_in_samples / self.transforms.hop_length, device = self.device)))\n",
    "        return (F,T)\n",
    "        \n",
    "\n",
    "    def shortest_duration_in_samples(self, mus):\n",
    "        # this function should return the shortest duration of the stems in the track\n",
    "        min = 100000000\n",
    "        for i, dur in self.durations.items():\n",
    "            if i in self.filtered_indices.values():\n",
    "                if dur < min:\n",
    "                    min = dur\n",
    "        return min\n",
    "\n",
    "    def longest_duration_in_samples(self, mus):\n",
    "        # this function should return the longest duration of the stems in the track\n",
    "        return max(min([track.stems.shape[1] for track in mus.tracks]))\n",
    "\n",
    "    def is_high_energy_segment(self, segment, threshold):\n",
    "        # this function decides based on the provided threshold whether a sufficient number of chunks in the segment have an energy above the threshold\n",
    "        mix_chunk_energies = self.segment_chunk_energies(segment)[:]\n",
    "        return len(torch.argwhere(mix_chunk_energies > threshold)) > (self.chunks_below_percentile * self.segment_chunks)\n",
    "\n",
    "    def high_energy_segments(self, segments):\n",
    "        # this function should take in a full track's stems, it will then split the track into segments. \n",
    "        # The segments should have an overlap factor of self.segment_overlap.\n",
    "        # Then, it will split each segment into chunks, and it will calculate the energy of each segment, and store the energy of each segment in a list.\n",
    "        # With this list, it will calculate the percentile of the energy of the chunks, and it will discard the segments where 25% of the chunks have an energy below the percentile.\n",
    "        # It will then return the list of segments that have a high enough energy.\n",
    "        high_energy_indices = []\n",
    "        threshold = self.segment_energy_threshold(segments)\n",
    "        for idx, segment in enumerate(segments):\n",
    "            if self.is_high_energy_segment(segment, threshold):\n",
    "                high_energy_indices.append(idx)\n",
    "        high_energy_indices = torch.as_tensor(high_energy_indices, device = self.device)\n",
    "        return torch.index_select(segments, 0, high_energy_indices)\n",
    "\n",
    "    def segment_energy_threshold(self, segments):\n",
    "        # this function should split every segment into self.segment_chunks chunks, and it will calculate the energy of each chunk using the RMS energy function.\n",
    "        # It will save the energy of each chunk in a list, and it will return the value self.percentile_dropped percentile of the list.\n",
    "        chunk_energies = []\n",
    "        for segment in segments:\n",
    "            chunk_energies.extend(self.segment_chunk_energies(segment))\n",
    "        chunk_energies = torch.stack(chunk_energies)\n",
    "        percentile = torch.quantile(chunk_energies, self.drop_percentile, interpolation='midpoint')\n",
    "        return percentile\n",
    "            \n",
    "    def segment_chunk_energies(self, segment):\n",
    "        # this function should split the segment into self.segment_chunks chunks, and it will calculate the energy of each chunk using the RMS energy function.\n",
    "        # It will save the energy of each chunk in a list, and it will return the list.\n",
    "        chunk_energies = []\n",
    "        segment_samples = segment.shape[1]\n",
    "        chunk_samples = int(segment_samples / self.segment_chunks)\n",
    "        for i in range(0, segment_samples, chunk_samples):\n",
    "            chunk = segment[:, i:i+chunk_samples]\n",
    "            mix_track = chunk[0,:,:]\n",
    "            rms = self.transforms.RMS(mix_track)\n",
    "            chunk_energies.append(rms)\n",
    "        return torch.stack(chunk_energies).view(self.segment_chunks)\n",
    "\n",
    "    def split_track(self, stems):\n",
    "        # this function should take in a full track, and it will split the track into segments. \n",
    "        # The segments should have an overlap factor of self.segment_overlap.\n",
    "        # We add zero padding to the track to make sure that the track is divisible by the segment length.\n",
    "        # Then, it will split each segment into chunks, and it will return the list of chunks.\n",
    "        # The input is a tensor with shape (num_stems, num_samples, num_channels)\n",
    "        # The output is a tensor array with shape (num_stems, num_segments, num_samples_per_segment, num_channels)\n",
    "        stems = self.add_zero_padding(stems)\n",
    "        segments = []\n",
    "        num_samples = stems.shape[1]\n",
    "        step_in_samples = int(self.segment_samples * (1 - self.segment_overlap))\n",
    "        for i in range(0, num_samples - step_in_samples, step_in_samples):\n",
    "            segment = stems[:, i:i+self.segment_samples]\n",
    "            segments.append(segment)\n",
    "        segments = torch.stack(segments)\n",
    "        return segments\n",
    "\n",
    "    def add_zero_padding(self, stems):\n",
    "        # this function should add zero padding to the track to make sure that the track is divisible by the segment length and the residue from the overlap.\n",
    "        # the length of the array has to be segment_length + k * samples_in_steps for some nonnegative integer k.\n",
    "        num_samples = stems.shape[1]\n",
    "        step_in_samples = int(self.segment_samples * (1 - self.segment_overlap))\n",
    "        samples_in_last_segment = num_samples % step_in_samples\n",
    "        if samples_in_last_segment != 0:\n",
    "            padding = torch.zeros((stems.shape[0], step_in_samples - samples_in_last_segment, stems.shape[2]), device = self.device)\n",
    "            return torch.cat((stems, padding), axis=1)\n",
    "        else:\n",
    "            return stems\n",
    "\n",
    "    def add_N_zeros(self, stems, N):\n",
    "        zeros = torch.zeros((stems.shape[0], N, stems.shape[2]))\n",
    "        return torch.cat((stems, zeros), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f80a5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SSloss(predicted_stfts, real_stfts):\n",
    "    # This function will calculate the loss for a given batch of predicted and real stems\n",
    "    # The loss will be the sum of the losses for each stem\n",
    "    # the dimensions of the predicted stems and real stems will be (batch_size, num_stems, stft_dim_F, stft_dim_T,num_channels)\n",
    "    # The loss will be a scalar\n",
    "    stft_loss = stft_mean_absolute_error(predicted_stfts, real_stfts)\n",
    "    # We want to find the signal representation from the STFT we computed. We can do this by taking the inverse STFT\n",
    "    # of each segment (input in the batch) separately and compare them. We don't need perfect reconstruction for our loss function,\n",
    "    # but we will need it to evaluate the model.\n",
    "    predicted_time_signals = batch_time_signals(predicted_stfts)\n",
    "    real_time_signals = batch_time_signals(real_stfts)\n",
    "    signal_loss = signal_mean_absolute_error(predicted_time_signals, real_time_signals)\n",
    "    return signal_loss + stft_loss\n",
    "\n",
    "def batch_time_signals(stfts):\n",
    "    # This function should compute the time signals for each segment in the batch\n",
    "    # input shape (batch_size, num_stems, stft_dim_F, stft_dim_T,num_channels)\n",
    "    # output shape (batch_size, num_stems, segment_samples, num_channels)\n",
    "    istft_list = []\n",
    "    for i in range(stfts.shape[0]):\n",
    "        for j in range(stfts.shape[1]):\n",
    "            istft = torch.istft(stfts[i, j], n_fft=2048, hop_length=1024, win_length=2048, window=torch.hann_window(2048, device = torch.device(\"cuda\")))\n",
    "            istft_list.append(istft)\n",
    "    istfts = torch.stack(istft_list, dim=0)\n",
    "    return istfts\n",
    "\n",
    "    \n",
    "    \n",
    "def signal_mean_absolute_error(predicted_time_signal, real_time_signal):\n",
    "    # This function will calculate the mean absolute error between the predicted and real time signals\n",
    "    # The dimensions of the predicted and real time signals will be (batch_size, num_stems, stft_dim_F, stft_dim_T,num_channels)\n",
    "    return torch.mean(torch.abs(predicted_time_signal - real_time_signal))\n",
    "\n",
    "def stft_mean_absolute_error(predicted_stfts, real_stfts):\n",
    "    # This function will calculate the mean absolute error between the predicted and real STFTs\n",
    "    # The dimensions of the predicted and real STFTs will be (batch_size, num_stems, stft_dim_F, stft_dim_T,num_channels)\n",
    "    return torch.mean(torch.abs(predicted_stfts - real_stfts))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d74837",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transforms():\n",
    "    #def __init__(self, sample_rate=44100, n_fft=2048, hop_length=1024, win_length=2048, window='hann', center=True, pad_mode='reflect', power=2.0, n_mels=128, fmin=0.0, fmax=None, htk=False, norm=1, top_db=80.0, ref=1.0, amin=1e-10):\n",
    "    def __init__(self, sample_rate=44100, n_fft=2048, hop_length=1024, win_length=2048, window='hann', center=True, pad_mode='reflect', power=2.0, n_mels=32, fmin=0.0, fmax=None, htk=False, norm=1, top_db=80.0, ref=1.0, amin=1e-10):\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.device(\"cuda\")\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "        self.sample_rate = sample_rate\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.win_length = win_length\n",
    "        self.window = window\n",
    "        self.center = center\n",
    "        self.pad_mode = pad_mode\n",
    "        self.power = power\n",
    "        self.n_mels = n_mels\n",
    "        self.fmin = fmin\n",
    "        self.fmax = fmax\n",
    "        self.htk = htk\n",
    "        self.norm = norm\n",
    "        self.top_db = top_db\n",
    "        self.ref = ref\n",
    "        self.amin = amin\n",
    "\n",
    "\n",
    "    def stft(self, x, n_fft=2048, hop_length=1024, win_length=2048, window=torch.hann_window(2048, device = torch.device('cuda')), center=True, pad_mode='reflect'):\n",
    "        return torch.stft(x, n_fft=n_fft, hop_length=hop_length, win_length=win_length, window=window, center=center, pad_mode=pad_mode, onesided=True, return_complex=False)\n",
    "\n",
    "    def istft(self, x, n_fft=2048, hop_length=1024, win_length=2048, window=torch.hann_window(2048,device = torch.device('cuda')), center=True):\n",
    "        return torch.istft(x, n_fft=n_fft, hop_length=hop_length, win_length=win_length, window=window, center=center, onesided=True)\n",
    "\n",
    "    def mel_spectrogram(self, x, sr=44100, n_fft=2048, hop_length = 1024, win_length = 2048, window = 'hann', n_mels=32):\n",
    "       # return self.mfcc_transform(x)\n",
    "        return torch.as_tensor(librosa.feature.melspectrogram(x.detach().cpu().numpy(), sr = 44100, n_fft = n_fft, hop_length = hop_length, window = window, n_mels = n_mels, win_length = win_length))\n",
    "    def chromagram(self, x, sr = 44100, n_fft = 2048, hop_length = 1024, win_length = 2048, window = 'hann'):\n",
    "        device = x.get_device()\n",
    "        return torch.as_tensor(librosa.feature.chroma_stft(x.detach().cpu().numpy(), sr=sr, n_fft = n_fft, hop_length = hop_length, win_length = win_length, window = window), device = device)\n",
    "    def amplitude_to_db(self, x, top_db=80.0, ref=1.0, amin=1e-10):\n",
    "        return librosa.amplitude_to_db(x, top_db=top_db, ref=ref, amin=amin)\n",
    "\n",
    "    def RMS(self, audio_tensor):\n",
    "        # this function is used to calculate the RMS of the audio signal\n",
    "        # the input will be an audio tensor of shape (num_samples)\n",
    "        # Note that this function is intended to use on smaller audio signals, and that\n",
    "        # typically, longer signals get windows similar to the STFT in order to calculate more localized\n",
    "        # RMS. The purpose of using this was for silent signal detection.\n",
    "        squared_tensor = torch.pow(audio_tensor, 2)\n",
    "        mean_power = torch.mean(squared_tensor)\n",
    "        rms = torch.sqrt(mean_power)\n",
    "        return rms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acda13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train():\n",
    "    def __init__(self, train_dataset, validation_dataset, collate, summary_writer, optimizer, model, learning_rate, loss, epochs = 1000, report_loss_frequency=15):\n",
    "        self.train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=1, collate_fn=collate)\n",
    "        self.validation_dataloader = torch.utils.data.DataLoader(validation_dataset, batch_size=1, collate_fn=collate)\n",
    "        self.writer = summary_writer\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss = loss\n",
    "        self.report_loss_frequency = report_loss_frequency\n",
    "        self.run_model(epochs=epochs)\n",
    "\n",
    "    def train_one_epoch(self, epoch_number):\n",
    "        accumulating_loss = 0.0\n",
    "        last_loss = 0.0\n",
    "        for i, data in enumerate(self.train_dataloader):\n",
    "            # We will use the PyTorch Data Loader class to easily iterate through the data and collect our batches.\n",
    "            # See musdb.py for a deeper explanation of how we collect batches. Simply speaking, we take a an arbitrary\n",
    "            # fixed-length part of the song and we split it into fixed length segments, taking the STFT of each. \n",
    "            mixture_stft, chromas, mfccs, real_stems_stft = data\n",
    "            # PyTorch accumulates gradients by default, so we zero out the gradients before each batch update in order to \n",
    "            # ensure that the optimizer only uses the gradients from the current batch during the update step.\n",
    "            # This also should help with performance and memory costs.\n",
    "            self.optimizer.zero_grad()\n",
    "            # We call our model to make a prediction\n",
    "            predicted_stems_stft = self.model(mixture_stft, chromas, mfccs)\n",
    "            # We compute the loss from our loss function defined in loss.py\n",
    "            loss = self.loss(predicted_stems_stft, real_stems_stft[:,3])\n",
    "            # We calculate the gradient\n",
    "            loss.backward()\n",
    "            # Then update the weights based on the gradient\n",
    "            self.optimizer.step()\n",
    "            # In order to report our loss, we print the loss every 15 batches\n",
    "            accumulating_loss += loss.item()\n",
    "            if i % self.report_loss_frequency == self.report_loss_frequency - 1:\n",
    "                last_loss = accumulating_loss / self.report_loss_frequency\n",
    "                print(\"Batch {}. Average loss over last {} batches is: {}\".format(i+1, self.report_loss_frequency, last_loss))\n",
    "                writer_index = epoch_number * len(self.train_dataloader) + i + 1\n",
    "                self.writer.add_scalar('Loss/train', last_loss, writer_index)\n",
    "                accumulating_loss = 0\n",
    "            \n",
    "        return last_loss\n",
    "\n",
    "    def run_model(self, epochs):\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        epoch_number = 0\n",
    "        #set arbitrarily large initial validation loss\n",
    "        best_vloss = 2.0 ** 30\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            print(\"STARTING EPOCH {}\".format(epoch_number + 1))\n",
    "            # We want to learn in our learning loops, so we make sure our gradients are updating during each epoch.\n",
    "            self.model.train(True)\n",
    "            avg_loss = self.train_one_epoch(epoch_number)\n",
    "            # We don't need or want the gradient opertaion on any of the \n",
    "            self.model.train(False)\n",
    "\n",
    "            accumulating_vloss = 0.0\n",
    "\n",
    "            for i, vdata in enumerate(self.validation_dataloader):\n",
    "                vmixture_stft, vchromas, vmfccs, vreal_stems_stft = vdata\n",
    "                vpredicted_stems_stft = self.model(vmixture_stft, vchromas, vmfccs)\n",
    "                vloss = self.loss(vpredicted_stems_stft, vreal_stems_stft[:,3])\n",
    "                accumulating_vloss += vloss\n",
    "                if i % self.report_loss_frequency == 0:\n",
    "                    print(\"Validation batch \", i, \" loss: \", vloss)\n",
    "                \n",
    "            \n",
    "            avg_vloss = accumulating_vloss / (i + 1)\n",
    "            print('LOSS training set: {} validation set: {}'.format(avg_loss, avg_vloss))\n",
    "            \n",
    "            # log all of this \n",
    "            self.writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "            self.writer.flush()\n",
    "\n",
    "            if avg_vloss < best_vloss:\n",
    "                best_vloss = avg_vloss\n",
    "                model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "                torch.save(self.model.state_dict(), model_path)\n",
    "            \n",
    "            epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef72278c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path):\n",
    "    model = my_model()\n",
    "    model.load_state_dict(model_path)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "class Evaluate():\n",
    "    def __init__(self, test_dataset, model_path):\n",
    "        self.test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size = 1, collate_fn = collate)\n",
    "        self.model = load_model(model_path)\n",
    "        self.mus = test_dataset.mus\n",
    "\n",
    "    def reconstruct_all_and_evaluate(self):\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(self.test_dataloader):\n",
    "                mixture_stft, _ = data\n",
    "                predicted_stems_stft = self.model(mixture_stft)\n",
    "                self.reconstruct_one_and_evaluate(predicted_stems_stft, i)\n",
    "\n",
    "    def reconstruct_one_and_evaluate(self, predicted_stems_stfts, i): \n",
    "        stem_estimates_time_signals = self.reconstruct_one_song(predicted_stems_stfts)\n",
    "        self.evaluate_one_song(stem_estimates_time_signals, i)\n",
    "\n",
    "    def evaluate_one_song(self, stems_estimates_time_signals, i):\n",
    "        # this function will evaluate the song using the museval package.\n",
    "        # it will receive the time signals of the stems and the song number in the dataset.\n",
    "        # it will both save the results in the results and print the scores after evaluating each song.\n",
    "        scores = museval.eval_mus_track(self.mus[i], stems_estimates_time_signals, output_dir='results')   \n",
    "        print(scores)\n",
    "\n",
    "    def reconstruct_one_song(self, predicted_stems_stft):\n",
    "        # this function will reconstruct the song from the predicted stems stft and return the time signals of the stems.\n",
    "        # We will use the overlap and add method to reconstruct the song after we recover the time signal for each segment for each stem.\n",
    "        time_signals = batch_time_signals(predicted_stems_stft)\n",
    "        stem_estimates_time_signals = self.overlap_and_add(time_signals)\n",
    "        return stem_estimates_time_signals\n",
    "\n",
    "    def overlap_and_add(self, time_signals):\n",
    "        # receives all the reconstructed time signals and overlaps them into one signal to reconstruct the original song.\n",
    "        # this will happen based on the parameters in the dataset for segment length and overlap.\n",
    "        step_in_samples = time_signals.shape[2] * (1 - self.test_dataset.overlap)\n",
    "        segment_length = time_signals.shape[2]\n",
    "        num_segments = time_signals.shape[1]\n",
    "        full_signal_length = segment_length + step_in_samples * (num_segments - 1)\n",
    "        new_signal = torch.zeros((time_signals.shape[0], full_signal_length))\n",
    "        for i in range(num_segments):\n",
    "            new_signal[:, i * step_in_samples : i * step_in_samples + segment_length] += time_signals[:, i, :]\n",
    "        return new_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1f079dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM memory % used: 24.26   TOTAL =  136878\n",
      "RAM memory % used: 26.8\n",
      "RAM Used (GB): 34.75286016\n"
     ]
    }
   ],
   "source": [
    "# UTILITIES\n",
    "\n",
    "import os\n",
    "import psutil\n",
    "total_memory, used_memory, free_memory = map(\n",
    "    int, os.popen('free -t -m').readlines()[-1].split()[1:])\n",
    " \n",
    "# Memory usage\n",
    "print(\"RAM memory % used:\", round((used_memory/total_memory) * 100, 2), \"  TOTAL = \", total_memory)\n",
    "# Importing the library\n",
    "\n",
    " \n",
    "# Getting % usage of virtual_memory ( 3rd field)\n",
    "print('RAM memory % used:', psutil.virtual_memory()[2])\n",
    "# Getting usage of virtual_memory in GB ( 4th field)\n",
    "print('RAM Used (GB):', psutil.virtual_memory()[3]/1000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "831540f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(path):\n",
    "    lightning = LightningModel.load_from_checkpoint('lightning_logs/version_1/checkpoints/epoch=2-step=222.ckpt')\n",
    "    return lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a485f36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_memory():\n",
    "    t = torch.cuda.get_device_properties(0).total_memory\n",
    "    r = torch.cuda.memory_reserved(0)\n",
    "    a = torch.cuda.memory_allocated(0)\n",
    "    f = r-a  # free inside reserved\n",
    "    GB = 1024 ** 3\n",
    "    print(\"Total: \", t / GB)\n",
    "    print(\"Reserved \", r / GB)\n",
    "    print(\"Allocated: \", a / GB)\n",
    "    print(\"free: \", f / GB)\n",
    "    \n",
    "def free_memory(var):\n",
    "    del var\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c914597",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae58560c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_one_song(predicted_stems_stft):\n",
    "    # this function will reconstruct the song from the predicted stems stft and return the time signals of the stems.\n",
    "    # We will use the overlap and add method to reconstruct the song after we recover the time signal for each segment for each stem.\n",
    "    time_signals = batch_time_signals(predicted_stems_stft)\n",
    "    stem_estimates_time_signals = overlap_and_add(time_signals, 0.5)\n",
    "    return stem_estimates_time_signals\n",
    "\n",
    "def overlap_and_add(time_signals, overlap):\n",
    "    # receives all the reconstructed time signals and overlaps them into one signal to reconstruct the original song.\n",
    "    # this will happen based on the parameters in the dataset for segment length and overlap.\n",
    "    step_in_samples = int(time_signals.shape[1]* (1 - overlap))\n",
    "    segment_length = time_signals.shape[1]\n",
    "    num_segments = time_signals.shape[0]\n",
    "    full_signal_length = int(segment_length + step_in_samples * (num_segments - 1))\n",
    "    new_signal = torch.zeros(full_signal_length).to('cuda:0')\n",
    "    for i in range(num_segments):\n",
    "        new_signal[i * step_in_samples : i * step_in_samples + segment_length] += time_signals[i, :]\n",
    "    return new_signal\n",
    "\n",
    "reconstruct_one_song(y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3ded30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87784a1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
