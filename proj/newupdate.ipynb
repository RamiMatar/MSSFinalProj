{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8073bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "from torchaudio.transforms import *\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import musdb\n",
    "import museval \n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c54cdb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataHandler():\n",
    "    def __init__(self,batch_size,shortest_duration, sampling_rate, longest_duration, segment_overlap, segment_chunks, segment_length, chunks_below_percentile, drop_percentile):\n",
    "        self.sample_rate = sampling_rate\n",
    "        self.shortest_duration = shortest_duration\n",
    "        self.longest_duration = longest_duration\n",
    "        self.segment_length = segment_length\n",
    "        self.segment_chunks = segment_chunks\n",
    "        self.chunks_below_percentile = chunks_below_percentile\n",
    "        self.segment_overlap = segment_overlap\n",
    "        self.drop_percentile = drop_percentile\n",
    "        self.segment_samples = int(self.segment_length * self.sample_rate)\n",
    "        self.chunk_samples = int(self.segment_samples / self.segment_chunks)\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def batchize_training_item(self,stems):\n",
    "        # we need to trim the stems to the shortest duration track, starting from a random location\n",
    "        stems = self.trim_stems(stems, self.random_start(stems.shape[1]))\n",
    "        # we split the stems from shape (num_stems, num_samples, num_channels) into a tensor with shape (num_stems, num_segments, num_samples_per_segment, num_channels)\n",
    "        print(stems.shape)\n",
    "        segments = self.split_track(stems)\n",
    "        print(segments.shape)\n",
    "        start = time.time()\n",
    "        # now we need to drop out the low energy segments\n",
    "        segments = self.high_energy_segments(segments)\n",
    "        print(segments.shape)\n",
    "        time_it(start, \"HIGH ENERGY SEGMENTS CHECK\")\n",
    "        # then we choose a random and continuous yet constant number of segments from the track minus the dropped segments\n",
    "        return segments[:self.batch_size]\n",
    "\n",
    "    def batchize_testing_item(self,stems):\n",
    "        # first we make sure the length of the song will produce a whole number of segments by padding with zeros to the end of the next segment.\n",
    "        stems = self.add_zero_padding(stems)\n",
    "        # now we extend the song with zeros to make sure we have an equal batch size for every output.\n",
    "        to_pad = self.longest_duration_in_samples() - stems.shape[1]\n",
    "        stems = self.add_N_zeros(stems, to_pad)\n",
    "        # now we split the stems into equal size segments\n",
    "        segments = self.split_track(stems)\n",
    "        # we are now ready to operate on the song since we don't want to drop or modify our data as we conserve it to reconstruct our signal.\n",
    "        return segments[:self.batch_size]\n",
    "        \n",
    "        \n",
    "    def trim_stems(self, stems, start):\n",
    "        # this function should trim the track to the shortest duration of the track from the start index, it allows looping back across the song.\n",
    "        if start + self.shortest_duration > stems.shape[1]:\n",
    "            first_half = stems[:, start:, :]\n",
    "            remaining = self.shortest_duration - (stems.shape[1] - start)\n",
    "            second_half = stems[:, :remaining, :]\n",
    "            return torch.cat((first_half, second_half), axis=1)\n",
    "        else:\n",
    "            return stems[:, start:start+self.shortest_duration, :]\n",
    "\n",
    "    def pad_stems(self, stems):\n",
    "        length_in_samples = stems.shape[1]\n",
    "        to_pad = self.longest_duration_in_samples() - length_in_samples\n",
    "\n",
    "    def num_segments_in_track(self, duration_in_samples):\n",
    "        # this function should return the number of segments in the track and consider the overlap factor, self.segment_overlap\n",
    "        return int(torch.ceil(torch.Tensor(duration_in_samples / (self.segment_samples * (1 - self.segment_overlap)))))\n",
    "\n",
    "    def random_start(self, duration_in_samples):\n",
    "        # this function should return a random start index for the track\n",
    "        return torch.randint(0, duration_in_samples, (1,))\n",
    "\n",
    "\n",
    "    def longest_duration_in_samples(self, mus):\n",
    "        # this function should return the longest duration of the stems in the track\n",
    "        return max(min([track.stems.shape[1] for track in mus.tracks]))\n",
    "\n",
    "    def is_high_energy_segment(self, segment, threshold):\n",
    "        # this function decides based on the provided threshold whether a sufficient number of chunks in the segment have an energy above the threshold\n",
    "        mix_chunk_energies = self.segment_chunk_energies(segment)[:]\n",
    "        return len(torch.argwhere(mix_chunk_energies > threshold)) > (self.chunks_below_percentile * self.segment_chunks)\n",
    "\n",
    "    def high_energy_segments(self, segments):\n",
    "        # this function should take in a full track's stems, it will then split the track into segments. \n",
    "        # The segments should have an overlap factor of self.segment_overlap.\n",
    "        # Then, it will split each segment into chunks, and it will calculate the energy of each segment, and store the energy of each segment in a list.\n",
    "        # With this list, it will calculate the percentile of the energy of the chunks, and it will discard the segments where 25% of the chunks have an energy below the percentile.\n",
    "        # It will then return the list of segments that have a high enough energy.\n",
    "        high_energy_indices = []\n",
    "        threshold = self.segment_energy_threshold(segments)\n",
    "        for idx, segment in enumerate(segments):\n",
    "            if self.is_high_energy_segment(segment, threshold):\n",
    "                high_energy_indices.append(idx)\n",
    "        high_energy_indices = torch.tensor(high_energy_indices, dtype= torch.int)\n",
    "        return torch.index_select(segments, 0, high_energy_indices)\n",
    "\n",
    "    def segment_energy_threshold(self, segments):\n",
    "        # this function should split every segment into self.segment_chunks chunks, and it will calculate the energy of each chunk using the RMS energy function.\n",
    "        # It will save the energy of each chunk in a list, and it will return the value self.percentile_dropped percentile of the list.\n",
    "        chunk_energies = []\n",
    "        for segment in segments:\n",
    "            chunk_energies.extend(self.segment_chunk_energies(segment))\n",
    "        chunk_energies = torch.stack(chunk_energies)\n",
    "        percentile = torch.quantile(chunk_energies, self.drop_percentile, interpolation='midpoint')\n",
    "        return percentile\n",
    "            \n",
    "    def segment_chunk_energies(self, segment):\n",
    "        # this function should split the segment into self.segment_chunks chunks, and it will calculate the energy of each chunk using the RMS energy function.\n",
    "        # It will save the energy of each chunk in a list, and it will return the list.\n",
    "        chunk_energies = []\n",
    "        segment_samples = segment.shape[1]\n",
    "        chunk_samples = int(segment_samples / self.segment_chunks)\n",
    "        for i in range(0, segment_samples, chunk_samples):\n",
    "            chunk = segment[:, i:i+chunk_samples]\n",
    "            mix_track = chunk[0,:,:]\n",
    "            squared_tensor = torch.pow(chunk, 2)\n",
    "            mean_power = torch.mean(squared_tensor)\n",
    "            rms = torch.sqrt(mean_power)\n",
    "            chunk_energies.append(rms)\n",
    "        return torch.stack(chunk_energies).view(self.segment_chunks)\n",
    "\n",
    "    def split_track(self, stems):\n",
    "        # this function should take in a full track, and it will split the track into segments. \n",
    "        # The segments should have an overlap factor of self.segment_overlap.\n",
    "        # We add zero padding to the track to make sure that the track is divisible by the segment length.\n",
    "        # Then, it will split each segment into chunks, and it will return the list of chunks.\n",
    "        # The input is a tensor with shape (num_stems, num_samples, num_channels)\n",
    "        # The output is a tensor array with shape (num_stems, num_segments, num_samples_per_segment, num_channels)\n",
    "        stems = self.add_zero_padding(stems)\n",
    "        segments = []\n",
    "        num_samples = stems.shape[1]\n",
    "        step_in_samples = int(self.segment_samples * (1 - self.segment_overlap))\n",
    "        for i in range(0, num_samples - step_in_samples, step_in_samples):\n",
    "            segment = stems[:, i:i+self.segment_samples]\n",
    "            segments.append(segment)\n",
    "        segments = torch.stack(segments)\n",
    "        return segments\n",
    "\n",
    "    def add_zero_padding(self, stems):\n",
    "        # this function should add zero padding to the track to make sure that the track is divisible by the segment length and the residue from the overlap.\n",
    "        # the length of the array has to be segment_length + k * samples_in_steps for some nonnegative integer k.\n",
    "        num_samples = stems.shape[1]\n",
    "        step_in_samples = int(self.segment_samples * (1 - self.segment_overlap))\n",
    "        samples_in_last_segment = num_samples % step_in_samples\n",
    "        if samples_in_last_segment != 0:\n",
    "            padding = torch.zeros((stems.shape[0], step_in_samples - samples_in_last_segment, stems.shape[2]))\n",
    "            return torch.cat((stems, padding), axis=1)\n",
    "        else:\n",
    "            return stems\n",
    "\n",
    "    def add_N_zeros(self, stems, N):\n",
    "        zeros = torch.zeros((stems.shape[0], N, stems.shape[2]))\n",
    "        return torch.cat((stems, zeros), axis = 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "765d7d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class newMus(torch.utils.data.Dataset):\n",
    "    def __init__(self, musdb_root, split='train', subset='train', filtered_indices = None, batch_size = None, is_wav=False, sample_rate=44100, segment_length = 10, segment_chunks = 10, discard_low_energy = True, segment_overlap = 0.5, drop_percentile =  0.1, chunks_below_percentile = 0.5):\n",
    "        assert(subset == 'train' or subset == 'test')\n",
    "        self.mode = subset\n",
    "        self.split = split \n",
    "        self.mus = musdb.DB(musdb_root, subsets=subset, split=split, is_wav=is_wav)\n",
    "        self.sample_rate = sample_rate\n",
    "        self.discard_low_energy = discard_low_energy\n",
    "        self.segment_length = segment_length\n",
    "        self.segment_chunks = segment_chunks\n",
    "        self.chunks_below_percentile = chunks_below_percentile\n",
    "        self.segment_overlap = segment_overlap\n",
    "        self.drop_percentile = drop_percentile\n",
    "        self.segment_samples = int(self.segment_length * self.sample_rate)\n",
    "        self.chunk_samples = int(self.segment_samples / self.segment_chunks)\n",
    "        if filtered_indices is None or batch_size is None:\n",
    "            self.durations = dict()\n",
    "            self.filtered_indices = dict()\n",
    "            self.len = self.init_durations()\n",
    "            self.shortest_duration = self.shortest_duration_in_samples(self.mus)\n",
    "            self.batch_size = self.find_batch_size()\n",
    "        else:\n",
    "            self.filtered_indices = [int(x) for x in filtered_indices.split(',')]\n",
    "            self.batch_size = batch_size\n",
    "            self.len = len(self.filtered_indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # this function should return a batch of segment STFTs from the song as well as their stem STFTs.\n",
    "        start = time.time()\n",
    "        track = self.mus.tracks[self.filtered_indices[idx]]\n",
    "        # stems is a list of the stems of the track, in the order of the stems in the track\n",
    "        stems = torch.Tensor(track.stems)\n",
    "        return stems\n",
    "\n",
    "    def init_durations(self):\n",
    "        pos = 0\n",
    "        for idx, track in enumerate(self.mus.tracks):\n",
    "            print(track.name)\n",
    "            self.durations[idx] = track.stems.shape[1]\n",
    "            if self.durations[idx] >= self.segment_samples * 9:\n",
    "                self.filtered_indices[pos] = idx\n",
    "                pos += 1\n",
    "        return pos\n",
    "        \n",
    "    def find_batch_size(self):\n",
    "        # this function should tell us how many STFTs we can fit into a batch based on finding the floor power of 2 of the number of STFTs we fit over the duration of the song\n",
    "        # each STFT will represent a STFT over a fixed segment length.\n",
    "        num_segments = self.num_segments_in_track(self.shortest_duration)\n",
    "        # we anticipate a drop of up to twice the drop percentile (impossible, just to be safe) of the segments.\n",
    "        num_segments = torch.Tensor([int(num_segments * (1 - 2 * self.drop_percentile))])\n",
    "        # We return the closest power of two to that anticipated number of segments. Of course we use a floor because we want to fill every batch.\n",
    "        return 2 ** int(torch.floor(torch.log2(num_segments)))\n",
    "\n",
    "    def num_segments_in_track(self, duration_in_samples):\n",
    "        # this function should return the number of segments in the track and consider the overlap factor, self.segment_overlap\n",
    "        return int(torch.ceil(torch.Tensor([duration_in_samples / (self.segment_samples * (1 - self.segment_overlap))])))\n",
    "\n",
    "    def shortest_duration_in_samples(self, mus):\n",
    "        # this function should return the shortest duration of the stems in the track\n",
    "        min = 100000000\n",
    "        for i, dur in self.durations.items():\n",
    "            if i in self.filtered_indices.values():\n",
    "                if dur < min:\n",
    "                    min = dur\n",
    "        return min\n",
    "\n",
    "    def longest_duration_in_samples(self, mus):\n",
    "        # this function should return the longest duration of the stems in the track\n",
    "        return max([duration for duration in self.durations.values()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3dd0de03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningModel(pl.LightningModule): \n",
    "    \n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.mus_path = hparams['mus_path']\n",
    "        self.bandwidths = [int(bandwidth) for bandwidth in hparams['bandwidths'].split(',')]\n",
    "        self.step = 0\n",
    "        self.n_mels = hparams['n_mels']\n",
    "        self.N = hparams['bandwidth_freq_out_size']\n",
    "        self.K = len(self.bandwidths)\n",
    "        self.time_steps = hparams['time_steps']\n",
    "        self.transforms = Transforms().double()\n",
    "        self.kernel1 = hparams['conv_1_kernel_size']\n",
    "        self.stride1 = hparams['conv_1_stride']\n",
    "        self.kernel2 = hparams['conv_2_kernel_size']\n",
    "        self.stride2 = hparams['conv_2_stride']\n",
    "        self.training_dataloader = None\n",
    "        self.testing_dataloader = None\n",
    "        self.validation_dataloader = None\n",
    "        self.bandsplit = BandSplit(self.bandwidths, self.N).double()\n",
    "        self.conv1 = ConvolutionLayer(self.K, self.K, self.kernel1, self.stride1).double()\n",
    "        self.conv2 = ConvolutionLayer(self.K, self.K, self.kernel2, self.stride2).double()\n",
    "        self.conv3 = ConvolutionLayer(1,8,kernel_size=(1,11),stride=(1,3)).double()\n",
    "        self.conv4 = ConvolutionLayer(8,22,kernel_size=(1,3),stride=(1,2)).double()\n",
    "        self.conv5 = ConvolutionLayer(1,8,kernel_size=(1,11),stride=(1,3)).double()\n",
    "        self.conv6 = ConvolutionLayer(8,22,kernel_size=(1,3),stride=(1,2)).double()\n",
    "        self.blstms1 = AlternatingBLSTMs(self.K, 70, 63, 64).double()\n",
    "        self.blstms2 = AlternatingBLSTMs(self.K, 70, 96, 64).double()\n",
    "        self.blstms3 = AlternatingBLSTMs(self.K, 70, 76, 63 ).double()\n",
    "        self.deconv1 = TransposeConvolutionLayer(self.K, self.K, self.kernel2, self.stride2).double()\n",
    "        self.deconv2 = TransposeConvolutionLayer(self.K, self.K, self.kernel1, self.stride1).double()\n",
    "        self.masks = MaskEstimation(self.bandwidths, 128,32).double()\n",
    "        self.data_handler = DataHandler(hparams['training_batch_size'], hparams['shortest_duration'], hparams['sampling_rate'], \n",
    "            hparams['longest_duration'], hparams['segment_overlap'], hparams['segment_chunks'], \n",
    "            hparams['segment_length'], hparams['chunks_below_percentile'], hparams['drop_percentile'])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        if self.step % 5 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "        self.step += 1\n",
    "        \n",
    "        data = self.training_dataloader.dataset.batchize_training_item(batch)\n",
    "        stfts, chromas, mfccs = self.transforms(data)\n",
    "        predicted_sources = self.forward_pass(stfts, chromas, mfccs)\n",
    "        loss = self.loss(predicted_sources, real_sources[:,3])\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        if self.step % 5 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "        self.step += 1\n",
    "        \n",
    "        data = self.validation_dataloader.dataset.batchize_training_item(batch)\n",
    "        stfts, chromas, mfccs = self.transforms(data)\n",
    "        predicted_sources = self.forward_pass(stfts, chromas, mfccs)\n",
    "        loss = self.loss(predicted_sources, real_sources[:,3])\n",
    "        self.log(\"valid_loss\", loss)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        pass\n",
    "    \n",
    "    def forward_pass(self, X, chromas, mfccs):\n",
    "        # takes in STFTs, chromas, mfccs\n",
    "        X1 = self.bandsplit(X)\n",
    "        batch_size = X1.shape[0]\n",
    "        #Shape: torch.Size([32, 22, 431, 128]) (batch_size, num_bands, time_steps, freq_N)\n",
    "        X2 = self.conv1(X1)\n",
    "        X3 = self.conv2(X2)\n",
    "        mfccs = mfccs.reshape(batch_size,1,self.n_mels,-1)\n",
    "        mfccs = self.conv3(mfccs)\n",
    "        mfccs = self.conv4(mfccs)\n",
    "        chromas = chromas.reshape(batch_size,1,12,-1)\n",
    "        chromas = self.conv5(chromas)\n",
    "        chromas = self.conv6(chromas)\n",
    "        X, _ = self.blstms1(X3)\n",
    "        xmfccs = torch.cat((mfccs,X), 2)\n",
    "        X, _ = self.blstms2(xmfccs)\n",
    "        xchromas = torch.cat((chromas,X),2)\n",
    "        X, _ = self.blstms3(xchromas)      \n",
    "        X = self.deconv1(X + X3)\n",
    "        X = self.deconv2(X + X2)\n",
    "        X = self.masks(torch.cat((X, X[:,:,:,-1].unsqueeze(3)), 3))\n",
    "        return X\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr = 0.001)\n",
    "        return optimizer\n",
    "    \n",
    "\n",
    "        \n",
    "class Chroma(nn.Module):\n",
    "    def __init__(self, n_fft, sampling_rate):\n",
    "        self.n_fft = n_fft\n",
    "        self.sampling_rate = sampling_rate\n",
    "        \n",
    "    def forward(self, x):\n",
    "        pass\n",
    "    \n",
    "        # x is a spectrogram with shape(... , T, F)\n",
    "class Transforms(nn.Module):\n",
    "    def __init__(self, input_freq = 44100, resample_freq = 16000, n_fft = 2048, hop_length = 1024, win_length=2048, n_mels = 32):\n",
    "        super().__init__()\n",
    "        self.resample = Resample(input_freq, resample_freq)\n",
    "        self.stft = Spectrogram(n_fft = n_fft, hop_length = hop_length, win_length = win_length)\n",
    "        self.mel = MelScale(sample_rate = resample_freq, n_mels = n_mels, n_stft = n_fft // 2 + 1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.resample(X)\n",
    "        \n",
    "        stft = self.stft(X)\n",
    "    \n",
    "        mfccs= self.mel(stft)\n",
    "        \n",
    "        chromas = self.mel(stft)\n",
    "        return stft, chromas, mfccs\n",
    "    \n",
    "def post_conv_dimensions(self,N,time_steps,in_channels):\n",
    "        x = torch.randn(1,in_channels,N,time_steps).to(self.device).double()\n",
    "        return self.conv2(self.conv1(x)).shape\n",
    "        \n",
    "class ConvolutionLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding=0, dtype='double'):\n",
    "        super(ConvolutionLayer, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding=padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "    \n",
    "class TransposeConvolutionLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, dtype='double'):\n",
    "        super(TransposeConvolutionLayer, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "# This class defines a module that runs t\n",
    "class AlternatingBLSTMs(nn.Module):\n",
    "    def __init__(self, num_bands, time_steps, N, out_size, axis=1):\n",
    "        super(AlternatingBLSTMs, self).__init__()\n",
    "        self.band_blstm = BandBiLSTM(num_bands, time_steps, N)\n",
    "        self.temporal_blstm = TemporalBiLSTM(num_bands, time_steps, N, out_size)\n",
    "        self.num_bands = num_bands\n",
    "        self.time_steps = time_steps\n",
    "        self.N = N\n",
    "        # hidden size = freq_steps_per_band * time_steps \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input shape: (batch_size, num_bands, N, time_steps)\n",
    "        # Prepare for Band BLSTM: shape = (batch_size, num_bands, N * time_steps)\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.reshape(batch_size, self.num_bands, -1)\n",
    "        x = self.band_blstm(x)\n",
    "        x = x.reshape(batch_size, self.time_steps, -1)\n",
    "        x = self.temporal_blstm(x)\n",
    "        #x += residual\n",
    "        # Return the output of the module\n",
    "        return x    \n",
    "    \n",
    "# This class defines a module that runs the input, with shape (num_bands, num_timesteps, N), through a normalization layer, then a temporal biLSTM, then a fully connected layer.\n",
    "# Then, the output of that layer is of the same shape as the input to the module, which will be fed into a similar structure, but this time with a band biLSTM, following the same normalization, biLSTM, FC structure.\n",
    "class BandBiLSTM(nn.Module):\n",
    "    def __init__(self, num_bands, time_steps, N, axis=1):\n",
    "        super(BandBiLSTM, self).__init__()\n",
    "        self.norm = nn.GroupNorm(num_bands, num_bands)\n",
    "        self.input_size = time_steps * N\n",
    "        self.hidden_size = self.input_size // 2\n",
    "        self.bilstm = nn.LSTM(self.input_size, self.hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(N, N)\n",
    "        self.axis = axis\n",
    "        self.N = N\n",
    "        self.num_bands = num_bands\n",
    "        self.time_steps = time_steps\n",
    "        # hidden size = freq_steps_per_band * time_steps \n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        # (batch_size,time_steps, num_bands, N)\n",
    "        x = self.norm(x)\n",
    "        residual = x.clone().detach()\n",
    "        x, lstm_vars = self.bilstm(x)\n",
    "        # (batch_size, num_bands, 2 * hidden_size)\n",
    "        x = x.reshape(batch_size, self.num_bands, self.time_steps, self.N)\n",
    "        # (batch_size, num_bands, time_steps, N)\n",
    "        x = self.fc(x)\n",
    "        # (batch_size, num_bands, time_steps, N)\n",
    "        #x += residual\n",
    "        # Return the output of the module\n",
    "        return x\n",
    "    \n",
    "class TemporalBiLSTM(nn.Module):\n",
    "    def __init__(self, num_bands, time_steps, N, out, axis=1):\n",
    "        super(TemporalBiLSTM, self).__init__()\n",
    "        self.norm = nn.GroupNorm(time_steps, time_steps)\n",
    "        self.input_size = num_bands * N\n",
    "        self.hidden_size = num_bands * out // 2\n",
    "        self.out = out\n",
    "        self.bilstm = nn.LSTM(self.input_size, self.hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(out, out)\n",
    "        self.axis = axis\n",
    "        self.N = N\n",
    "        self.time_steps = time_steps\n",
    "        self.num_bands = num_bands\n",
    "        # hidden size = freq_steps_per_band * time_steps \n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        # (batch_size,time_steps, num_bands, N)\n",
    "        x = self.norm(x)\n",
    "        residual = x.clone().detach()\n",
    "        x, lstm_vars = self.bilstm(x)\n",
    "        # (batch_size, num_bands, 2 * hidden_size)\n",
    "        x = x.reshape(batch_size, self.num_bands, self.time_steps, self.out)\n",
    "        # (batch_size, num_bands, time_steps, N)\n",
    "        x = self.fc(x)\n",
    "        x = x.permute(0,1,3,2)\n",
    "        # (batch_size, num_bands, time_steps, N)\n",
    "        #x += residual\n",
    "        # Return the output of the module\n",
    "        return x, lstm_vars\n",
    "\n",
    "class BandSplit(torch.nn.Module):\n",
    "    # Input shape: torch.Size([16, 2, 1025, 431, 2])\n",
    "    def __init__(self, bandwidths, N):\n",
    "        # bandwidth\n",
    "        super(BandSplit, self).__init__()\n",
    "        self.bandwidths = bandwidths\n",
    "        self.norm_layers = torch.nn.ModuleList([torch.nn.LayerNorm(2 * bandwidth) for bandwidth in self.bandwidths])\n",
    "        self.fc_layers = torch.nn.ModuleList([torch.nn.Linear(2 * bandwidth, N) for bandwidth in self.bandwidths])\n",
    "\n",
    "    def forward(self, X):\n",
    "        subband_spectrograms = []\n",
    "        K = len(self.bandwidths)\n",
    "        for i in range(K):\n",
    "            start_index = sum(self.bandwidths[:i])\n",
    "            end_index = start_index + self.bandwidths[i]\n",
    "            subband_spectrogram = X[:, :,start_index:end_index, :]\n",
    "            subband_spectrogram = subband_spectrogram.permute(0,1,4,2,3)\n",
    "            subband_spectrogram = subband_spectrogram.reshape(2 * X.shape[0], X.shape[3], 2 * self.bandwidths[i])\n",
    "            subband_spectrograms.append(subband_spectrogram)\n",
    "\n",
    "        subband_features = []\n",
    "        for i in range(K):\n",
    "            norm_output = self.norm_layers[i](subband_spectrograms[i])\n",
    "            fc_output = self.fc_layers[i](norm_output)\n",
    "            subband_features.append(fc_output)\n",
    "\n",
    "        Z = torch.stack(subband_features, dim=1)\n",
    "        Z = Z.permute(0,1,3,2)\n",
    "        return Z\n",
    "    \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, hidden_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.MLP = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, out_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.MLP(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class MaskEstimation(nn.Module):\n",
    "    def __init__(self, bandwidths, N, batch_size):\n",
    "        super(MaskEstimation, self).__init__()\n",
    "        self.num_bands = len(bandwidths)\n",
    "        self.bandwidths = bandwidths\n",
    "        self.batch_size = batch_size\n",
    "        self.norm_layers = torch.nn.ModuleList([torch.nn.LayerNorm(N) for bandwidth in self.bandwidths])\n",
    "        self.MLP_layers = torch.nn.ModuleList([MLP(N, bandwidth * 2, N * 2) for bandwidth in self.bandwidths])\n",
    "    def forward(self, x):\n",
    "        # Input shape: (batch_size, num_bands, N, T)\n",
    "        time_steps = x.shape[3]\n",
    "        x = x.permute(1, 0 , 3, 2)\n",
    "        out = []\n",
    "        # shape: (num_bands, batch_size, T, N)\n",
    "        for i in range(self.num_bands):\n",
    "            y = self.norm_layers[i](x[i])\n",
    "            y = self.MLP_layers[i](y)\n",
    "            out.append(y)\n",
    "        out = torch.cat(out, 2)\n",
    "        out = out.reshape(self.batch_size // 2, 2, sum(self.bandwidths), time_steps, 2)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2547507e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "        \"mus_path\": \"musdb/\",\n",
    "        \"num_bandwidths\": 23,\n",
    "        \"bandwidths\": \"20,20,20,30,30,30,30,30,30,30,30,30,30,50,50,50,50,70,70,100,100,125\",\n",
    "        \"bandwidth_freq_out_size\": 128,\n",
    "        \"n_fft\": 2048,\n",
    "        \"hop_length\": 1024,\n",
    "        \"win_length\": 2048,\n",
    "        \"conv_1_kernel_size\":(1,7),\n",
    "        \"conv_1_stride\":(1,3),\n",
    "        \"conv_2_kernel_size\":(4,4),\n",
    "        \"conv_2_stride\":(2,2),\n",
    "        \"conv_3_kernel_size\":(1,7),\n",
    "        \"conv_3_stride\":(1,3),\n",
    "        \"conv_3_ch_out_1\":8,\n",
    "        \"time_steps\": 431,\n",
    "        \"freq_bands\": 1025,\n",
    "        \"n_mels\": 32,\n",
    "        \"input_sampling_rate\": 44100,\n",
    "        \"resampling_rate\": 16000,\n",
    "        \"shortest_duration\" : 5019648,\n",
    "        \"longest_duration\" : 20000000,\n",
    "        \"segment_length\" : 10,\n",
    "        \"sampling_rate\" : 44100,\n",
    "        \"resampling_rate\" : 16000,\n",
    "        \"discard_low_energy\" : True,\n",
    "        \"drop_percentile\" : 0.1,\n",
    "        'chunks_below_percentile' : 0.5,\n",
    "        'segment_overlap' : 0.5,\n",
    "        'segment_chunks' : 10,\n",
    "        'training_batch_size' : 16,\n",
    "        'testing_batch_size' : 64,\n",
    "        'filtered_training_indices' : '0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,50,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85',\n",
    "        'filtered_validation_indices' : '0,1,2,3,4,5,6,7,8,9,10,11,12,13'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0139a56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "musValidation = newMus('musdb/', 'valid', 'train', batch_size = 16, filtered_indices = hparams['filtered_validation_indices'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbc36dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "musTraining = newMus('musdb/', batch_size = 16, filtered_indices = hparams['filtered_training_indices'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6cd5147",
   "metadata": {},
   "outputs": [],
   "source": [
    "lightning = LightningModel(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0e2a27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2\n",
      "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 2 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name       | Type                      | Params\n",
      "----------------------------------------------------------\n",
      "0  | transforms | Transforms                | 0     \n",
      "1  | bandsplit  | BandSplit                 | 269 K \n",
      "2  | conv1      | ConvolutionLayer          | 3.5 K \n",
      "3  | conv2      | ConvolutionLayer          | 7.8 K \n",
      "4  | conv3      | ConvolutionLayer          | 112   \n",
      "5  | conv4      | ConvolutionLayer          | 594   \n",
      "6  | conv5      | ConvolutionLayer          | 112   \n",
      "7  | conv6      | ConvolutionLayer          | 594   \n",
      "8  | blstms1    | AlternatingBLSTMs         | 128 M \n",
      "9  | blstms2    | AlternatingBLSTMs         | 286 M \n",
      "10 | blstms3    | AlternatingBLSTMs         | 182 M \n",
      "11 | deconv1    | TransposeConvolutionLayer | 7.8 K \n",
      "12 | deconv2    | TransposeConvolutionLayer | 3.5 K \n",
      "13 | masks      | MaskEstimation            | 1.3 M \n",
      "----------------------------------------------------------\n",
      "599 M     Trainable params\n",
      "0         Non-trainable params\n",
      "599 M     Total params\n",
      "2,399.781 Total estimated model params size (MB)\n",
      "2023-01-29 08:15:19.274032: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-29 08:15:19.802900: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51f5ac83d4b64ad08d6d2db9dae74517",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ProcessExitedException",
     "evalue": "process 1 terminated with signal SIGSEGV",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mProcessExitedException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m'\u001b[39m, devices \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlightning\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmusTraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmusValidation\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py:608\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    606\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`Trainer.fit()` requires a `LightningModule`, got: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m_lightning_module \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m--> 608\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/call.py:36\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 36\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlauncher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/strategies/launchers/multiprocessing.py:113\u001b[0m, in \u001b[0;36m_MultiProcessingLauncher.launch\u001b[0;34m(self, function, trainer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    111\u001b[0m     process_args \u001b[38;5;241m=\u001b[39m [trainer, function, args, kwargs, return_queue]\n\u001b[0;32m--> 113\u001b[0m \u001b[43mmp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_processes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrapping_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocess_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnprocs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_processes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_start_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m worker_output \u001b[38;5;241m=\u001b[39m return_queue\u001b[38;5;241m.\u001b[39mget()\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trainer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py:198\u001b[0m, in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# Loop on join until it returns True or raises an exception.\u001b[39;00m\n\u001b[0;32m--> 198\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/spawn.py:140\u001b[0m, in \u001b[0;36mProcessContext.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exitcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    139\u001b[0m     name \u001b[38;5;241m=\u001b[39m signal\u001b[38;5;241m.\u001b[39mSignals(\u001b[38;5;241m-\u001b[39mexitcode)\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ProcessExitedException(\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocess \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m terminated with signal \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m    142\u001b[0m         (error_index, name),\n\u001b[1;32m    143\u001b[0m         error_index\u001b[38;5;241m=\u001b[39merror_index,\n\u001b[1;32m    144\u001b[0m         error_pid\u001b[38;5;241m=\u001b[39mfailed_process\u001b[38;5;241m.\u001b[39mpid,\n\u001b[1;32m    145\u001b[0m         exit_code\u001b[38;5;241m=\u001b[39mexitcode,\n\u001b[1;32m    146\u001b[0m         signal_name\u001b[38;5;241m=\u001b[39mname\n\u001b[1;32m    147\u001b[0m     )\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ProcessExitedException(\n\u001b[1;32m    150\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocess \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m terminated with exit code \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m    151\u001b[0m         (error_index, exitcode),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    154\u001b[0m         exit_code\u001b[38;5;241m=\u001b[39mexitcode\n\u001b[1;32m    155\u001b[0m     )\n",
      "\u001b[0;31mProcessExitedException\u001b[0m: process 1 terminated with signal SIGSEGV"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=1, accelerator='gpu', devices = 2)\n",
    "trainer.fit(lightning, musTraining, musValidation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e112327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_waveform(waveform, sr, title=\"Waveform\"):\n",
    "    waveform = waveform.numpy()\n",
    "\n",
    "    num_channels, num_frames = waveform.shape\n",
    "    time_axis = torch.arange(0, num_frames) / sr\n",
    "\n",
    "    figure, axes = plt.subplots(num_channels, 1)\n",
    "    axes.plot(time_axis, waveform[0], linewidth=1)\n",
    "    axes.grid(True)\n",
    "    figure.suptitle(title)\n",
    "    plt.show(block=False)\n",
    "\n",
    "\n",
    "def plot_spectrogram(specgram, title=None, ylabel=\"freq_bin\"):\n",
    "    fig, axs = plt.subplots(1, 1)\n",
    "    axs.set_title(title or \"Spectrogram (db)\")\n",
    "    axs.set_ylabel(ylabel)\n",
    "    axs.set_xlabel(\"frame\")\n",
    "    im = axs.imshow(librosa.power_to_db(specgram), origin=\"lower\", aspect=\"auto\")\n",
    "    fig.colorbar(im, ax=axs)\n",
    "    plt.show(block=False)\n",
    "    \n",
    "def check_memory():\n",
    "    t = torch.cuda.get_device_properties(0).total_memory\n",
    "    r = torch.cuda.memory_reserved(0)\n",
    "    a = torch.cuda.memory_allocated(0)\n",
    "    f = r-a  # free inside reserved\n",
    "    GB = 1024 ** 3\n",
    "    print(\"Total: \", t / GB)\n",
    "    print(\"Reserved \", r / GB)\n",
    "    print(\"Allocated: \", a / GB)\n",
    "    print(\"free: \", f / GB)\n",
    "    \n",
    "def free_memory(var):\n",
    "    del var\n",
    "    torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
