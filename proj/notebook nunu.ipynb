{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install musdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import musdb\n",
    "help(musdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3NAI6ctWyQnD",
    "outputId": "78008373-44f6-4b90-91af-6881e741f445"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = BandSplit([20,20,20,30,30,30,30,30,30,30,30,30,30,50,50,50,50,70,70,100,100,125],128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "b.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "b.double()\n",
    "c = b(y[0])\n",
    "c.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = c.permute(0,1,3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conv1 = ConvolutionLayer(22, 22, (1,7), (1,3),).to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2 = ConvolutionLayer(22, 22, (3,3), (2,2)).to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chromas = y[1]\n",
    "mfccs = y[2]\n",
    "batch_size = 32\n",
    "\n",
    "chromas = chromas.reshape(batch_size, 1, 12, 431)\n",
    "chromas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv3 = ConvolutionLayer(1,8,kernel_size=(1,11),stride=(1,6)).to('cuda:0')\n",
    "conv4 = ConvolutionLayer(8,22,kernel_size=(1,3),stride=(1,2)).to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conv3(chromas.float()).shape\n",
    "h = conv3(chromas.float())\n",
    "j = conv4(h)\n",
    "j.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = conv1(c.float())\n",
    "print(d.shape)\n",
    "e = conv2(d)\n",
    "print(e.shape)\n",
    "f = conv2(e)\n",
    "f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to('cuda:0');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([20,20,20,30,30,30,30,30,30,30,30,30,30,50,50,50,50,70,70,100,100,125],128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0].double();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, chromas, mfccs = model(y[0].double(), y[1], y[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mfccs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.cat((mfccs,out,torch.zeros(32,22,1,70).to('cuda:0')), 2)\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bblstm = BandBiLSTM(96, 22, 48).to('cuda:0')\n",
    "bblstm.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.reshape(32,22,70*96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blstmt1 = bblstm(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blstmt1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tblstm = TemporalBiLSTM(70, 96, 35).to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tblstm.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blstmt2 = tblstm(blstmt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blstmt2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = nn.GroupNorm(22, 22).to('cuda:0').double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm(z).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ablstms = AlternatingBLSTMs(22, 70, 96).to('cuda:0').double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = z.to('cuda:1')\n",
    "ablstms(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bblstm(z).permute(0,2,3,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bblstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chromas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deconv = TransposeConvolutionLayer(22, 22, (2,2), (2,2)).to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deconv.double()\n",
    "deconv(out).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(\n",
    "\tparam.numel() for param in conv4.parameters()\n",
    ")\n",
    "total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = blstm1(c.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def test_module(module, input_size):\n",
    "    \"\"\"\n",
    "    Test an nn.Module subclass with random input data of the given size.\n",
    "    \n",
    "    :param module: The nn.Module subclass to test.\n",
    "    :param input_size: A tuple of integers specifying the size of the input data.\n",
    "    \"\"\"\n",
    "    # Create random input data with the specified size\n",
    "    input_data = torch.randn(input_size)\n",
    "    print(f'Input shape: {input_data.shape}')\n",
    "\n",
    "    # Run the input data through the module\n",
    "    output = module(input_data)\n",
    "    print(f'Output shape: {output.shape}')\n",
    "    \n",
    "    \n",
    "    \n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, bandwidths, N, kernel1 = (1,7), stride1=(1,3), kernel2 = (3,3), stride2 = (2,2), device = 'cuda:0', dtype = 'double', n_mels = 32):\n",
    "        super(Model, self).__init__()\n",
    "        self.device = device\n",
    "        self.N = N\n",
    "        self.n_mels = n_mels\n",
    "        self.K = len(bandwidths)\n",
    "        self.bandsplit = BandSplit(bandwidths, N).to(device)\n",
    "        self.bandsplit.double()\n",
    "        self.conv1 = ConvolutionLayer(self.K, self.K, kernel1, stride1).to(device)\n",
    "        self.conv1.double()\n",
    "        self.conv2 = ConvolutionLayer(self.K, self.K, kernel2, stride2).to(device)\n",
    "        self.conv2.double()\n",
    "        self.conv3 = ConvolutionLayer(1,8,kernel_size=(1,11),stride=(1,3)).to(device)\n",
    "        self.conv3.double()\n",
    "        self.conv4 = ConvolutionLayer(8,22,kernel_size=(1,3),stride=(1,2)).to(device)\n",
    "        self.conv4.double()\n",
    "        self.conv5 = ConvolutionLayer(1,8,kernel_size=(1,11),stride=(1,6)).to(device)\n",
    "        self.conv5.double()\n",
    "        self.conv6 = ConvolutionLayer(8,22,kernel_size=(1,3),stride=(1,2)).to(device)\n",
    "        self.conv6.double()\n",
    "        self.bandblstm1 = BandBiLSTM()\n",
    "        \n",
    "    def forward(self, X, chromas, mfccs):\n",
    "        X = self.bandsplit(X)\n",
    "        #Shape: torch.Size([32, 22, 431, 128]) (batch_size, num_bands, time_steps, freq_N)\n",
    "        X = self.conv1(X)\n",
    "        X = self.conv2(X)\n",
    "        batch_size = X.shape[0]\n",
    "        mfccs = mfccs.reshape(batch_size,1,self.n_mels,-1)\n",
    "        mfccs = mfccs.to(self.device)\n",
    "        mfccs = self.conv3(mfccs)\n",
    "        mfccs = self.conv4(mfccs)\n",
    "        chromas = chromas.reshape(batch_size,1,12,-1)\n",
    "        chromas = chromas.to(self.device)\n",
    "        chromas = self.conv5(chromas)\n",
    "        chromas = self.conv6(chromas)\n",
    "        xmfccs = torch.cat(X, mfccs, 2)\n",
    "        \n",
    "        return X, chromas, mfccs\n",
    "        \n",
    "        \n",
    "class ConvolutionLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding=0, dtype='double'):\n",
    "        super(ConvolutionLayer, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding=padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "    \n",
    "class TransposeConvolutionLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, dtype='double'):\n",
    "        super(TransposeConvolutionLayer, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class AlternatingBLSTMs(nn.Module):\n",
    "    def __init__(self, num_bands, time_steps, N, axis=1):\n",
    "        super(AlternatingBLSTMs, self).__init__()\n",
    "        self.band_blstm = BandBiLSTM(num_bands, time_steps, N, num_bands // 2)\n",
    "        self.temporal_blstm = TemporalBiLSTM(num_bands, time_steps, N, time_steps // 2)\n",
    "        self.num_bands = num_bands\n",
    "        self.time_steps = time_steps\n",
    "        self.N = N\n",
    "        # hidden size = freq_steps_per_band * time_steps \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input shape: (batch_size, num_bands, N, time_steps)\n",
    "        # Prepare for Band BLSTM: shape = (batch_size, num_bands, N * time_steps)\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.reshape(batch_size, self.num_bands, -1)\n",
    "        x = self.band_blstm(x)\n",
    "        x = x.reshape(batch_size, self.N, -1)\n",
    "        x = self.temporal_blstm(x)\n",
    "        \n",
    "        #x += residual\n",
    "        # Return the output of the module\n",
    "        return x    \n",
    "# This class defines a module that runs the input, with shape (num_bands, num_timesteps, N), through a normalization layer, then a temporal biLSTM, then a fully connected layer.\n",
    "# Then, the output of that layer is of the same shape as the input to the module, which will be fed into a similar structure, but this time with a band biLSTM, following the same normalization, biLSTM, FC structure.\n",
    "class BandBiLSTM(nn.Module):\n",
    "    def __init__(self, num_bands, time_steps, N, hidden_size, axis=1):\n",
    "        super(BandBiLSTM, self).__init__()\n",
    "        self.norm = nn.GroupNorm(num_bands, num_bands)\n",
    "        self.input_size = time_steps * N\n",
    "        self.hidden_size = self.input_size // 2\n",
    "        self.bilstm = nn.LSTM(self.input_size, self.hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(N, N)\n",
    "        self.axis = axis\n",
    "        self.N = N\n",
    "        self.num_bands = num_bands\n",
    "        self.hidden_size = hidden_size\n",
    "        self.time_steps = time_steps\n",
    "        # hidden size = freq_steps_per_band * time_steps \n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"BAND INPUT SHAPE: \", x.shape)\n",
    "        # (batch_size,time_steps, num_bands, N)\n",
    "        x = self.norm(x)\n",
    "        residual = x.clone().detach()\n",
    "        x, lstm_vars = self.bilstm(x)\n",
    "        # (batch_size, num_bands, 2 * hidden_size)\n",
    "        x = x.reshape(batch_size, self.num_bands, self.time_steps, self.N)\n",
    "        # (batch_size, num_bands, time_steps, N)\n",
    "        x = self.fc(x)\n",
    "        # (batch_size, num_bands, time_steps, N)\n",
    "        print(x.shape)\n",
    "        #x += residual\n",
    "        # Return the output of the module\n",
    "        return x\n",
    "    \n",
    "class TemporalBiLSTM(nn.Module):\n",
    "    def __init__(self, num_bands, time_steps, N, hidden_size, axis=1):\n",
    "        super(TemporalBiLSTM, self).__init__()\n",
    "        self.norm = nn.GroupNorm(time_steps, time_steps)\n",
    "        self.input_size = num_bands * N\n",
    "        self.hidden_size = self.input_size // 2\n",
    "        self.bilstm = nn.LSTM(self.input_size, self.hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(N, N)\n",
    "        self.axis = axis\n",
    "        self.N = N\n",
    "        self.time_steps = time_steps\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_bands = num_bands\n",
    "        # hidden size = freq_steps_per_band * time_steps \n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"TEMPORAL INPUT SHAPE: \", x.shape)\n",
    "        # (batch_size,time_steps, num_bands, N)\n",
    "        x = self.norm(x)\n",
    "        residual = x.clone().detach()\n",
    "        x, lstm_vars = self.bilstm(x)\n",
    "        # (batch_size, num_bands, 2 * hidden_size)\n",
    "        x = x.reshape(batch_size, self.num_bands, self.time_steps, self.N)\n",
    "        # (batch_size, num_bands, time_steps, N)\n",
    "        x = self.fc(x)\n",
    "        # (batch_size, num_bands, time_steps, N)\n",
    "        print(x.shape)\n",
    "        #x += residual\n",
    "        # Return the output of the module\n",
    "        return x, lstm_vars\n",
    "\n",
    "class BandSplit(torch.nn.Module):\n",
    "    # Input shape: torch.Size([16, 2, 1025, 431, 2])\n",
    "    def __init__(self, bandwidths, N):\n",
    "        # bandwidth\n",
    "        super(BandSplit, self).__init__()\n",
    "        self.bandwidths = bandwidths\n",
    "        self.norm_layers = torch.nn.ModuleList([torch.nn.LayerNorm(2 * bandwidth) for bandwidth in self.bandwidths])\n",
    "        self.fc_layers = torch.nn.ModuleList([torch.nn.Linear(2 * bandwidth, N) for bandwidth in self.bandwidths])\n",
    "\n",
    "    def forward(self, X):\n",
    "        subband_spectrograms = []\n",
    "        K = len(self.bandwidths)\n",
    "        for i in range(K):\n",
    "            start_index = sum(self.bandwidths[:i])\n",
    "            end_index = start_index + self.bandwidths[i]\n",
    "            subband_spectrogram = X[:, :,start_index:end_index, :]\n",
    "            subband_spectrogram = subband_spectrogram.permute(0,1,4,2,3)\n",
    "            subband_spectrogram = subband_spectrogram.reshape(2 * X.shape[0], X.shape[3], 2 * self.bandwidths[i])\n",
    "            subband_spectrograms.append(subband_spectrogram)\n",
    "\n",
    "        subband_features = []\n",
    "        for i in range(K):\n",
    "            norm_output = self.norm_layers[i](subband_spectrograms[i])\n",
    "            fc_output = self.fc_layers[i](norm_output)\n",
    "            subband_features.append(fc_output)\n",
    "\n",
    "        Z = torch.stack(subband_features, dim=1)\n",
    "        Z = Z.permute(0,1,3,2)\n",
    "        return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LicwZGsW58Pe"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def test_module(module, input_size):\n",
    "    \"\"\"\n",
    "    Test an nn.Module subclass with random input data of the given size.\n",
    "    \n",
    "    :param module: The nn.Module subclass to test.\n",
    "    :param input_size: A tuple of integers specifying the size of the input data.\n",
    "    \"\"\"\n",
    "    # Create random input data with the specified size\n",
    "    input_data = torch.randn(input_size)\n",
    "    print(f'Input shape: {input_data.shape}')\n",
    "\n",
    "    # Run the input data through the module\n",
    "    output = module(input_data)\n",
    "    print(f'Output shape: {output.shape}')\n",
    "\n",
    "\n",
    "\n",
    "class BandSplit(torch.nn.Module):\n",
    "    def __init__(self, bandwidths, N):\n",
    "        # bandwidth\n",
    "        super(my_model, self).__init__()\n",
    "        self.bandwidths = bandwidths\n",
    "        self.norm_layers = torch.nn.ModuleList([torch.nn.LayerNorm(2 * bandwidth) for bandwidth in self.bandwidths])\n",
    "        self.fc_layers = torch.nn.ModuleList([torch.nn.Linear(2 * bandwidth, N) for bandwidth in self.bandwidths])\n",
    "\n",
    "    def forward(self, X):\n",
    "        subband_spectrograms = []\n",
    "        K = len(self.bandwidths)\n",
    "        for i in range(K):\n",
    "            start_index = sum(self.bandwidths[:i])\n",
    "            end_index = start_index + self.bandwidths[i]\n",
    "            subband_spectrogram = X[:, start_index:end_index, :]\n",
    "            subband_spectrograms.append(subband_spectrogram)\n",
    "\n",
    "        subband_features = []\n",
    "        for i in range(K):\n",
    "            norm_output = self.norm_layers[i](subband_spectrograms[i])\n",
    "            fc_output = self.fc_layers[i](norm_output)\n",
    "            subband_features.append(fc_output)\n",
    "\n",
    "        Z = torch.stack(subband_features, dim=1)\n",
    "\n",
    "        return Z\n",
    "\n",
    "\n",
    "# This class defines a module that runs the input, with shape (num_bands, num_timesteps, N), through a normalization layer, then a temporal biLSTM, then a fully connected layer.\n",
    "# Then, the output of that layer is of the same shape as the input to the module, which will be fed into a similar structure, but this time with a band biLSTM, following the same normalization, biLSTM, FC structure.\n",
    "class GeneralBiLSTMUnit(nn.Module):\n",
    "    def __init__(self, N, hidden_size, axis=1):\n",
    "        super(GeneralBiLSTMUnit, self).__init__()\n",
    "        self.norm = nn.GroupNorm(1, N)\n",
    "        self.bilstm = nn.LSTM(N, hidden_size, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size*2, N)\n",
    "        self.axis = axis\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Normalize the input\n",
    "        x = self.norm(x)\n",
    "\n",
    "        # Apply the temporal biLSTM layer\n",
    "        x, _ = self.bilstm(x.transpose(0,1))\n",
    "\n",
    "        # Apply the fully connected layer\n",
    "        x = self.fc(x)\n",
    "        x = x.transpose(0,1)\n",
    "        \n",
    "        x += input \n",
    "        # Return the output of the module\n",
    "        return x\n",
    "\n",
    "class generalTransformerUnit(nn.Module):\n",
    "    def __init__(self, N, hidden_size, axis=1):\n",
    "        super(GeneralBiLSTMUnit, self).__init__()\n",
    "        self.norm = nn.GroupNorm(1, N)\n",
    "        self.bilstm = nn.(N, hidden_size, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size*2, N)\n",
    "        self.axis = axis\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Normalize the input\n",
    "        x = self.norm(x)\n",
    "\n",
    "        # Apply the temporal biLSTM layer\n",
    "        x, _ = self.transformer(x.transpose(0,1))\n",
    "\n",
    "        # Apply the fully connected layer\n",
    "        x = self.fc(x)\n",
    "        x = x.transpose(0,1)\n",
    "        \n",
    "        x += input \n",
    "        # Return the output of the module\n",
    "        return x\n",
    "\n",
    "\n",
    "class InterleavedBiLSTMs(nn.Module):\n",
    "    def __init__(self, N, hidden_size):\n",
    "        super(InterleavedBiLSTMs, self).__init__()\n",
    "        self.temporal_bilstm = GeneralBiLSTMUnit(N, hidden_size)\n",
    "        self.band_bilstm = GeneralBiLSTMUnit(N, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.temporal_bilstm(x)\n",
    "        x = self.band_bilstm(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2WbTPSjaKXzM"
   },
   "outputs": [],
   "source": [
    "class my_model(nn.Module):\n",
    "    def __init__(self, x):\n",
    "        super().__init__()\n",
    "        self.linear = nn.LayerNorm(x.shape)\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E07xzVjQAD9N"
   },
   "outputs": [],
   "source": [
    "test_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "u0odoBGcxvyv"
   },
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    # we define a custom function that overrides the PyTorch built in collate in order to make sure\n",
    "    # our input to the model matches the dimensionality we want.\n",
    "    # our input to the function is a list of (input, label) tuples. The list will have size = batch_size as defined in the DataLoader\n",
    "    # We will use a batch_size of 1 to receive a list of tuples (in this case a list with 1 tuple), each tuple has two elements:\n",
    "    # First, the input, a tensor with shape (batch_size, stft_dim_F, stft_dim_T, 2)\n",
    "    # Second, the label, a tensor with shape (num_stems - 1, batch_size, stft_dim_F, stft_dim_T, 2)\n",
    "    input, chromas, mfccs, labels = batch[0]\n",
    "    return input, chromas, mfccs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = Transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m musValidation \u001b[38;5;241m=\u001b[39m \u001b[43mMusdbDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../../musdb18/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 30\u001b[0m, in \u001b[0;36mMusdbDataset.__init__\u001b[0;34m(self, transforms, musdb_root, split, subset, is_wav, sample_rate, segment_length, segment_chunks, discard_low_energy, segment_overlap, drop_percentile, chunks_below_percentile)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmus \u001b[38;5;241m=\u001b[39m musdb\u001b[38;5;241m.\u001b[39mDB(musdb_root, subsets\u001b[38;5;241m=\u001b[39msubset, split\u001b[38;5;241m=\u001b[39msplit, is_wav\u001b[38;5;241m=\u001b[39mis_wav)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdurations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_durations\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms \u001b[38;5;241m=\u001b[39m transforms\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_rate \u001b[38;5;241m=\u001b[39m sample_rate\n",
      "Cell \u001b[0;32mIn[13], line 110\u001b[0m, in \u001b[0;36mMusdbDataset.init_durations\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minit_durations\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, track \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmus\u001b[38;5;241m.\u001b[39mtracks):\n\u001b[0;32m--> 110\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdurations[idx] \u001b[38;5;241m=\u001b[39m \u001b[43mtrack\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstems\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    111\u001b[0m         \u001b[38;5;28mprint\u001b[39m(track)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/musdb/audio_classes.py:161\u001b[0m, in \u001b[0;36mMultiTrack.stems\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# read from disk to save RAM otherwise\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_wav \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath):\n\u001b[0;32m--> 161\u001b[0m         S, rate \u001b[38;5;241m=\u001b[39m \u001b[43mstempeg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_stems\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_start\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m            \u001b[49m\u001b[43mduration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_duration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m            \u001b[49m\u001b[43minfo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m            \u001b[49m\u001b[43msample_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m            \u001b[49m\u001b[43mffmpeg_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ms16le\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m         rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrate\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/stempeg/read.py:304\u001b[0m, in \u001b[0;36mread_stems\u001b[0;34m(filename, start, duration, stem_id, always_3d, dtype, ffmpeg_format, info, sample_rate, reader, multiprocess)\u001b[0m\n\u001b[1;32m    301\u001b[0m     stems \u001b[38;5;241m=\u001b[39m [t[:min_length, :] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m stems]\n\u001b[1;32m    303\u001b[0m \u001b[38;5;66;03m# aggregate list of stems to numpy tensor\u001b[39;00m\n\u001b[0;32m--> 304\u001b[0m stems \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstems\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;66;03m# If ChannelsReader is used, demultiplex from channels\u001b[39;00m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(reader, (ChannelsReader)) \u001b[38;5;129;01mand\u001b[39;00m stems\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "musValidation = MusdbDataset(T, \"../../musdb18/\", \"valid\", \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mus = musdb.DB(\"../../musdb18/\", subsets = \"train\", split = \"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(mus, batch_size=1, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7204704"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min([int(track.duration * 44100) for track in mus.tracks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m musTraining \u001b[38;5;241m=\u001b[39m \u001b[43mMusdbDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../../musdb18/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[36], line 30\u001b[0m, in \u001b[0;36mMusdbDataset.__init__\u001b[0;34m(self, transforms, musdb_root, split, subset, is_wav, sample_rate, segment_length, segment_chunks, discard_low_energy, segment_overlap, drop_percentile, chunks_below_percentile)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmus \u001b[38;5;241m=\u001b[39m musdb\u001b[38;5;241m.\u001b[39mDB(musdb_root, subsets\u001b[38;5;241m=\u001b[39msubset, split\u001b[38;5;241m=\u001b[39msplit, is_wav\u001b[38;5;241m=\u001b[39mis_wav)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdurations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_durations\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms \u001b[38;5;241m=\u001b[39m transforms\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_rate \u001b[38;5;241m=\u001b[39m sample_rate\n",
      "Cell \u001b[0;32mIn[36], line 110\u001b[0m, in \u001b[0;36mMusdbDataset.init_durations\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minit_durations\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, track \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmus\u001b[38;5;241m.\u001b[39mtracks):\n\u001b[0;32m--> 110\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdurations[idx] \u001b[38;5;241m=\u001b[39m \u001b[43mtrack\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstems\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    111\u001b[0m         \u001b[38;5;28mprint\u001b[39m(track)\n\u001b[1;32m    112\u001b[0m         \u001b[38;5;28mprint\u001b[39m(track\u001b[38;5;241m.\u001b[39mduration \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m44100\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/musdb/audio_classes.py:161\u001b[0m, in \u001b[0;36mMultiTrack.stems\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# read from disk to save RAM otherwise\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_wav \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath):\n\u001b[0;32m--> 161\u001b[0m         S, rate \u001b[38;5;241m=\u001b[39m \u001b[43mstempeg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_stems\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_start\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m            \u001b[49m\u001b[43mduration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_duration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m            \u001b[49m\u001b[43minfo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m            \u001b[49m\u001b[43msample_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m            \u001b[49m\u001b[43mffmpeg_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ms16le\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m         rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrate\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/stempeg/read.py:304\u001b[0m, in \u001b[0;36mread_stems\u001b[0;34m(filename, start, duration, stem_id, always_3d, dtype, ffmpeg_format, info, sample_rate, reader, multiprocess)\u001b[0m\n\u001b[1;32m    301\u001b[0m     stems \u001b[38;5;241m=\u001b[39m [t[:min_length, :] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m stems]\n\u001b[1;32m    303\u001b[0m \u001b[38;5;66;03m# aggregate list of stems to numpy tensor\u001b[39;00m\n\u001b[0;32m--> 304\u001b[0m stems \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstems\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;66;03m# If ChannelsReader is used, demultiplex from channels\u001b[39;00m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(reader, (ChannelsReader)) \u001b[38;5;129;01mand\u001b[39;00m stems\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "musTraining = MusdbDataset(T, \"../../musdb18/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install musdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainloader = torch.utils.data.DataLoader(musValidation, batch_size=1, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 919
    },
    "collapsed": true,
    "id": "F0WM94k-FTuk",
    "outputId": "d8f3a363-b549-4009-c6f3-ac1df4ddcffb"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable MultiTrack object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m trainloader:\n\u001b[1;32m      3\u001b[0m     y \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:61\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcollate\u001b[39m(batch):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# we define a custom function that overrides the PyTorch built in collate in order to make sure\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# our input to the model matches the dimensionality we want.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# First, the input, a tensor with shape (batch_size, stft_dim_F, stft_dim_T, 2)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# Second, the label, a tensor with shape (num_stems - 1, batch_size, stft_dim_F, stft_dim_T, 2)\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28minput\u001b[39m, chromas, mfccs, labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m, chromas, mfccs, labels\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable MultiTrack object"
     ]
    }
   ],
   "source": [
    "\n",
    "y = 0\n",
    "for x in trainloader:\n",
    "    y = x\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = my_model(torch.randn([]))\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.001)\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/train_{}'.format(timestamp))\n",
    "train = Train(musTraining, musValidation, writer, optimizer, model, 0.001, SSloss, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "94EMbmuf8TsS"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is the file to retreive the data from MusDB, it allows us to choose between the HQ dataset with .wav files\n",
    "or the compressed dataset with .stem.mp4 files. It also allows us to choose between the train and test subsets of the dataset.\n",
    "The class MusdbDataset is a subclass of torch.utils.data.Dataset, which allows us to use the PyTorch DataLoader class\n",
    "to load the data in batches. The __init__ method initializes the dataset, and the __getitem__ method returns the data\n",
    "for a given index. The __len__ method returns the length of the dataset.\n",
    "\n",
    "NOTES TO SELF: Fix threshold to set only off mix source\n",
    "                OTHER functions than RMS\n",
    "                Make self.batch_size\n",
    "\"\"\"\n",
    "\n",
    "import musdb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# Padding, constant batch size\n",
    "\n",
    "\n",
    "class MusdbDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, transforms, musdb_root, split='train', subset='train', is_wav=False, sample_rate=44100, segment_length = 10, segment_chunks = 10, discard_low_energy = True, segment_overlap = 0.5, drop_percentile =  0.1, chunks_below_percentile = 0.5):\n",
    "        # Check if a GPU is available\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.device(\"cuda\")\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "        assert(subset == 'train' or subset == 'test')\n",
    "        self.mode = subset\n",
    "        self.mus = musdb.DB(musdb_root, subsets=subset, split=split, is_wav=is_wav)\n",
    "        self.durations = dict()\n",
    "        self.init_durations()\n",
    "        self.transforms = transforms\n",
    "        self.sample_rate = sample_rate\n",
    "        self.shortest_duration = self.shortest_duration_in_samples(self.mus)\n",
    "        self.split = split\n",
    "        self.discard_low_energy = discard_low_energy\n",
    "        self.segment_length = segment_length\n",
    "        self.segment_chunks = segment_chunks\n",
    "        self.chunks_below_percentile = chunks_below_percentile\n",
    "        self.segment_overlap = segment_overlap\n",
    "        self.drop_percentile = drop_percentile\n",
    "        self.segment_samples = int(self.segment_length * self.sample_rate)\n",
    "        self.stft_size = self.STFT_dimensions(self.segment_samples)\n",
    "        self.chunk_samples = int(self.segment_samples / self.segment_chunks)\n",
    "        self.num_stems = self.mus.tracks[0].stems.shape[0]\n",
    "        self.num_channels = self.mus.tracks[0].stems.shape[2]\n",
    "        self.batch_size = self.find_batch_size()\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mus.tracks)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # this function should return a batch of segment STFTs from the song as well as their stem STFTs.\n",
    "        track = self.mus.tracks[idx]\n",
    "        # stems is a list of the stems of the track, in the order of the stems in the track\n",
    "        stems = torch.as_tensor(track.stems, device = self.device)\n",
    "        print(\"STEMS SHAPE :\", stems.shape)\n",
    "        if self.mode == 'train':\n",
    "            segments = self._batchize_training_item(stems)\n",
    "        else:\n",
    "            segments = self._batchize_testing_item(stems)\n",
    "        print(\"SEGMENT SHAPE: \", segments.shape)\n",
    "        # we apply a STFT to each of those segments, and that is how we achieve our constant batch and input sizes.\n",
    "        stfts = self.stft_segments(segments)\n",
    "        print(\"STFTs SHAPE: \", stfts.shape)\n",
    "        chromas, mfccs = self.chroma_and_mfcc_segments(segments)\n",
    "        print(\"Chromas SHAPE: \", chromas.shape)\n",
    "        # we then return the segments as a torch tensor\n",
    "        data = stfts[:,0]\n",
    "        labels = stfts[:,1:]\n",
    "        mfccs\n",
    "        print(\"DATA :\", data.shape, \" LABELS: \", labels.shape)\n",
    "        return data, chromas, mfccs, labels\n",
    "\n",
    "    def _batchize_training_item(self,stems):\n",
    "        # we need to trim the stems to the shortest duration track, starting from a random location\n",
    "        stems = self.trim_stems(stems, self.random_start(stems.shape[1]))\n",
    "        # we split the stems from shape (num_stems, num_samples, num_channels) into a tensor with shape (num_stems, num_segments, num_samples_per_segment, num_channels)\n",
    "        segments = self.split_track(stems)\n",
    "        # now we need to drop out the low energy segments\n",
    "        print(type(segments))\n",
    "        segments = self.high_energy_segments(segments)\n",
    "        # then we choose a random and continuous yet constant number of segments from the track minus the dropped segments\n",
    "        return segments\n",
    "\n",
    "    def _batchize_testing_item(self,stems):\n",
    "        # first we make sure the length of the song will produce a whole number of segments by padding with zeros to the end of the next segment.\n",
    "        stems = self.add_zero_padding(stems)\n",
    "        # now we extend the song with zeros to make sure we have an equal batch size for every output.\n",
    "        to_pad = self.longest_duration_in_samples() - stems.shape[1]\n",
    "        stems = self.add_N_zeros(stems, to_pad)\n",
    "        # now we split the stems into equal size segments\n",
    "        segments = self.split_track(stems)\n",
    "        # we are now ready to operate on the song since we don't want to drop or modify our data as we conserve it to reconstruct our signal.\n",
    "        return segments\n",
    "\n",
    "    def collate(batch):\n",
    "        # we define a custom function that overrides the PyTorch built in collate in order to make sure\n",
    "        # our input to the model matches the dimensionality we want.\n",
    "        # our input to the function is a list of (input, label) tuples. The list will have size = batch_size as defined in the DataLoader\n",
    "        # We will use a batch_size of 1 to receive a list of tuples (in this case a list with 1 tuple), each tuple has two elements:\n",
    "        # First, the input, a tensor with shape (batch_size, stft_dim_F, stft_dim_T, 2)\n",
    "        # Second, the label, a tensor with shape (num_stems - 1, batch_size, stft_dim_F, stft_dim_T, 2)\n",
    "        input, chromas, mfccs, labels = batch[0]\n",
    "        return input, chromas, mfccs, labels\n",
    "\n",
    "    def init_durations(self):\n",
    "        for idx, track in enumerate(self.mus.tracks):\n",
    "            self.durations[idx] = track.stems.shape[1]\n",
    "            print(track)\n",
    "            print(track.duration * 44100)\n",
    "            print(self.duration[idx])\n",
    "\n",
    "    def stft_segments(self, segments):\n",
    "        # this function should take in a list of segments and apply a STFT to each of them\n",
    "        # it should return a tensor of STFTs, of shape (num_stems, batch_size, STFT_F, STFT_T, num_channels)\n",
    "        # where batch_size is the number of STFTs we can fit into a batch\n",
    "        # STFT_F is the number of frequency bins in the STFT\n",
    "        # STFT_T is the number of time bins in the STFT\n",
    "        # num_stems is the number of stems in the track\n",
    "        # num_channels is the number of channels in the track\n",
    "        # the STFTs should be applied to each segment, and then the segments should be concatenated along the batch_size axis\n",
    "        stem_stfts = []\n",
    "        batch_size = self.batch_size\n",
    "        # we then keep only the first batch_size segments, and apply a STFT to each of them. \n",
    "        all_segments = segments[:batch_size, :, :, :].view(self.num_stems, self.num_channels, batch_size, self.segment_samples)\n",
    "        for i in range(self.num_stems):\n",
    "            channel_stfts = []\n",
    "            # we take the first batch of stfts, we reshape it to allow for stft operation using torch.stft and taking advantage of 2D batched STFTs\n",
    "            for j in range(self.num_channels): \n",
    "                # we apply a STFT to each segment\n",
    "                stem_batch_channel_segments = all_segments[i,j,:,:]\n",
    "                batch_stfts = self.transforms.stft(stem_batch_channel_segments)\n",
    "                channel_stfts.append(batch_stfts)\n",
    "                # we then add the STFT to the list of STFTs\n",
    "            stem_stfts.append(torch.stack(channel_stfts))\n",
    "        stem_stfts = torch.stack(stem_stfts)\n",
    "        return stem_stfts.view(batch_size, self.num_stems, self.num_channels, self.stft_size[0], self.stft_size[1], 2)\n",
    "        \n",
    "    def chroma_and_mfcc_segments(self, segments):\n",
    "        # this function should take in a list of segments and apply a STFT to each of them\n",
    "        # it should return a tensor of STFTs, of shape (num_stems, batch_size, STFT_F, STFT_T, num_channels)\n",
    "        # where batch_size is the number of STFTs we can fit into a batch\n",
    "        # STFT_F is the number of frequency bins in the STFT\n",
    "        # STFT_T is the number of time bins in the STFT\n",
    "        # num_stems is the number of stems in the track\n",
    "        # num_channels is the number of channels in the track\n",
    "        # the STFTs should be applied to each segment, and then the segments should be concatenated along the batch_size axis\n",
    "        batch_size = self.batch_size\n",
    "        # we then keep only the first batch_size segments, and apply a STFT to each of them. \n",
    "        all_segments = segments[:batch_size, :, :, :].view(self.num_stems, self.num_channels, batch_size, self.segment_samples)\n",
    "        channel_chromas = []\n",
    "        channel_mfccs = []\n",
    "        # we take the first batch of stfts, we reshape it to allow for stft operation using torch.stft and taking advantage of 2D batched STFTs\n",
    "        for j in range(self.num_channels): \n",
    "            print(\"123    : \", j)\n",
    "            # we apply a STFT to each segment\n",
    "            stem_batch_channel_segments = all_segments[0,j,:,:]\n",
    "            batch_chromas = self.transforms.chromagram(stem_batch_channel_segments)\n",
    "            print(\"1234   :\", j)\n",
    "            batch_mfccs = self.transforms.mel_spectrogram(stem_batch_channel_segments)\n",
    "            print(\"12345  :\", j)\n",
    "            channel_chromas.append(batch_chromas)\n",
    "            channel_mfccs.append(batch_mfccs)\n",
    "            # we then add the STFT to the list of STFTs\n",
    "        channel_mfccs = torch.stack(channel_mfccs)\n",
    "        channel_chromas = torch.stack(channel_chromas)\n",
    "        return channel_chromas.view(batch_size * self.num_channels, 12, self.stft_size[1]),  channel_mfccs.view(batch_size * self.num_channels, self.transforms.n_mels , self.stft_size[1])\n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def trim_stems(self, stems, start):\n",
    "        print(self.shortest_duration)\n",
    "        # this function should trim the track to the shortest duration of the track from the start index, it allows looping back across the song.\n",
    "        if start + self.shortest_duration > stems.shape[1]:\n",
    "            first_half = stems[:, start:, :]\n",
    "            remaining = self.shortest_duration - (stems.shape[1] - start)\n",
    "            second_half = stems[:, :remaining, :]\n",
    "            return torch.cat((first_half, second_half), axis=1)\n",
    "        else:\n",
    "            return stems[:, start:start+self.shortest_duration, :]\n",
    "\n",
    "    def pad_stems(self, stems):\n",
    "        length_in_samples = stems.shape[1]\n",
    "        to_pad = self.longest_duration_in_samples() - length_in_samples\n",
    "\n",
    "    def find_batch_size(self):\n",
    "        # this function should tell us how many STFTs we can fit into a batch based on finding the floor power of 2 of the number of STFTs we fit over the duration of the song\n",
    "        # each STFT will represent a STFT over a fixed segment length.\n",
    "        num_segments = self.num_segments_in_track(self.shortest_duration)\n",
    "        # we anticipate a drop of up to twice the drop percentile (impossible, just to be safe) of the segments.\n",
    "        num_segments = torch.as_tensor(int(num_segments * (1 - 2 * self.drop_percentile)), device = self.device)\n",
    "        # We return the closest power of two to that anticipated number of segments. Of course we use a floor because we want to fill every batch.\n",
    "        return 2 ** int(torch.floor(torch.log2(num_segments)))\n",
    "\n",
    "    def num_segments_in_track(self, duration_in_samples):\n",
    "        # this function should return the number of segments in the track and consider the overlap factor, self.segment_overlap\n",
    "        return int(torch.ceil(torch.as_tensor(duration_in_samples / (self.segment_samples * (1 - self.segment_overlap)), device=self.device)))\n",
    "\n",
    "    def random_start(self, duration_in_samples):\n",
    "        # this function should return a random start index for the track\n",
    "        return torch.randint(0, duration_in_samples, (1,))\n",
    "\n",
    "\n",
    "    def STFT_dimensions(self, duration_in_samples):\n",
    "        # this function should return the dimensions of the STFT of the shortest track in the dataset\n",
    "        F = int(self.transforms.n_fft / 2 + 1)\n",
    "        T = int(torch.ceil(torch.as_tensor(duration_in_samples / self.transforms.hop_length, device = self.device)))\n",
    "        return (F,T)\n",
    "        \n",
    "\n",
    "    def shortest_duration_in_samples(self, mus):\n",
    "        # this function should return the shortest duration of the stems in the track\n",
    "        return int(min(self.durations.values()))\n",
    "\n",
    "    def longest_duration_in_samples(self, mus):\n",
    "        # this function should return the longest duration of the stems in the track\n",
    "        return max(min([track.stems.shape[1] for track in mus.tracks]))\n",
    "\n",
    "    def is_high_energy_segment(self, segment, threshold):\n",
    "        # this function decides based on the provided threshold whether a sufficient number of chunks in the segment have an energy above the threshold\n",
    "        mix_chunk_energies = self.segment_chunk_energies(segment)[:]\n",
    "        return len(torch.argwhere(mix_chunk_energies > threshold)) > (self.chunks_below_percentile * self.segment_chunks)\n",
    "\n",
    "    def high_energy_segments(self, segments):\n",
    "        # this function should take in a full track's stems, it will then split the track into segments. \n",
    "        # The segments should have an overlap factor of self.segment_overlap.\n",
    "        # Then, it will split each segment into chunks, and it will calculate the energy of each segment, and store the energy of each segment in a list.\n",
    "        # With this list, it will calculate the percentile of the energy of the chunks, and it will discard the segments where 25% of the chunks have an energy below the percentile.\n",
    "        # It will then return the list of segments that have a high enough energy.\n",
    "        high_energy_indices = []\n",
    "        threshold = self.segment_energy_threshold(segments)\n",
    "        for idx, segment in enumerate(segments):\n",
    "            if self.is_high_energy_segment(segment, threshold):\n",
    "                high_energy_indices.append(idx)\n",
    "        high_energy_indices = torch.as_tensor(high_energy_indices, device = self.device)\n",
    "        return torch.index_select(segments, 0, high_energy_indices)\n",
    "\n",
    "    def segment_energy_threshold(self, segments):\n",
    "        # this function should split every segment into self.segment_chunks chunks, and it will calculate the energy of each chunk using the RMS energy function.\n",
    "        # It will save the energy of each chunk in a list, and it will return the value self.percentile_dropped percentile of the list.\n",
    "        chunk_energies = []\n",
    "        for segment in segments:\n",
    "            chunk_energies.extend(self.segment_chunk_energies(segment))\n",
    "        chunk_energies = torch.stack(chunk_energies)\n",
    "        percentile = torch.quantile(chunk_energies, self.drop_percentile, interpolation='midpoint')\n",
    "        return percentile\n",
    "            \n",
    "    def segment_chunk_energies(self, segment):\n",
    "        # this function should split the segment into self.segment_chunks chunks, and it will calculate the energy of each chunk using the RMS energy function.\n",
    "        # It will save the energy of each chunk in a list, and it will return the list.\n",
    "        chunk_energies = []\n",
    "        segment_samples = segment.shape[1]\n",
    "        chunk_samples = int(segment_samples / self.segment_chunks)\n",
    "        for i in range(0, segment_samples, chunk_samples):\n",
    "            chunk = segment[:, i:i+chunk_samples]\n",
    "            mix_track = chunk[0,:,:]\n",
    "            rms = self.transforms.RMS(mix_track)\n",
    "            chunk_energies.append(rms)\n",
    "        return torch.stack(chunk_energies).view(self.segment_chunks)\n",
    "\n",
    "    def split_track(self, stems):\n",
    "        # this function should take in a full track, and it will split the track into segments. \n",
    "        # The segments should have an overlap factor of self.segment_overlap.\n",
    "        # We add zero padding to the track to make sure that the track is divisible by the segment length.\n",
    "        # Then, it will split each segment into chunks, and it will return the list of chunks.\n",
    "        # The input is a tensor with shape (num_stems, num_samples, num_channels)\n",
    "        # The output is a tensor array with shape (num_stems, num_segments, num_samples_per_segment, num_channels)\n",
    "        stems = self.add_zero_padding(stems)\n",
    "        segments = []\n",
    "        num_samples = stems.shape[1]\n",
    "        step_in_samples = int(self.segment_samples * (1 - self.segment_overlap))\n",
    "        for i in range(0, num_samples - step_in_samples, step_in_samples):\n",
    "            segment = stems[:, i:i+self.segment_samples]\n",
    "            segments.append(segment)\n",
    "        segments = torch.stack(segments)\n",
    "        return segments\n",
    "\n",
    "    def add_zero_padding(self, stems):\n",
    "        # this function should add zero padding to the track to make sure that the track is divisible by the segment length and the residue from the overlap.\n",
    "        # the length of the array has to be segment_length + k * samples_in_steps for some nonnegative integer k.\n",
    "        num_samples = stems.shape[1]\n",
    "        step_in_samples = int(self.segment_samples * (1 - self.segment_overlap))\n",
    "        samples_in_last_segment = num_samples % step_in_samples\n",
    "        if samples_in_last_segment != 0:\n",
    "            padding = torch.zeros((stems.shape[0], step_in_samples - samples_in_last_segment, stems.shape[2]), device = self.device)\n",
    "            return torch.cat((stems, padding), axis=1)\n",
    "        else:\n",
    "            return stems\n",
    "\n",
    "    def add_N_zeros(self, stems, N):\n",
    "        zeros = torch.zeros((stems.shape[0], N, stems.shape[2]))\n",
    "        return torch.cat((stems, zeros), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ZDph6Wtw8kL0"
   },
   "outputs": [],
   "source": [
    "#This file will hold the loss functions for each loss I will test, including a composite loss function\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def SSloss(predicted_stfts, real_stfts):\n",
    "    # This function will calculate the loss for a given batch of predicted and real stems\n",
    "    # The loss will be the sum of the losses for each stem\n",
    "    # the dimensions of the predicted stems and real stems will be (batch_size, num_stems, stft_dim_F, stft_dim_T,num_channels)\n",
    "    # The loss will be a scalar\n",
    "    stft_loss = stft_mean_absolute_error(predicted_stfts, real_stfts)\n",
    "    # We want to find the signal representation from the STFT we computed. We can do this by taking the inverse STFT\n",
    "    # of each segment (input in the batch) separately and compare them. We don't need perfect reconstruction for our loss function,\n",
    "    # but we will need it to evaluate the model.\n",
    "    predicted_time_signals = batch_time_signals(predicted_stfts)\n",
    "    real_time_signals = batch_time_signals(real_stfts)\n",
    "    signal_loss = signal_mean_absolute_error(predicted_time_signals, real_time_signals)\n",
    "    return signal_loss + stft_loss\n",
    "\n",
    "def batch_time_signals(stfts):\n",
    "    # This function should compute the time signals for each segment in the batch\n",
    "    # input shape (batch_size, num_stems, stft_dim_F, stft_dim_T,num_channels)\n",
    "    # output shape (batch_size, num_stems, segment_samples, num_channels)\n",
    "    istft_list = []\n",
    "    for i in range(stfts.shape[0]):\n",
    "        for j in range(stfts.shape[1]):\n",
    "            istft = torch.istft(stfts[i, j], n_fft=2048, hop_length=1024, win_length=2048, window=torch.hann_window(2048, device = torch.device(\"cuda\")))\n",
    "            istft_list.append(istft)\n",
    "    istfts = torch.stack(istft_list, dim=0)\n",
    "    return istfts\n",
    "\n",
    "    \n",
    "    \n",
    "def signal_mean_absolute_error(predicted_time_signal, real_time_signal):\n",
    "    # This function will calculate the mean absolute error between the predicted and real time signals\n",
    "    # The dimensions of the predicted and real time signals will be (batch_size, num_stems, stft_dim_F, stft_dim_T,num_channels)\n",
    "    return torch.mean(torch.abs(predicted_time_signal - real_time_signal))\n",
    "\n",
    "def stft_mean_absolute_error(predicted_stfts, real_stfts):\n",
    "    # This function will calculate the mean absolute error between the predicted and real STFTs\n",
    "    # The dimensions of the predicted and real STFTs will be (batch_size, num_stems, stft_dim_F, stft_dim_T,num_channels)\n",
    "    return torch.mean(torch.abs(predicted_stfts - real_stfts))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "YQNYCjfp8x8r"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def test_module(module, input_size):\n",
    "    \"\"\"\n",
    "    Test an nn.Module subclass with random input data of the given size.\n",
    "    \n",
    "    :param module: The nn.Module subclass to test.\n",
    "    :param input_size: A tuple of integers specifying the size of the input data.\n",
    "    \"\"\"\n",
    "    # Create random input data with the specified size\n",
    "    input_data = torch.randn(input_size)\n",
    "    print(f'Input shape: {input_data.shape}')\n",
    "\n",
    "    # Run the input data through the module\n",
    "    output = module(input_data)\n",
    "    print(f'Output shape: {output.shape}')\n",
    "\n",
    "\n",
    "class SingularValueRegularization(nn.Module):\n",
    "    def __init__(self, num_singular_values, eps=1e-3):\n",
    "        super().__init__()\n",
    "        self.num_singular_values = num_singular_values # number of singular values to keep\n",
    "        self.eps = eps \n",
    "    \n",
    "    def forward(self, x):\n",
    "        u, s, v = torch.svd(x) # perform singular value decomposition on x\n",
    "        s[self.num_singular_values:] = self.eps # set the remaining singular values to epsilon\n",
    "        x_hat = u @ torch.diag(s) @ v # construct x_hat using the first num_singular_values singular values\n",
    "        return x_hat\n",
    "\n",
    "\n",
    "class my_model(torch.nn.Module):\n",
    "    def __init__(self, bandwidths, N):\n",
    "        # bandwidth\n",
    "        super(my_model, self).__init__()\n",
    "        self.bandwidths = bandwidths\n",
    "        self.norm_layers = torch.nn.ModuleList([torch.nn.LayerNorm(2 * bandwidth) for bandwidth in self.bandwidths])\n",
    "        self.fc_layers = torch.nn.ModuleList([torch.nn.Linear(2 * bandwidth, N) for bandwidth in self.bandwidths])\n",
    "\n",
    "    def forward(self, X):\n",
    "        subband_spectrograms = []\n",
    "        K = len(self.bandwidths)\n",
    "        for i in range(K):\n",
    "            start_index = sum(self.bandwidths[:i])\n",
    "            end_index = start_index + self.bandwidths[i]\n",
    "            subband_spectrogram = X[:, start_index:end_index, :]\n",
    "            subband_spectrograms.append(subband_spectrogram)\n",
    "\n",
    "        subband_features = []\n",
    "        for i in range(K):\n",
    "            norm_output = self.norm_layers[i](subband_spectrograms[i])\n",
    "            fc_output = self.fc_layers[i](norm_output)\n",
    "            subband_features.append(fc_output)\n",
    "\n",
    "        Z = torch.stack(subband_features, dim=1)\n",
    "\n",
    "        return Z\n",
    "\n",
    "\n",
    "# This class defines a module that runs the input, with shape (num_bands, num_timesteps, N), through a normalization layer, then a temporal biLSTM, then a fully connected layer.\n",
    "# Then, the output of that layer is of the same shape as the input to the module, which will be fed into a similar structure, but this time with a band biLSTM, following the same normalization, biLSTM, FC structure.\n",
    "class GeneralBiLSTMUnit(nn.Module):\n",
    "    def __init__(self, N, hidden_size, axis=1):\n",
    "        super(GeneralBiLSTMUnit, self).__init__()\n",
    "        self.norm = nn.GroupNorm(1, N)\n",
    "        self.bilstm = nn.LSTM(N, hidden_size, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size*2, N)\n",
    "        self.axis = axis\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Normalize the input\n",
    "        x = self.norm(x)\n",
    "\n",
    "        # Apply the temporal biLSTM layer\n",
    "        x, _ = self.bilstm(x.transpose(0,1))\n",
    "\n",
    "        # Apply the fully connected layer\n",
    "        x = self.fc(x)\n",
    "        x = x.transpose(0,1)\n",
    "        \n",
    "        x += input \n",
    "        # Return the output of the module\n",
    "        return x\n",
    "\n",
    "class InterleavedBiLSTMs(nn.Module):\n",
    "    def __init__(self, N, hidden_size):\n",
    "        super(InterleavedBiLSTMs, self).__init__()\n",
    "        self.temporal_bilstm = GeneralBiLSTMUnit(N, hidden_size)\n",
    "        self.band_bilstm = GeneralBiLSTMUnit(N, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.temporal_bilstm(x)\n",
    "        x = self.band_bilstm(x)\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torchaudio 0.13.0\n",
      "Uninstalling torchaudio-0.13.0:\n",
      "  Would remove:\n",
      "    /opt/conda/lib/python3.10/site-packages/torchaudio-0.13.0.dist-info/*\n",
      "    /opt/conda/lib/python3.10/site-packages/torchaudio/*\n",
      "Proceed (Y/n)? ^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip uninstall torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "gLvGC9MG88Mu"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import torch\n",
    "import librosa\n",
    "# import torchaudio\n",
    "\n",
    "class Transforms():\n",
    "    #def __init__(self, sample_rate=44100, n_fft=2048, hop_length=1024, win_length=2048, window='hann', center=True, pad_mode='reflect', power=2.0, n_mels=128, fmin=0.0, fmax=None, htk=False, norm=1, top_db=80.0, ref=1.0, amin=1e-10):\n",
    "    def __init__(self, sample_rate=44100, n_fft=2048, hop_length=1024, win_length=2048, window='hann', center=True, pad_mode='reflect', power=2.0, n_mels=32, fmin=0.0, fmax=None, htk=False, norm=1, top_db=80.0, ref=1.0, amin=1e-10):\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.device(\"cuda\")\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "        self.sample_rate = sample_rate\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.win_length = win_length\n",
    "        self.window = window\n",
    "        self.center = center\n",
    "        self.pad_mode = pad_mode\n",
    "        self.power = power\n",
    "        self.n_mels = n_mels\n",
    "        self.fmin = fmin\n",
    "        self.fmax = fmax\n",
    "        self.htk = htk\n",
    "        self.norm = norm\n",
    "        self.top_db = top_db\n",
    "        self.ref = ref\n",
    "        self.amin = amin\n",
    "        #self.mfcc_transform = torchaudio.transforms.MelSpectrogram(sample_rate = 44100, n_fft = 2048, win_length= 2048, hop_length=1024, n_mels=64)\n",
    "\n",
    "    def __call__(self, x): \n",
    "        #x = torch.from_numpy(x)\n",
    "        #x = torch.Tensor(x)\n",
    "        x = torch.tensor(x)\n",
    "        x = x.permute(1, 0)\n",
    "        x = self.stft(x)\n",
    "        x = self.mel_spectrogram(x)\n",
    "        x = self.amplitude_to_db(x)\n",
    "        return x\n",
    "\n",
    "    def stft(self, x, n_fft=2048, hop_length=1024, win_length=2048, window=torch.hann_window(2048, device = torch.device('cuda')), center=True, pad_mode='reflect'):\n",
    "        return torch.stft(x, n_fft=n_fft, hop_length=hop_length, win_length=win_length, window=window, center=center, pad_mode=pad_mode, onesided=True, return_complex=False)\n",
    "\n",
    "    def istft(self, x, n_fft=2048, hop_length=1024, win_length=2048, window=torch.hann_window(2048,device = torch.device('cuda')), center=True):\n",
    "        return torch.istft(x, n_fft=n_fft, hop_length=hop_length, win_length=win_length, window=window, center=center, onesided=True)\n",
    "\n",
    "    def mel_spectrogram(self, x, sr=44100, n_fft=2048, hop_length = 1024, win_length = 2048, window = 'hann', n_mels=32):\n",
    "       # return self.mfcc_transform(x)\n",
    "        return torch.as_tensor(librosa.feature.melspectrogram(x.detach().cpu().numpy(), sr = 44100, n_fft = n_fft, hop_length = hop_length, window = window, n_mels = n_mels, win_length = win_length))\n",
    "    def chromagram(self, x, sr = 44100, n_fft = 2048, hop_length = 1024, win_length = 2048, window = 'hann'):\n",
    "        device = x.get_device()\n",
    "        return torch.as_tensor(librosa.feature.chroma_stft(x.detach().cpu().numpy(), sr=sr, n_fft = n_fft, hop_length = hop_length, win_length = win_length, window = window), device = device)\n",
    "    def amplitude_to_db(self, x, top_db=80.0, ref=1.0, amin=1e-10):\n",
    "        return librosa.amplitude_to_db(x, top_db=top_db, ref=ref, amin=amin)\n",
    "\n",
    "    def RMS(self, audio_tensor):\n",
    "        # this function is used to calculate the RMS of the audio signal\n",
    "        # the input will be an audio tensor of shape (num_samples)\n",
    "        # Note that this function is intended to use on smaller audio signals, and that\n",
    "        # typically, longer signals get windows similar to the STFT in order to calculate more localized\n",
    "        # RMS. The purpose of using this was for silent signal detection.\n",
    "        squared_tensor = torch.pow(audio_tensor, 2)\n",
    "        mean_power = torch.mean(squared_tensor)\n",
    "        rms = torch.sqrt(mean_power)\n",
    "        return rms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "6xiLn4FA9EJF"
   },
   "outputs": [],
   "source": [
    "# This class should define the training loop for the model. We will define a training loop function\n",
    "# as well as a train_one_epoch function. This is based on the tutorial on the official PyTorch website: \n",
    "# https://pytorch.org/tutorials/beginner/introyt/trainingyt.html#the-training-loop\n",
    "# we will also implement our training via train/validate/test splits, and save our best models parameters.\n",
    "# I will use SummaryWriter, a useful class for reporting our data from PyTorch.\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "class Train():\n",
    "    def __init__(self, train_dataset, validation_dataset, summary_writer, optimizer, model, learning_rate, loss, epochs = 1000, report_loss_frequency=15):\n",
    "        self.train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=1, collate_fn=collate)\n",
    "        self.validation_dataloader = torch.utils.data.DataLoader(validation_dataset, batch_size=1, collate_fn=collate)\n",
    "        self.writer = summary_writer\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss = loss\n",
    "        self.report_loss_frequency = report_loss_frequency\n",
    "        self.run_model(epochs=epochs)\n",
    "\n",
    "    def train_one_epoch(self, epoch_number):\n",
    "        epoch_loss = 0.0\n",
    "        last_loss = 0.0\n",
    "        for i, data in enumerate(self.train_dataloader):\n",
    "            # We will use the PyTorch Data Loader class to easily iterate through the data and collect our batches.\n",
    "            # See musdb.py for a deeper explanation of how we collect batches. Simply speaking, we take a an arbitrary\n",
    "            # fixed-length part of the song and we split it into fixed length segments, taking the STFT of each. \n",
    "            mixture_stft, real_stems_stft = data\n",
    "            # PyTorch accumulates gradients by default, so we zero out the gradients before each batch update in order to \n",
    "            # ensure that the optimizer only uses the gradients from the current batch during the update step.\n",
    "            # This also should help with performance and memory costs.\n",
    "            self.optimizer.zero_grad()\n",
    "            # We call our model to make a prediction\n",
    "            predicted_stems_stft = self.model(mixture_stft)\n",
    "            # We compute the loss from our loss function defined in loss.py\n",
    "            loss = self.loss(predicted_stems_stft, real_stems_stft)\n",
    "            # We calculate the gradient\n",
    "            loss.backward()\n",
    "            # Then update the weights based on the gradient\n",
    "            self.optimizer.step()\n",
    "            # In order to report our loss, we print the loss every 15 batches\n",
    "            accumulating_loss += loss.item()\n",
    "            if i % self.report_loss_frequency == self.report_loss_frequency - 1:\n",
    "                last_loss = accumulating_loss\n",
    "                print(\"Batch {}. Average loss over last {} batches is: {}\".format(i+1, last_loss))\n",
    "                writer_index = epoch_number * len(self.train_dataloader) + i + 1\n",
    "                self.writer.add_scalar('Loss/train', last_loss, writer_index)\n",
    "                accumulating_loss = 0\n",
    "            \n",
    "            return last_loss\n",
    "\n",
    "    def run_model(self, epochs):\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        epoch_number = 0\n",
    "        #set arbitrarily large initial validation loss\n",
    "        best_vloss = 2.0 ** 30\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            print(\"STARTING EPOCH {}\".format(epoch_number + 1))\n",
    "            # We want to learn in our learning loops, so we make sure our gradients are updating during each epoch.\n",
    "            self.model.train(True)\n",
    "            avg_loss = self.train_one_epoch(epoch_number)\n",
    "            # We don't need or want the gradient opertaion on any of the \n",
    "            self.model.train(False)\n",
    "\n",
    "            accumulating_vloss = 0.0\n",
    "\n",
    "            for i, vdata in enumerate(self.validation_dataloader):\n",
    "                vmixture_stft, vreal_stems_stft = vdata\n",
    "                vpredicted_stems_stft = self.model(vmixture_stft)\n",
    "                vloss = self.loss(vpredicted_stems_stft, vreal_stems_stft)\n",
    "                accumulating_vloss += vloss\n",
    "            \n",
    "            avg_vloss = accumulating_vloss / (i + 1)\n",
    "            print('LOSS training set: {} validation set: {}'.format(avg_loss, avg_vloss))\n",
    "            \n",
    "            # log all of this \n",
    "            self.writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "            self.writer.flush()\n",
    "\n",
    "            if avg_vloss < best_vloss:\n",
    "                best_vloss = avg_vloss\n",
    "                model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "                torch.save(self.model.state_dict(), model_path)\n",
    "            \n",
    "            epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "V-b-ssPg9IUR"
   },
   "outputs": [],
   "source": [
    "# I will use the museval package to calculate the SDR, SIR, and SAR scores as well as\n",
    "# the time invariant SDR, SIR SAR variants.\n",
    "# Importantly, this class will allow us to test the model on full songs. For training, we make some sacrifices to opitimize \n",
    "# computations and introduce randomization and we end up processing the song in parts over training. This is also because our reconstruction\n",
    "# task is to recreate the segments of the song that were tested and to compare only with those features.\n",
    "import torch\n",
    "import museval \n",
    "\n",
    "def load_model(model_path):\n",
    "    model = my_model()\n",
    "    model.load_state_dict(model_path)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "class Evaluate():\n",
    "    def __init__(self, test_dataset, model_path):\n",
    "        self.test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size = 1, collate_fn = collate)\n",
    "        self.model = load_model(model_path)\n",
    "        self.mus = test_dataset.mus\n",
    "\n",
    "    def reconstruct_all_and_evaluate(self):\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(self.test_dataloader):\n",
    "                mixture_stft, _ = data\n",
    "                predicted_stems_stft = self.model(mixture_stft)\n",
    "                self.reconstruct_one_and_evaluate(predicted_stems_stft, i)\n",
    "\n",
    "    def reconstruct_one_and_evaluate(self, predicted_stems_stfts, i): \n",
    "        stem_estimates_time_signals = self.reconstruct_one_song(predicted_stems_stfts)\n",
    "        self.evaluate_one_song(stem_estimates_time_signals, i)\n",
    "\n",
    "    def evaluate_one_song(self, stems_estimates_time_signals, i):\n",
    "        # this function will evaluate the song using the museval package.\n",
    "        # it will receive the time signals of the stems and the song number in the dataset.\n",
    "        # it will both save the results in the results and print the scores after evaluating each song.\n",
    "        scores = museval.eval_mus_track(self.mus[i], stems_estimates_time_signals, output_dir='results')   \n",
    "        print(scores)\n",
    "\n",
    "    def reconstruct_one_song(self, predicted_stems_stft):\n",
    "        # this function will reconstruct the song from the predicted stems stft and return the time signals of the stems.\n",
    "        # We will use the overlap and add method to reconstruct the song after we recover the time signal for each segment for each stem.\n",
    "        time_signals = batch_time_signals(predicted_stems_stft)\n",
    "        stem_estimates_time_signals = self.overlap_and_add(time_signals)\n",
    "        return stem_estimates_time_signals\n",
    "\n",
    "    def overlap_and_add(self, time_signals):\n",
    "        # receives all the reconstructed time signals and overlaps them into one signal to reconstruct the original song.\n",
    "        # this will happen based on the parameters in the dataset for segment length and overlap.\n",
    "        step_in_samples = time_signals.shape[2] * (1 - self.test_dataset.overlap)\n",
    "        segment_length = time_signals.shape[2]\n",
    "        num_segments = time_signals.shape[1]\n",
    "        full_signal_length = segment_length + step_in_samples * (num_segments - 1)\n",
    "        new_signal = torch.zeros((time_signals.shape[0], full_signal_length))\n",
    "        for i in range(num_segments):\n",
    "            new_signal[:, i * step_in_samples : i * step_in_samples + segment_length] += time_signals[:, i, :]\n",
    "        return new_signal\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5YGv-8DC9QJa"
   },
   "outputs": [],
   "source": [
    "zz = torch.Tensor(10043)\n",
    "zz.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hq21ry7MA51w",
    "outputId": "8589fb76-ade5-4d47-8ea3-e4df2fb011b8"
   },
   "outputs": [],
   "source": [
    "int(torch.as_tensor(0.43))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aUU8IL4xGxqe"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
